[
  {
    "name": "Retrieval-augmented generation (RAG)",
    "ring": "adopt",
    "quadrant": "techniques",
    "isNew": "FALSE",
    "status": "moved in",
    "description": "\"<p><strong><a href=\"\"https://arxiv.org/abs/2005.11401v4\"\" target=\"\"_blank\"\" aria-label=\"\"Retrieval-augmented generation (RAG). This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Retrieval-augmented generation (RAG)<span class=\"\"pop-out-icon\"\"></span></a></strong> is the preferred pattern for our teams to improve the quality of responses generated by a large language model (LLM). We’ve successfully used it in several projects, including the popular <a href=\"\"https://www.jugalbandi.ai/mission\"\" target=\"\"_blank\"\" aria-label=\"\"Jugalbandi AI Platform. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Jugalbandi AI Platform<span class=\"\"pop-out-icon\"\"></span></a>. With RAG, information about relevant and trustworthy documents — in formats like HTML and PDF — are stored in databases that supports a vector data type or efficient document search, such as <a href=\"\"https://www.thoughtworks.com/radar/platforms/pgvector\"\">pgvector</a>, <a href=\"\"https://www.thoughtworks.com/radar/platforms/qdrant\"\">Qdrant</a> or <a href=\"\"https://www.thoughtworks.com/radar/platforms/elasticsearch-relevance-engine\"\">Elasticsearch Relevance Engine</a>. For a given prompt, the database is queried to retrieve relevant documents, which are then combined with the prompt to provide richer context to the LLM. This results in higher quality output and greatly reduced hallucinations. The context window — which determines the maximum size of the LLM input — is limited, which means that selecting the most relevant documents is crucial. We improve the relevancy of the content that is added to the prompt by reranking. Similarly, the documents are usually too large to calculate an embedding, which means they must be split into smaller chunks. This is often a difficult problem, and one approach is to have the chunks overlap to a certain extent.</p>\""
  },
  {
    "name": "Automatically generate Backstage entity descriptors",
    "ring": "trial",
    "quadrant": "techniques",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><a href=\"\"https://www.thoughtworks.com/radar/platforms/backstage\"\">Backstage</a> from Spotify has been widely adopted across our client base as the preferred platform to host developer experience portals. Backstage, on its own, is just a shell that hosts plugins and provides an interface to manage the catalog of assets that make up a platform ecosystem. Any entity to be displayed or managed by Backstage is configured in the catalog-info file, which contains data such as status, lifecycle, dependencies and APIs among other details. By default, individual entity descriptors are written by hand and usually maintained and versioned by the team responsible for the component in question. Keeping the descriptors up to date can be tedious and create a barrier to developer adoption. Also, there is always the possibility that changes are overlooked or that some components are missed entirely. We've found it more efficient and less error-prone to <strong>automatically generate Backstage entity descriptors</strong>. Most organizations have existing sources of information that can jump-start the process of populating catalog entries. Good development practices, for example, putting appropriate tags on AWS resources or adding metadata to source files, can simplify entity discovery and descriptor generation. These automated processes can then be run on a regular basis — once a day, for example — to keep the catalog fresh and up to date.</p>\""
  },
  {
    "name": "Combining traditional NLP with LLMs",
    "ring": "trial",
    "quadrant": "techniques",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p>Large language models (LLMs) are the Swiss Army knives of natural language processing (NLP). But they’re also quite expensive and not always the best tool for the job — sometimes it's more effective to use a proper corkscrew. Indeed, there’s a lot of potential in <strong>combining traditional NLP with LLMs</strong>, or in building multiple NLP approaches in conjunction with LLMs to implement use cases and leverage LLMs for the steps where you actually need their capabilities. Traditional data science and NLP approaches for document clustering, topic identification and classification and even summarization are cheaper and can be more effective for solving a part of your use case problem. We then use LLMs when we need to generate and summarize longer texts, or combine multiple large documents, to take advantage of the LLM's superior attention span and memory. For example, we’ve successfully used this combination of techniques to generate a comprehensive trends report for a domain from a large corpus of individual trend documents, using traditional clustering alongside the generative power of LLMs.</p>\""
  },
  {
    "name": "Continuous compliance",
    "ring": "trial",
    "quadrant": "techniques",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong>Continuous compliance</strong> is the practice of ensuring that software development processes and technologies comply with industry regulations and security standards on an ongoing basis, by heavily leveraging automation. Manually checking for security vulnerabilities and adhering to regulations can slow down development and introduce errors. As an alternative, organizations can automate compliance checks and audits. They can integrate tools into software development pipelines, allowing teams to detect and address compliance issues early in the development process. Codifying compliance rules and best practices helps enforce policies and standards consistently across teams. It enables you to scan code changes for vulnerabilities, enforce coding standards and track infrastructure configuration changes to ensure they meet compliance requirements. Lastly, automated reporting of the above simplifies audits and provides clear evidence of compliance. We’ve already talked about techniques like publishing <a href=\"\"https://www.thoughtworks.com/radar/techniques/software-bill-of-materials\"\">SBOMs</a> and applying the recommendations from <a href=\"\"https://www.thoughtworks.com/radar/techniques/slsa\"\">SLSA</a> — they can be very good starting points. The benefits of this technique are multifold. First, automation leads to more secure software by identifying and mitigating vulnerabilities early and, second, development cycles accelerate as manual tasks are eliminated. Reduced costs and enhanced consistency are additional perks. For safety-critical industries like software-driven vehicles, automated continuous compliance can improve the efficiency and reliability of the certification process, ultimately leading to safer and more reliable vehicles on the road.</p>\""
  },
  {
    "name": "Edge functions",
    "ring": "trial",
    "quadrant": "techniques",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p>Although not a new concept, we've noticed the growing availability and use of decentralized code execution via content delivery networks (CDNs). Services such as <a href=\"\"https://www.thoughtworks.com/radar/platforms/cloudflare-workers\"\">Cloudflare Workers</a> or <a href=\"\"https://aws.amazon.com/developer/application-security-performance/articles/cloudfront-edge-functions/\"\" target=\"\"_blank\"\" aria-label=\"\"Amazon CloudFront Edge Functions. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Amazon CloudFront Edge Functions<span class=\"\"pop-out-icon\"\"></span></a> provide a mechanism to execute snippets of serverless code close to the client's geographic location. <strong>Edge functions</strong> not only offer lower latency if a response can be generated at the edge, they also present an opportunity to rewrite requests and responses in a location-specific way on their way to and from the regional server. For example, you might rewrite a request URL to route to a specific server that has local data relevant to a field found in the request body. This approach is best suited to short, fast-running stateless processes since the computational power at the edge is limited.</p>\""
  },
  {
    "name": "Security champions",
    "ring": "trial",
    "quadrant": "techniques",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong>Security champions</strong> are team members who think critically about security repercussions of both technical and nontechnical delivery decisions. They raise these questions and concerns with team leadership and have a firm understanding of basic security guidelines and requirements. They help development teams approach all activities during software delivery with a security mindset, thus reducing the overall security risks for the systems they develop. A security champion is not a separate position but a responsibility assigned to an existing member of the team who is guided by appropriate training from security practitioners. Equipped with this training, security champions improve the security awareness of the team by spreading knowledge and acting as a bridge between the development and security teams. One great example of an activity security champions can help drive within the team is <a href=\"\"https://www.thoughtworks.com/radar/techniques/threat-modeling\"\">threat modeling</a>, which helps teams think about security risks from the start. Appointing and training a security champion on a team is a great first step, but relying solely on champions without proper commitment from leaders can lead to problems. Building a security mindset, in our experience, requires commitment from the entire team and managers.</p>\""
  },
  {
    "name": "Text to SQL",
    "ring": "trial",
    "quadrant": "techniques",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong>Text to SQL</strong> is a technique that converts natural language queries into SQL queries that can be executed by a database. Although large language models (LLMs) can understand and transform natural language, creating accurate SQL for your own schema can be challenging. Enter <a href=\"\"https://github.com/vanna-ai/vanna\"\" target=\"\"_blank\"\" aria-label=\"\"Vanna. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Vanna<span class=\"\"pop-out-icon\"\"></span></a>, an open-source Python <a href=\"\"https://www.thoughtworks.com/radar/techniques/retrieval-augmented-generation-rag\"\">retrieval-augmented generation (RAG)</a> framework for SQL generation. Vanna works in two steps: first you create embeddings with the data definition language statements (DDLs) and sample SQLs for your schema, and then you ask questions in natural language. Although Vanna can work with any LLMs, we encourage you to assess <a href=\"\"https://www.numbersstation.ai/post/introducing-nsql-open-source-sql-copilot-foundation-models\"\" target=\"\"_blank\"\" aria-label=\"\"NSQL. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">NSQL<span class=\"\"pop-out-icon\"\"></span></a>, a <a href=\"\"https://www.thoughtworks.com/radar/techniques/domain-specific-llms\"\">domain-specific LLM</a> for text-to-SQL tasks.</p>\""
  },
  {
    "name": "Tracking health over debt",
    "ring": "trial",
    "quadrant": "techniques",
    "isNew": "FALSE",
    "status": "no change",
    "description": "\"<p>We keep experiencing the improvements teams make to their ecosystem by treating the health rating the same as other service-level objectives (SLOs) and prioritizing enhancements accordingly, instead of solely focusing on tracking <a href=\"\"https://martinfowler.com/bliki/TechnicalDebt.html\"\" target=\"\"_blank\"\" aria-label=\"\"technical debt. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">technical debt<span class=\"\"pop-out-icon\"\"></span></a>. By allocating resources efficiently to address the most impactful issues related to health, teams and organizations can reduce long-term maintenance costs and evolve products more efficiently. This approach also enhances communication between technical and nontechnical stakeholders, fostering a common understanding of the system's state. Although metrics may vary among organizations (see this <a href=\"\"https://www.rea-group.com/about-us/news-and-insights/blog/what-good-software-looks-like-at-rea/\"\" target=\"\"_blank\"\" aria-label=\"\"blog post. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">blog post<span class=\"\"pop-out-icon\"\"></span></a> for examples) they ultimately contribute to long-term sustainability and ensure software remains adaptable and competitive. In a rapidly changing digital landscape, focusing on <strong>tracking health over debt</strong> of systems provides a structured and evidence-based strategy to maintain and enhance them.</p>\""
  },
  {
    "name": "AI team assistants",
    "ring": "assess",
    "quadrant": "techniques",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p>AI coding assistance tools like <a href=\"\"https://www.thoughtworks.com/radar/tools/github-copilot\"\">GitHub Copilot</a> are currently mostly talked about in the context of assisting and enhancing an individual's work. However, software delivery is and will remain team work, so you should be looking for ways to create <strong>AI team assistants</strong> to help create the \"\"10x team,\"\" as opposed to a bunch of siloed AI-assisted <a href=\"\"https://www.thoughtworks.com/radar/techniques/10x-engineers\"\">10x engineers</a>. We've started using a team assistance approach that can increase knowledge amplification, upskilling and alignment through a combination of prompts and knowledge sources. Standardized prompts facilitate the use of agreed-upon best practices in the team context, such as techniques and templates for user story writing or the implementation of practices like threat modeling. In addition to prompts, knowledge sources made available through <a href=\"\"https://www.thoughtworks.com/radar/techniques/retrieval-augmented-generation-rag\"\">retrieval-augmented generation</a> provide contextually relevant information from organizational guidelines or industry-specific knowledge bases. This approach gives team members access to the knowledge and resources they need just in time.</p>\""
  },
  {
    "name": "Graph analysis for LLM-backed chats",
    "ring": "assess",
    "quadrant": "techniques",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p>Chatbots backed by large language models (LLMs) are gaining a lot of popularity right now, and we’re seeing emerging techniques around productionizing and productizing them. One such productization challenge is understanding how users are conversing with a chatbot that is driven by something as generic as an LLM, where the conversation can go in many directions. Understanding the reality of conversation flows is crucial to improving the product and improving conversion rates. One technique to tackle this problem is to use <strong>graph analysis for LLM-backed chats</strong>. The agents that support a chat with a specific desired outcome — such as a shopping action or a successful resolution of a customer's problem — can usually be represented as a desired state machine. By loading all conversations into a graph, you can analyze actual patterns and look for discrepancies to the expected state machine. This helps find bugs and opportunities for product improvement.</p>\""
  },
  {
    "name": "LLM-backed ChatOps",
    "ring": "assess",
    "quadrant": "techniques",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong>LLM-backed ChatOps</strong> is an emerging application of large language models through a chat platform (primarily Slack) that allows engineers to build, deploy and operate software via natural language. This has the potential to streamline engineering workflows by enhancing the discoverability and user-friendliness of platform services. At the time of writing, two early examples are <a href=\"\"https://www.promptops.com/devops/\"\" target=\"\"_blank\"\" aria-label=\"\"PromptOps. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">PromptOps<span class=\"\"pop-out-icon\"\"></span></a> and <a href=\"\"https://www.kubiya.ai/\"\" target=\"\"_blank\"\" aria-label=\"\"Kubiya. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Kubiya<span class=\"\"pop-out-icon\"\"></span></a>. However, considering the finesse needed for production environments, organizations should thoroughly evaluate these tools before allowing them anywhere near production.</p>\""
  },
  {
    "name": "LLM-powered autonomous agents",
    "ring": "assess",
    "quadrant": "techniques",
    "isNew": "FALSE",
    "status": "no change",
    "description": "\"<p><strong>LLM-powered autonomous agents</strong> are evolving beyond single agents and static multi-agent systems with the emergence of frameworks like <a href=\"\"https://microsoft.github.io/autogen/\"\" target=\"\"_blank\"\" aria-label=\"\"Autogen. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Autogen<span class=\"\"pop-out-icon\"\"></span></a> and <a href=\"\"https://www.crewai.io/\"\" target=\"\"_blank\"\" aria-label=\"\"CrewAI. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">CrewAI<span class=\"\"pop-out-icon\"\"></span></a>. These frameworks allow users to define agents with specific roles, assign tasks and enable agents to collaborate on completing those tasks through delegation or conversation. Similar to single-agent systems that emerged earlier, such as <a href=\"\"https://github.com/Significant-Gravitas/AutoGPT\"\" target=\"\"_blank\"\" aria-label=\"\"AutoGPT. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">AutoGPT<span class=\"\"pop-out-icon\"\"></span></a>, individual agents can break down tasks, utilize preconfigured tools and request human input. Although still in the early stages of development, this area is developing rapidly and holds exciting potential for exploration.</p>\""
  },
  {
    "name": "Using GenAI to understand legacy codebases",
    "ring": "assess",
    "quadrant": "techniques",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p>Generative AI (GenAI) and large language models (LLMs) can help developers both write and understand code. In practical application, this is so far mostly limited to smaller code snippets, but more products and technology developments are emerging for <strong>using GenAI to understand legacy codebases</strong>. This is particularly useful in the case of  legacy codebases that aren’t well-documented or where the documentation is outdated or misleading. For example, <a href=\"\"https://www.driverai.com/\"\" target=\"\"_blank\"\" aria-label=\"\"Driver AI. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Driver AI<span class=\"\"pop-out-icon\"\"></span></a> or <a href=\"\"https://bloop.ai/\"\" target=\"\"_blank\"\" aria-label=\"\"bloop. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">bloop<span class=\"\"pop-out-icon\"\"></span></a> use  <a href=\"\"https://www.thoughtworks.com/radar/techniques/retrieval-augmented-generation-rag\"\">RAG</a> approaches that combine language intelligence and code search with LLMs to help users find their way around a codebase. Emerging models with larger and larger context windows will also help to make these techniques more viable for sizable codebases. Another promising application of GenAI for legacy code is in the space of mainframe modernization, where bottlenecks often form around reverse engineers who need to understand the existing codebase and turn that understanding into requirements for the modernization project. Using GenAI to assist those reverse engineers can help them get their work done faster.</p>\""
  },
  {
    "name": "VISS",
    "ring": "assess",
    "quadrant": "techniques",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p>Zoom recently open-sourced its Vulnerability Impact Scoring System, or <strong><a href=\"\"https://github.com/zoom/viss\"\" target=\"\"_blank\"\" aria-label=\"\"VISS. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">VISS<span class=\"\"pop-out-icon\"\"></span></a></strong>. This system is mainly focused on vulnerability scoring that prioritizes actual demonstrated security measures. VISS differs from the Common Vulnerability Scoring System (CVSS) by not focusing on worst-case scenarios and attempting to more objectively measure the impact of vulnerabilities from a defender's perspective. To this aim, VISS provides a <a href=\"\"https://viss.zoom.com/calculator/\"\" target=\"\"_blank\"\" aria-label=\"\"web-based UI. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">web-based UI<span class=\"\"pop-out-icon\"\"></span></a> to calculate the vulnerability score based on several parameters — categorized into platform, infrastructure and data groups — including the impact on the platform, the number of tenants impacted, data impact and more. Although we don't have too much practical experience with this specific tool yet, we think this kind of priority-tailored assessment <a href=\"\"https://viss.zoom.com/specifications\"\" target=\"\"_blank\"\" aria-label=\"\"approach. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">approach<span class=\"\"pop-out-icon\"\"></span></a> based on industry and context is worth practicing.</p>\""
  },
  {
    "name": "Broad integration tests",
    "ring": "hold",
    "quadrant": "techniques",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p>While we applaud a focus on automated testing, we continue to see numerous organizations over-invested in what we believe to be ineffective <strong>broad integration tests</strong>. As the term \"\"integration test\"\" is ambiguous, we've taken the broad classification from Martin Fowler's <a href=\"\"https://martinfowler.com/bliki/IntegrationTest.html\"\" target=\"\"_blank\"\" aria-label=\"\"bliki entry. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">bliki entry<span class=\"\"pop-out-icon\"\"></span></a> on the subject which indicates a test that requires live versions of all run-time dependencies. Such a test is obviously expensive, because it requires a full-featured test environment with all the necessary infrastructure, data and services. Managing the right versions of all those dependencies requires significant coordination overhead, which tends to slow down release cycles. Finally, the tests themselves are often fragile and unhelpful. For example, it takes effort to determine if a test failed because of the new code, mismatched version dependencies or the environment, and the error message rarely helps pinpoint the source of the error. Those criticisms don't mean that we take issue with automated \"\"black box\"\" integration testing in general, but we find a more helpful approach is one that balances the need for confidence with release frequency. This can be done in two stages by first validating the behavior of the system under test assuming a certain set of responses from run-time dependencies, and then validating those assumptions. The first stage uses service virtualization to create test doubles of run-time dependencies and validates the behavior of the system under test. This simplifies test data management concerns and allows for deterministic tests. The second stage uses <a href=\"\"https://martinfowler.com/bliki/ContractTest.html\"\" target=\"\"_blank\"\" aria-label=\"\"contract tests. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">contract tests<span class=\"\"pop-out-icon\"\"></span></a> to validate those environmental assumptions with real dependencies.</p>\""
  },
  {
    "name": "Overenthusiastic LLM use",
    "ring": "hold",
    "quadrant": "techniques",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p>In the rush to leverage the latest in AI, many organizations are quickly adopting large language models (LLMs) for a variety of applications, from content generation to complex decision-making processes. The allure of LLMs is undeniable; they offer a seemingly effortless solution to complex problems, and developers can often create such a solution quickly and without needing years of deep machine learning experience. It can be tempting to roll out an LLM-based solution as soon as it’s more or less working and then move on. Although these LLM-based proofs of value are useful, we advise teams to look carefully at what the technology is being used for and to consider whether an LLM is actually the right end-stage solution. Many problems that an LLM can solve — such as sentiment analysis or content classification — can be solved more cheaply and easily using traditional natural language processing (NLP). Analyzing what the LLM is doing and then analyzing other potential solutions not only mitigates the risks associated with <strong>overenthusiastic LLM use</strong> but also promotes a more nuanced understanding and application of AI technologies.</p>\""
  },
  {
    "name": "Rush to fine-tune LLMs",
    "ring": "hold",
    "quadrant": "techniques",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p>As organizations are looking for ways to make large language models (LLMs) work in the context of their product, domain or organizational knowledge, we're seeing a <strong>rush to fine-tune LLMs</strong>. While fine-tuning an LLM can be a powerful tool to gain more task-specificity for a use case, in many cases it’s not needed. One of the most common cases of a misguided rush to fine-tuning is about making an LLM-backed application aware of specific knowledge and facts or an organization's codebases. In the vast majority of these cases, using a form of <a href=\"\"https://www.thoughtworks.com/radar/techniques/retrieval-augmented-generation-rag\"\">retrieval-augmented generation (RAG)</a> offers a better solution and a better cost-benefit ratio. Fine-tuning requires considerable computational resources and expertise and introduces even more challenges around sensitive and proprietary data than RAG. There is also a risk of underfitting, when you don't have enough data available for fine-tuning, or, less frequently, overfitting, when you have too much data and are therefore not hitting the right balance of task specificity that you need. Look closely at these trade-offs and consider the alternatives before you rush to fine-tune an LLM for your use case.</p>\""
  },
  {
    "name": "Web components for SSR web apps",
    "ring": "hold",
    "quadrant": "techniques",
    "isNew": "FALSE",
    "status": "no change",
    "description": "\"<p>With the adoption of frameworks like <a href=\"\"https://www.thoughtworks.com/radar/languages-and-frameworks/next-js\"\">Next.js</a> and <a href=\"\"https://www.thoughtworks.com/radar/languages-and-frameworks/htmx\"\">htmx</a>, we’re seeing more usage of server-side rendering (SSR). As a browser technology, it's not trivial to use <a href=\"\"https://www.thoughtworks.com/radar/platforms/web-components-standard\"\">web components</a> on the server. Frameworks have sprung up to make this easier, sometimes even using a browser engine, but the complexity is still there. Our developers find themselves needing workarounds and extra effort to order front-end components and server-side components. Worse than the developer experience is the user experience: page load performance is impacted when custom web components have to be loaded and hydrated in the browser, and even with pre-rendering and careful tweaking of the component, a \"\"flash of unstyled content\"\" or some layout shifting is all but unavoidable. As mentioned in the previous Radar, one of our teams had to move their design system away from the web components-based <a href=\"\"https://www.thoughtworks.com/radar/languages-and-frameworks/stencil\"\">Stencil</a> because of these issues. Recently, we received reports from another team that they ended up replacing server-side–generated components with browser-side components because of the development complexity. We caution against the use of <strong>web components for SSR web apps</strong>, even if supported by frameworks.</p>\""
  },
  {
    "name": "CloudEvents",
    "ring": "adopt",
    "quadrant": "platforms",
    "isNew": "FALSE",
    "status": "moved in",
    "description": "\"<p>Events are common mechanisms in event-driven architecture or <a href=\"\"https://www.thoughtworks.com/radar/techniques/serverless-architecture\"\">serverless</a> applications. However, producers or cloud providers tend to support them in different forms, which prevents interoperability across platforms and infrastructures. <strong><a href=\"\"https://cloudevents.io/\"\" target=\"\"_blank\"\" aria-label=\"\"CloudEvents. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">CloudEvents<span class=\"\"pop-out-icon\"\"></span></a></strong> is a specification for describing event data in common formats to provide interoperability across services, platforms and systems. It provides SDKs in multiple languages so you can embed the spec into your application or toolchain. Our teams use it not only for cross-cloud platform purposes but also for domain event specification, among other scenarios. CloudEvents is hosted by the <a href=\"\"https://www.cncf.io/\"\" target=\"\"_blank\"\" aria-label=\"\"Cloud Native Computing Foundation. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Cloud Native Computing Foundation<span class=\"\"pop-out-icon\"\"></span></a> (CNCF) and is now a graduated project. Our teams default to using CloudEvents for building event-driven architectures and for that reason we’re moving it to Adopt.</p>\""
  },
  {
    "name": "Arm in the cloud",
    "ring": "trial",
    "quadrant": "platforms",
    "isNew": "FALSE",
    "status": "no change",
    "description": "\"<p><a href=\"\"https://www.arm.com/markets/computing-infrastructure/cloud-computing\"\" target=\"\"_blank\"\" aria-label=\"\"Arm compute instances in the cloud. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Arm compute instances in the cloud<span class=\"\"pop-out-icon\"\"></span></a> have become increasingly popular in recent years due to their cost and energy efficiency compared to traditional x86-based instances. Many cloud providers now offer Arm-based instances, including <a href=\"\"https://aws.amazon.com/ec2/graviton/\"\" target=\"\"_blank\"\" aria-label=\"\"AWS. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">AWS<span class=\"\"pop-out-icon\"\"></span></a>, <a href=\"\"https://azure.microsoft.com/blog/azure-virtual-machines-with-ampere-altra-arm-based-processors-generally-available/\"\" target=\"\"_blank\"\" aria-label=\"\"Azure. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Azure<span class=\"\"pop-out-icon\"\"></span></a> and <a href=\"\"https://cloud.google.com/blog/products/compute/tau-t2a-is-first-compute-engine-vm-on-an-arm-chip\"\" target=\"\"_blank\"\" aria-label=\"\"GCP. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">GCP<span class=\"\"pop-out-icon\"\"></span></a>. The cost benefits of running <strong>Arm in the cloud</strong> can be particularly beneficial for businesses that run large workloads or need to scale. We’re seeing many teams of ours moving to Arm instances for workloads like JVM services and even databases (including RDS) without any change in the code and minimal changes in the build scripts. New cloud-based applications and systems increasingly default to Arm in the cloud. Based on our experiences, we recommend Arm compute instances for all workloads unless there are architecture-specific dependencies. The tooling to support multiple architectures, such as <a href=\"\"https://www.docker.com/blog/multi-arch-build-and-images-the-simple-way/\"\" target=\"\"_blank\"\" aria-label=\"\"multi-arch Docker images. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">multi-arch Docker images<span class=\"\"pop-out-icon\"\"></span></a>, also simplifies build and deploy workflows.</p>\""
  },
  {
    "name": "Azure Container Apps",
    "ring": "trial",
    "quadrant": "platforms",
    "isNew": "FALSE",
    "status": "moved in",
    "description": "\"<p><strong><a href=\"\"https://azure.microsoft.com/en-us/products/container-apps\"\" target=\"\"_blank\"\" aria-label=\"\"Azure Container Apps. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Azure Container Apps<span class=\"\"pop-out-icon\"\"></span></a></strong> is a managed <a href=\"\"https://www.thoughtworks.com/radar/platforms/kubernetes\"\">Kubernetes</a> application platform that streamlines the deployment of containerized workloads. In comparison to Azure Kubernetes Service (AKS), the operational and administrative burden of running containerized applications is reduced, but this comes at the expense of some flexibility and control, which is a trade-off teams need to consider. Another product in this area, Azure Container Instances, is usually too limited for production use. Our teams started using Azure Container Apps last year, when it was still in public preview, with good results, even when running large containers. Now that it is generally available, we’re considering it for more use cases. Both <a href=\"\"https://www.thoughtworks.com/radar/tools/dapr\"\">Dapr</a> and the <a href=\"\"https://www.thoughtworks.com/radar/tools/keda\"\">KEDA Autoscaler</a> are supported.</p>\""
  },
  {
    "name": "Azure OpenAI Service",
    "ring": "trial",
    "quadrant": "platforms",
    "isNew": "FALSE",
    "status": "moved in",
    "description": "\"<p><strong><a href=\"\"https://azure.microsoft.com/en-us/products/ai-services/openai-service/\"\" target=\"\"_blank\"\" aria-label=\"\"Azure OpenAI Service. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Azure OpenAI Service<span class=\"\"pop-out-icon\"\"></span></a></strong> provides access to OpenAI's GPT-4, GPT-35-Turbo, Embeddings, DALL-E model and more through a REST API, a Python SDK and web-based interface. The models can be adapted to tasks such as content generation, summarization, semantic search and translating natural language to code. Fine-tuning is also available via few-shot learning and the customization of hyperparameters. In comparison to OpenAI's own API, Azure OpenAI Service benefits from Azure's enterprise-grade security and compliance features, is available for more regions (although <a href=\"\"https://azure.microsoft.com/en-us/explore/global-infrastructure/products-by-region/?regions=all&amp;products=cognitive-services\"\" target=\"\"_blank\"\" aria-label=\"\"availability is limited. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">availability is limited<span class=\"\"pop-out-icon\"\"></span></a> for each of the larger geographic regions) and supports private networking, content filtering and manual model version control. For these reasons and our positive experience with it, we recommend that enterprises already using Azure consider using Azure OpenAI Service instead of the OpenAI API.</p>\""
  },
  {
    "name": "DataHub",
    "ring": "trial",
    "quadrant": "platforms",
    "isNew": "FALSE",
    "status": "no change",
    "description": "\"<p>When you build data products using <a href=\"\"https://www.thoughtworks.com/radar/techniques/data-product-thinking-for-fair-data\"\">data product thinking</a>, it's essential to consider data lineage, data discoverability and data governance. Our teams have found that <strong><a href=\"\"https://datahubproject.io/\"\" target=\"\"_blank\"\" aria-label=\"\"DataHub. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">DataHub<span class=\"\"pop-out-icon\"\"></span></a></strong> can provide particularly useful support here. Although earlier versions of DataHub required you to fork and manage the sync from the main product (if there was a need to update the metadata model), improvements in recent releases have introduced features that allow our teams to implement <a href=\"\"https://datahubproject.io/docs/metadata-models-custom/#the-future\"\" target=\"\"_blank\"\" aria-label=\"\"custom metadata models. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">custom metadata models<span class=\"\"pop-out-icon\"\"></span></a> with a plugin-based architecture. Another useful feature of DataHub is the robust end-to-end data lineage from source to processing to consumption. DataHub supports both push-based integration as well as pull-based lineage extraction that automatically crawls the technical metadata across data sources, schedulers, orchestrators (scanning the Airflow DAG), processing pipeline tasks and dashboards, to name a few. As an open-source option for a holistic data catalog, DataHub is emerging as a default choice for our teams.</p>\""
  },
  {
    "name": "Infrastructure orchestration platforms",
    "ring": "trial",
    "quadrant": "platforms",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p>In-house infrastructure orchestration codebases frequently become a time sink to maintain and troubleshoot. <strong>Infrastructure orchestration platforms</strong> are appearing, promising to standardize and productize various aspects of infrastructure code delivery and deployment workflows. These include build tools like <a href=\"\"https://www.thoughtworks.com/radar/tools/terragrunt\"\">Terragrunt</a> and <a href=\"\"https://terraspace.cloud/\"\" target=\"\"_blank\"\" aria-label=\"\"Terraspace. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Terraspace<span class=\"\"pop-out-icon\"\"></span></a>, services from IaC tool vendors such as <a href=\"\"https://developer.hashicorp.com/terraform/cloud-docs\"\" target=\"\"_blank\"\" aria-label=\"\"Terraform Cloud. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Terraform Cloud<span class=\"\"pop-out-icon\"\"></span></a> and <a href=\"\"https://www.pulumi.com/product/pulumi-cloud/\"\" target=\"\"_blank\"\" aria-label=\"\"Pulumi Cloud. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Pulumi Cloud<span class=\"\"pop-out-icon\"\"></span></a> as well as tool-agnostic platforms and services like <a href=\"\"https://www.env0.com/\"\" target=\"\"_blank\"\" aria-label=\"\"env0. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">env0<span class=\"\"pop-out-icon\"\"></span></a> and <a href=\"\"https://spacelift.io/\"\" target=\"\"_blank\"\" aria-label=\"\"Spacelift. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Spacelift<span class=\"\"pop-out-icon\"\"></span></a>. There is a rich ecosystem of Terraform-specific orchestration tools and services, often called <a href=\"\"https://itnext.io/spice-up-your-infrastructure-as-code-with-tacos-1a9c179e0783\"\" target=\"\"_blank\"\" aria-label=\"\"TACOS. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">TACOS<span class=\"\"pop-out-icon\"\"></span></a> (Terraform Automation and Collaboration Software), including <a href=\"\"https://www.runatlantis.io/\"\" target=\"\"_blank\"\" aria-label=\"\"Atlantis. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Atlantis<span class=\"\"pop-out-icon\"\"></span></a>, <a href=\"\"https://digger.dev/\"\" target=\"\"_blank\"\" aria-label=\"\"Digger. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Digger<span class=\"\"pop-out-icon\"\"></span></a>, <a href=\"\"https://www.scalr.com/\"\" target=\"\"_blank\"\" aria-label=\"\"Scalr. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Scalr<span class=\"\"pop-out-icon\"\"></span></a>, <a href=\"\"https://terramate.io/\"\" target=\"\"_blank\"\" aria-label=\"\"Terramate. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Terramate<span class=\"\"pop-out-icon\"\"></span></a> and <a href=\"\"https://terrateam.io/\"\" target=\"\"_blank\"\" aria-label=\"\"Terrateam. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Terrateam<span class=\"\"pop-out-icon\"\"></span></a>. Each of these platforms enables different workflows, including <a href=\"\"https://www.thoughtworks.com/radar/techniques/gitops\"\">GitOps</a>, <a href=\"\"https://www.thoughtworks.com/radar/techniques/continuous-delivery-cd\"\">Continuous Delivery</a> and compliance as code. We welcome the growth of solutions in this space. We recommend infrastructure and platform engineering teams explore how to use them to reduce the amount of non-differentiating custom code they need to develop and maintain their infrastructure. Standardization of how infrastructure code is structured, shared, delivered and deployed should also create opportunities for the emergence of an ecosystem of compatible tools for testing, measuring and monitoring infrastructure.</p>\""
  },
  {
    "name": "Pulumi",
    "ring": "trial",
    "quadrant": "platforms",
    "isNew": "FALSE",
    "status": "no change",
    "description": "\"<p>Tooling in the infrastructure-as-code space continues to evolve, and we’re pleased to see that <strong><a href=\"\"https://pulumi.io/\"\" target=\"\"_blank\"\" aria-label=\"\"Pulumi. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Pulumi<span class=\"\"pop-out-icon\"\"></span></a></strong> is no exception to this trend. The platform recently added support for Java and YAML, for <a href=\"\"https://www.pulumi.com/docs/pulumi-cloud/deployments/\"\" target=\"\"_blank\"\" aria-label=\"\"managing infrastructure at scale. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">managing infrastructure at scale<span class=\"\"pop-out-icon\"\"></span></a> as well as for a multitude of cloud <a href=\"\"https://www.pulumi.com/product/esc/\"\" target=\"\"_blank\"\" aria-label=\"\"configurations and integrations. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">configurations and integrations<span class=\"\"pop-out-icon\"\"></span></a>, making the platform even more compelling. For our teams, it’s still the main alternative to <a href=\"\"https://www.thoughtworks.com/radar/tools/terraform\"\">Terraform</a> for developing code for multiple cloud platforms.</p>\""
  },
  {
    "name": "Rancher Desktop",
    "ring": "trial",
    "quadrant": "platforms",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p>Changes in licensing for <a href=\"\"https://www.thoughtworks.com/radar/platforms/docker\"\">Docker</a> Desktop have left us scrambling for alternatives for running a fleet of containers on a developer's local laptop. Recently we've had good success with <strong><a href=\"\"https://rancherdesktop.io/\"\" target=\"\"_blank\"\" aria-label=\"\"Rancher Desktop. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Rancher Desktop<span class=\"\"pop-out-icon\"\"></span></a></strong>. This free and open-source app is relatively easy to download and install for Apple, Windows or Linux machines and provides a handy local <a href=\"\"https://www.thoughtworks.com/radar/platforms/kubernetes\"\">Kubernetes</a> cluster with a GUI for configuration and monitoring. Although <a href=\"\"https://www.thoughtworks.com/radar/platforms/colima\"\">Colima</a> has become our Docker Desktop alternative of choice, it’s primarily a CLI tool. In contrast, Rancher Desktop will appeal to those who don't want to give up the graphical interface that Docker Desktop provides. Like Colima, Rancher Desktop allows you to choose between dockerd or containerd as the underlying container run time. The choice of direct containerd frees you from the DockerCLI, but the dockerd option provides compatibility with other tools that depend on it to communicate with the run-time daemon.</p>\""
  },
  {
    "name": "Weights & Biases",
    "ring": "trial",
    "quadrant": "platforms",
    "isNew": "FALSE",
    "status": "no change",
    "description": "\"<p><strong><a href=\"\"https://wandb.ai/\"\" target=\"\"_blank\"\" aria-label=\"\"Weights &amp; Biases. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Weights &amp; Biases<span class=\"\"pop-out-icon\"\"></span></a></strong> is a machine learning (ML) platform for building models faster through experiment tracking, data set versioning, visualizing model performance and model management. It can be integrated with existing ML code to get live metrics, terminal logs and system statistics streamed to the dashboard for further analysis. Recently, Weights &amp; Biases has expanded into LLM observability with <a href=\"\"https://wandb.ai/site/traces\"\" target=\"\"_blank\"\" aria-label=\"\"Traces. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Traces<span class=\"\"pop-out-icon\"\"></span></a>. Traces visualizes the execution flow of prompt chains as well as intermediate inputs/outputs and provides metadata around chain execution (such as tokens used and start and end time). Our teams find it useful for debugging and getting a greater understanding of the chain architecture.</p>\""
  },
  {
    "name": "Bun",
    "ring": "assess",
    "quadrant": "platforms",
    "isNew": "FALSE",
    "status": "no change",
    "description": "\"<p><strong><a href=\"\"https://github.com/oven-sh/bun\"\" target=\"\"_blank\"\" aria-label=\"\"Bun. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Bun<span class=\"\"pop-out-icon\"\"></span></a></strong> is a new JavaScript run time, similar to <a href=\"\"https://www.thoughtworks.com/radar/platforms/node-js\"\">Node.js</a> or <a href=\"\"https://www.thoughtworks.com/radar/platforms/deno\"\">Deno</a>. Unlike Node.js or Deno, however, Bun is built using WebKit's JavaScriptCore instead of Chrome's V8 engine. Designed as a drop-in replacement for Node.js, Bun is a single binary (written in <a href=\"\"https://www.thoughtworks.com/radar/languages-and-frameworks/zig\"\">Zig</a>) that acts as a bundler, transpiler and package manager for JavaScript and <a href=\"\"https://www.thoughtworks.com/radar/languages-and-frameworks/typescript\"\">TypeScript</a> applications. Since our last volume, Bun has gone from beta into a stable 1.0 release. Bun has been built from the ground up with several optimizations — including fast startup, improved server-side rendering and a much faster alternative package manager — and we encourage you to assess it for your JavaScript run-time engine.</p>\""
  },
  {
    "name": "Chronosphere",
    "ring": "assess",
    "quadrant": "platforms",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p>When managing distributed architectures, accounting for the cost of sorting, indexing and accessing data is as critical as observability. <strong><a href=\"\"https://chronosphere.io/\"\" target=\"\"_blank\"\" aria-label=\"\"Chronosphere. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Chronosphere<span class=\"\"pop-out-icon\"\"></span></a></strong> takes a unique approach to cost management, tracking the use of observability data so that organizations can consider the cost-value trade-offs of various metrics. With the help of the <a href=\"\"https://chronosphere.io/learn/introducing-the-metrics-usage-analyzer/\"\" target=\"\"_blank\"\" aria-label=\"\"Metrics Usage Analyzer. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Metrics Usage Analyzer<span class=\"\"pop-out-icon\"\"></span></a>, part of the <a href=\"\"https://chronosphere.io/platform/control-plane/#how-it-works\"\" target=\"\"_blank\"\" aria-label=\"\"Chronosphere Control Plane. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Chronosphere Control Plane<span class=\"\"pop-out-icon\"\"></span></a>, teams can identify and exclude metrics they rarely (or never) use, thus yielding significant cost savings by reducing the amount of data organizations have to comb through. Given these advantages, as well as the ability of Chronosphere to match the functionality of other observability tools for cloud-hosted solutions, we believe it to be a compelling option for organizations to look into.</p>\""
  },
  {
    "name": "DataOS",
    "ring": "assess",
    "quadrant": "platforms",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p>With <a href=\"\"https://www.thoughtworks.com/radar/techniques/data-mesh\"\">data mesh</a> adoption on the rise, our teams have been on the lookout for data platforms that treat <a href=\"\"https://www.thoughtworks.com/radar/techniques/data-product-thinking-for-fair-data\"\">data products</a> as a first-class entity. <strong><a href=\"\"https://themoderndatacompany.com/dataos/\"\" target=\"\"_blank\"\" aria-label=\"\"DataOS. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">DataOS<span class=\"\"pop-out-icon\"\"></span></a></strong> is one such product. It provides end-to-end lifecycle management to design, build, deploy and evolve data products. It offers standardized <a href=\"\"https://dataos.info/resources/bundle/#structure-of-a-bundle-yaml-manifest\"\" target=\"\"_blank\"\" aria-label=\"\"declarative specs. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">declarative specs<span class=\"\"pop-out-icon\"\"></span></a> written in YAML that abstract the low-level complexity of infrastructure setup and allow developers to define the data products easily via CLI/API. It supports <a href=\"\"https://dataos.info/resources/policy/#access-policy\"\" target=\"\"_blank\"\" aria-label=\"\"access control policies. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">access control policies<span class=\"\"pop-out-icon\"\"></span></a> with <a href=\"\"https://dataos.info/resources/policy/understanding_abac_pdp_and_pep/\"\" target=\"\"_blank\"\" aria-label=\"\"ABAC. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">ABAC<span class=\"\"pop-out-icon\"\"></span></a> and <a href=\"\"https://dataos.info/resources/policy/#data-policy\"\" target=\"\"_blank\"\" aria-label=\"\"data policies. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">data policies<span class=\"\"pop-out-icon\"\"></span></a> for filtering and masking data. Another notable feature is its ability to <a href=\"\"https://dataos.info/resources/depot/\"\" target=\"\"_blank\"\" aria-label=\"\"federate data. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">federate data<span class=\"\"pop-out-icon\"\"></span></a> across a variety of data sources, which reduces data duplication and the movement of data to a central place. DataOS fits best for greenfield scenarios where it does the heavy lifting since it provides an out-of-the-box solution for data governance, data discoverability, infrastructure resource management and observability. For brownfield scenarios, the ability to <a href=\"\"https://dataos.info/resources/operator/\"\" target=\"\"_blank\"\" aria-label=\"\"orchestrate resources outside of DataOS. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">orchestrate resources outside of DataOS<span class=\"\"pop-out-icon\"\"></span></a> (for example, data stacks like Databricks) is in its nascent stage and still evolving. If your ecosystem doesn’t exert a lot of opinion on data tooling, DataOS is a good way to expedite your journey for building, deploying and consuming data products in an end-to-end fashion.</p>\""
  },
  {
    "name": "Dify",
    "ring": "assess",
    "quadrant": "platforms",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://github.com/langgenius/dify\"\" target=\"\"_blank\"\" aria-label=\"\"Dify. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Dify<span class=\"\"pop-out-icon\"\"></span></a></strong> is a UI-driven platform for developing large language model (LLM) applications that makes prototyping them even more accessible. It supports the development of chat and text generation apps with prompt templates. Additionally, Dify supports <a href=\"\"https://www.thoughtworks.com/radar/techniques/retrieval-augmented-generation-rag\"\">retrieval-augmented generation (RAG)</a> with imported data sets and can work with multiple models. We’re excited about this category of applications. Based on our experience, however, Dify is not quite ready for prime time yet, because some features are buggy or don't seem fully fleshed out. At the moment, though, we’re not aware of a competitor that is better.</p>\""
  },
  {
    "name": "Elasticsearch Relevance Engine",
    "ring": "assess",
    "quadrant": "platforms",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p>Although vector databases have been gaining popularity for <a href=\"\"https://www.thoughtworks.com/radar/techniques/retrieval-augmented-generation-rag\"\">retrieval-augmented generation (RAG)</a> use cases, <a href=\"\"https://arxiv.org/abs/2004.13969\"\" target=\"\"_blank\"\" aria-label=\"\"research. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">research<span class=\"\"pop-out-icon\"\"></span></a> and <a href=\"\"https://engineering.atspotify.com/2022/03/introducing-natural-language-search-for-podcast-episodes/\"\" target=\"\"_blank\"\" aria-label=\"\"experience reports. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">experience reports<span class=\"\"pop-out-icon\"\"></span></a> suggest combining traditional full-text search with vector search (into a hybrid search) can yield superior results. Through <strong><a href=\"\"https://www.elastic.co/elasticsearch/elasticsearch-relevance-engine\"\" target=\"\"_blank\"\" aria-label=\"\"Elasticsearch Relevance Engine (ESRE). This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Elasticsearch Relevance Engine (ESRE)<span class=\"\"pop-out-icon\"\"></span></a></strong>, the well-established full-text search platform <a href=\"\"https://www.thoughtworks.com/radar/platforms/elastic-search\"\">Elasticsearch</a> supports built-in and custom embedding models, vector search and hybrid search with ranking mechanisms such as Reciprocal Rank Fusion. Even though this space is still maturing, in our experience, using these ESRE features along with the traditional filtering, sorting and ranking capabilities that come with Elasticsearch has yielded promising results, suggesting that established search platforms that support semantic search are not to be passed over.</p>\""
  },
  {
    "name": "FOCUS",
    "ring": "assess",
    "quadrant": "platforms",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p>Cloud and SaaS billing data can be complex, inconsistent among providers and difficult to understand. The FinOps Open Cost and Usage Specification (<strong><a href=\"\"https://focus.finops.org/\"\" target=\"\"_blank\"\" aria-label=\"\"FOCUS. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">FOCUS<span class=\"\"pop-out-icon\"\"></span></a></strong>) aims to reduce this friction with a spec containing a set of terminologies (aligned with the <a href=\"\"https://www.finops.org/framework/\"\" target=\"\"_blank\"\" aria-label=\"\"FinOps framework. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">FinOps framework<span class=\"\"pop-out-icon\"\"></span></a>), a schema and a minimum set of requirements for billing data. The spec is intended to support use cases common to a variety of FinOps practitioners. Although still in the early stages of development and adoption, it’s worth watching because, with growing industry adoption, FOCUS will make it easier for platforms and end users to get a holistic view of cloud spend across a long tail of cloud and SaaS providers.</p>\""
  },
  {
    "name": "Gemini Nano",
    "ring": "assess",
    "quadrant": "platforms",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p>Google's Gemini is a family of foundational LLMs designed to run on a wide range of hardware, from data centers to mobile phones. <strong><a href=\"\"https://ai.google.dev/tutorials/android_aicore\"\" target=\"\"_blank\"\" aria-label=\"\"Gemini Nano. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Gemini Nano<span class=\"\"pop-out-icon\"\"></span></a></strong> has been specifically optimized and scaled down to run on mobile silicon accelerators. It enables capabilities such as high-quality text summarization, contextual smart replies and advanced grammar correction. For example, the language understanding of Gemini Nano enables the Pixel 8 Pro to summarize content in the Recorder app. Running on-device removes many of the latency and privacy concerns associated with cloud-based systems and allows the features to work without network connection. <a href=\"\"https://developer.android.com/ml/aicore\"\" target=\"\"_blank\"\" aria-label=\"\"Android AICore. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Android AICore<span class=\"\"pop-out-icon\"\"></span></a> simplifies the integration of the model into Android apps, but only a few devices are supported at the time of writing.</p>\""
  },
  {
    "name": "HyperDX",
    "ring": "assess",
    "quadrant": "platforms",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://github.com/hyperdxio/hyperdx\"\" target=\"\"_blank\"\" aria-label=\"\"HyperDX. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">HyperDX<span class=\"\"pop-out-icon\"\"></span></a></strong> is an open-source observability platform that unifies all three pillars of observability: logs, metrics and tracing. With it, you can correlate end-to-end and go from browser session replay to logs and traces in just a few clicks. The platform leverages <a href=\"\"https://www.thoughtworks.com/radar/platforms/clickhouse\"\">ClickHouse</a> as a central data store for all telemetry data, and it scales to aggregate log patterns and condense billions of events into distinctive clusters. Although you can choose from several observability platforms, we want to highlight HyperDX for its unified developer experience.</p>\""
  },
  {
    "name": "IcePanel",
    "ring": "assess",
    "quadrant": "platforms",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://icepanel.io/\"\" target=\"\"_blank\"\" aria-label=\"\"IcePanel. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">IcePanel<span class=\"\"pop-out-icon\"\"></span></a></strong> facilitates collaborative architectural modeling and diagramming using the <a href=\"\"https://c4model.com/\"\" target=\"\"_blank\"\" aria-label=\"\"C4 model. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">C4 model<span class=\"\"pop-out-icon\"\"></span></a>, which allows technical and business stakeholders to zoom in to the level of technical detail they need. It supports modeling architecture objects whose metadata and connections can be reused across diagrams, along with the visualization of <a href=\"\"https://docs.icepanel.io/advanced-features/flows/\"\" target=\"\"_blank\"\" aria-label=\"\"flows. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">flows<span class=\"\"pop-out-icon\"\"></span></a> between those objects. Versioning and tagging allows collaborators to model different architecture states (e.g., as-is versus to-be) and track user-defined classifications of various parts of the architecture. We’re keeping an eye on IcePanel for its potential to improve architecture collaboration, particularly for organizations with complex architectures. For an alternative that better supports diagrams as code, check out <a href=\"\"https://structurizr.com/\"\" target=\"\"_blank\"\" aria-label=\"\"Structurizr. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Structurizr<span class=\"\"pop-out-icon\"\"></span></a>.</p>\""
  },
  {
    "name": "Langfuse",
    "ring": "assess",
    "quadrant": "platforms",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://langfuse.com/\"\" target=\"\"_blank\"\" aria-label=\"\"Langfuse. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Langfuse<span class=\"\"pop-out-icon\"\"></span></a></strong> is an engineering platform for observability, testing and monitoring large language model (LLM) applications. Its SDKs support Python, JavaScript and TypeScript, OpenAI, <a href=\"\"https://www.thoughtworks.com/radar/languages-and-frameworks/langchain\"\">LangChain</a> and <a href=\"\"https://www.thoughtworks.com/radar/tools/litellm\"\">LiteLLM</a> among other languages and frameworks. You can self-host the open-source version or use it as a paid cloud service. Our teams have had a positive experience, particularly in debugging complex LLM chains, analyzing completions and monitoring key metrics such as cost and latency across users, sessions, geographies, features and model versions. If you’re looking to build data-driven LLM applications, Langfuse is a good option to consider.</p>\""
  },
  {
    "name": "Qdrant",
    "ring": "assess",
    "quadrant": "platforms",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://github.com/qdrant/qdrant\"\" target=\"\"_blank\"\" aria-label=\"\"Qdrant. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Qdrant<span class=\"\"pop-out-icon\"\"></span></a></strong> is an open-source vector database written in <a href=\"\"https://www.thoughtworks.com/radar/languages-and-frameworks/rust\"\">Rust</a>. In the September 2023 edition of the Radar, we talked about <a href=\"\"https://www.thoughtworks.com/radar/platforms/pgvector\"\">pgvector</a>, a PostgreSQL extension for vector search. However, if you have to scale the vector database horizontally across nodes, we encourage you to assess Qdrant. It has built-in single instruction, multiple data (SIMD) acceleration support for improved search performance, and it helps you associate JSON payloads with vectors.</p>\""
  },
  {
    "name": "RISC-V for embedded",
    "ring": "assess",
    "quadrant": "platforms",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p>While the Arm architecture continues to expand its impact — we've updated our assessment of <a href=\"\"https://www.thoughtworks.com/radar/platforms/arm-in-the-cloud\"\">Arm in the cloud</a> in this edition — interest in the newer and less established <a href=\"\"https://riscv.org/\"\" target=\"\"_blank\"\" aria-label=\"\"RISC-V. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">RISC-V<span class=\"\"pop-out-icon\"\"></span></a> architecture also grows. RISC-V doesn’t bring breakthroughs in performance or efficiency — in fact, its per-watt performance is similar to Arm’s, and it can't quite compete on absolute performance — but it’s open source, modular and not tied to a single company. This makes it an attractive proposition for embedded systems, where the cost of licensing proprietary architectures is a significant concern. This is also why the field of <strong>RISC-V for embedded</strong> is maturing, and several companies, including <a href=\"\"https://www.sifive.com/about\"\" target=\"\"_blank\"\" aria-label=\"\"SiFive. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">SiFive<span class=\"\"pop-out-icon\"\"></span></a> and <a href=\"\"https://www.espressif.com/en/media_overview/news/risc-v-with-esp32-c3\"\" target=\"\"_blank\"\" aria-label=\"\"espressif. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">espressif<span class=\"\"pop-out-icon\"\"></span></a>, are offering development boards and SoCs for a wide range of applications. Microcontrollers and microprocessors capable of running the Linux kernel are available today, along with the corresponding software stack and toolchains. We’re keeping an eye on this space and expect to see more adoption in the coming years.</p>\""
  },
  {
    "name": "Tigerbeetle",
    "ring": "assess",
    "quadrant": "platforms",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://github.com/tigerbeetle/tigerbeetle\"\" target=\"\"_blank\"\" aria-label=\"\"Tigerbeetle. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Tigerbeetle<span class=\"\"pop-out-icon\"\"></span></a></strong> is an open-source distributed database for financial accounting. Unlike other databases, it’s designed to be a domain-specific state machine for safety and performance. The state from one node in the cluster is replicated in a deterministic order to other nodes via the <a href=\"\"https://pmg.csail.mit.edu/papers/vr-revisited.pdf\"\" target=\"\"_blank\"\" aria-label=\"\"Viewstamped Replication. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Viewstamped Replication<span class=\"\"pop-out-icon\"\"></span></a> consensus protocol. We quite like the <a href=\"\"https://github.com/tigerbeetle/tigerbeetle/blob/main/docs/DESIGN.md\"\" target=\"\"_blank\"\" aria-label=\"\"design decisions. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">design decisions<span class=\"\"pop-out-icon\"\"></span></a> behind Tigerbeetle to implement double-entry bookkeeping with strict serializability guarantees.</p>\""
  },
  {
    "name": "WebTransport",
    "ring": "assess",
    "quadrant": "platforms",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://developer.chrome.com/docs/capabilities/web-apis/webtransport\"\" target=\"\"_blank\"\" aria-label=\"\"WebTransport. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">WebTransport<span class=\"\"pop-out-icon\"\"></span></a></strong> is a protocol that builds on top of HTTP/3 and offers bidirectional communication between servers and apps. WebTransport offers several benefits over its predecessor, WebSockets, including faster connections, lower latency and the ability to handle both reliable and ordered data streams as well as unordered ones (such as UDP). It can handle multiple streams in the same connection without head-of-line blocking, allowing for more efficient communication in complex applications. Overall, WebTransport is suitable for a wide range of use cases, including real-time web apps, streaming media and Internet of Things (IoT) data communications. Even though WebTransport is still in the early stages — support across browsers is gradually maturing, with popular libraries such as socket.io <a href=\"\"https://socket.io/get-started/webtransport\"\" target=\"\"_blank\"\" aria-label=\"\"adding support for WebTransport. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">adding support for WebTransport<span class=\"\"pop-out-icon\"\"></span></a> — our teams are currently assessing its potential for real-time IoT apps.</p>\""
  },
  {
    "name": "Zarf",
    "ring": "assess",
    "quadrant": "platforms",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://github.com/defenseunicorns/zarf\"\" target=\"\"_blank\"\" aria-label=\"\"Zarf. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Zarf<span class=\"\"pop-out-icon\"\"></span></a></strong> is a declarative package manager for offline and semi-connected <a href=\"\"https://www.thoughtworks.com/radar/platforms/kubernetes\"\">Kubernetes</a> environments. With Zarf, you can build and configure applications while connected to the internet; once created, you can package and ship to a disconnected environment for deployment. As a standalone tool, Zarf packs several useful features, including automatic <a href=\"\"https://www.thoughtworks.com/radar/techniques/software-bill-of-materials\"\">Software Bill of Materials (SBOM)</a> generation, built-in <a href=\"\"https://www.thoughtworks.com/radar/platforms/docker\"\">Docker</a> registry, Gitea and K9s dashboards to manage clusters from the terminal. <a href=\"\"https://www.itopstimes.com/contain/air-gap-kubernetes-considerations-for-running-cloud-native-applications-without-the-cloud/\"\" target=\"\"_blank\"\" aria-label=\"\"Air-gap software delivery. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Air-gap software delivery<span class=\"\"pop-out-icon\"\"></span></a> for cloud-native applications has its challenges; Zarf addresses most of them.</p>\""
  },
  {
    "name": "ZITADEL",
    "ring": "assess",
    "quadrant": "platforms",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://github.com/zitadel/zitadel\"\" target=\"\"_blank\"\" aria-label=\"\"ZITADEL. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">ZITADEL<span class=\"\"pop-out-icon\"\"></span></a></strong> is an open-source identity and user management tool, and an alternative to <a href=\"\"https://www.thoughtworks.com/radar/platforms/keycloak\"\">Keycloak</a>. It’s lightweight (written in Golang), has flexible deployment options and is easy to configure and manage. It’s also multi-tenant, offers comprehensive features for building secure and scalable authentication systems, particularly for B2B applications, and has built-in security features like multi-factor authentication and audit trails. By using ZITADEL, developers can reduce development time, enhance application security and achieve scalability for growing user bases. If you're looking for a user-friendly, secure and open-source tool for user management, ZITADEL is a strong contender.</p>\""
  },
  {
    "name": "Conan",
    "ring": "adopt",
    "quadrant": "tools",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://conan.io/\"\" target=\"\"_blank\"\" aria-label=\"\"Conan. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Conan<span class=\"\"pop-out-icon\"\"></span></a></strong> is an open-source dependency management tool for C/C++ applications. It provides an intuitive interface for defining, fetching and managing dependencies which makes it easy for developers to integrate third-party libraries into their projects. Conan works across all major operating systems and can target a variety of platforms, including servers and desktop, mobile and embedded devices. It can also be used for building and publishing C/C++ libraries and packages. The packages can be shared across teams via JFrog Artifactory servers. By leveraging prebuilt binaries, it significantly reduces build times, especially for hefty dependencies. Conan integrates with popular build systems like CMake and also has a Python SDK for extending the build system for tasks like signing. In our experience, Conan translates to improved build reproducibility across environments and faster development cycles. The resulting codebases are cleaner and easier to maintain, a major win for large-scale projects. If you're wrestling with dependency management in your C or C++ projects, Conan is a must-consider tool to boost your development efficiency.</p>\""
  },
  {
    "name": "Kaniko",
    "ring": "adopt",
    "quadrant": "tools",
    "isNew": "FALSE",
    "status": "moved in",
    "description": "\"<p>We added <strong><a href=\"\"https://github.com/GoogleContainerTools/kaniko\"\" target=\"\"_blank\"\" aria-label=\"\"Kaniko. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Kaniko<span class=\"\"pop-out-icon\"\"></span></a></strong> to the Radar in October 2022, shortly after <a href=\"\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md#dockershim-removed-from-kubelet\"\" target=\"\"_blank\"\" aria-label=\"\"Kubernetes moved on from supporting Docker. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Kubernetes moved on from supporting Docker<span class=\"\"pop-out-icon\"\"></span></a>, highlighting at the time the trend away from Docker as the default to build container images within container-based pipelines. Since then, we've expanded our experience with Kaniko across different pipelines' tooling and configurations. Our teams appreciate its flexibility and performance which is why we’re moving it to Adopt, highlighting Kaniko as the default tool in this space.</p>\""
  },
  {
    "name": "Karpenter",
    "ring": "adopt",
    "quadrant": "tools",
    "isNew": "FALSE",
    "status": "moved in",
    "description": "\"<p>One of the fundamental capabilities of <a href=\"\"https://www.thoughtworks.com/radar/platforms/kubernetes\"\">Kubernetes</a> is horizontal autoscaling: its ability to launch new pods when additional capacity is needed and shut them down when loads decrease. However, this only works if the nodes needed to host the pods already exist. <a href=\"\"https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler\"\" target=\"\"_blank\"\" aria-label=\"\"Cluster Autoscaler. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Cluster Autoscaler<span class=\"\"pop-out-icon\"\"></span></a> can do some rudimentary cluster expansion triggered by pod failures, but it has limited flexibility; <strong><a href=\"\"https://karpenter.sh/\"\" target=\"\"_blank\"\" aria-label=\"\"Karpenter. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Karpenter<span class=\"\"pop-out-icon\"\"></span></a></strong>, however, is a smarter, open-source <a href=\"\"https://www.thoughtworks.com/radar/tools/kubernetes-operators\"\">Kubernetes Operator</a> node autoscaler: it analyzes current workloads and pod scheduling constraints, selects an appropriate instance type and then starts or stops it as needed. Karpenter is an operator in the spirit of tools such as <a href=\"\"https://www.thoughtworks.com/radar/tools/crossplane\"\">Crossplane</a> that can provision cloud resources outside the cluster. Even though Karpenter was originally developed by AWS for EKS, it’s becoming the default node autoprovisioner across all cloud Kubernetes service providers, and Azure recently started supporting Karpenter with <a href=\"\"https://github.com/Azure/karpenter-provider-azure\"\" target=\"\"_blank\"\" aria-label=\"\"AKS Karpenter Provider. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">AKS Karpenter Provider<span class=\"\"pop-out-icon\"\"></span></a>.</p>\""
  },
  {
    "name": "42Crunch API Conformance Scan",
    "ring": "trial",
    "quadrant": "tools",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://42crunch.com/api-conformance-scan/\"\" target=\"\"_blank\"\" aria-label=\"\"42Crunch API Conformance Scan. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">42Crunch API Conformance Scan<span class=\"\"pop-out-icon\"\"></span></a></strong> is a dynamic testing tool designed to identify discrepancies between your API's documented behavior and its actual implementation. This tool takes your API's spec definition in OpenAPI format, which outlines the expected functionalities and responses, and compares it to the API's actual behavior. By generating real traffic and interacting with live endpoints, the tool can identify any discrepancies between what the API promises and what it delivers. This translates into several benefits for development teams. For example, it catches inconsistencies early in development, saving time and preventing issues from reaching production. The tool also helps improve API quality and security by identifying potential vulnerabilities arising from deviations from the documented behavior. Overall, API Scan helps you assess the security posture of your APIs by identifying problems such as weak authentication protocols, insecure data handling practices and insufficient input validation. It provides detailed reports highlighting any issues found, along with recommendations for remediation.</p>\""
  },
  {
    "name": "actions-runner-controller",
    "ring": "trial",
    "quadrant": "tools",
    "isNew": "FALSE",
    "status": "moved in",
    "description": "\"<p><strong><a href=\"\"https://github.com/actions-runner-controller/actions-runner-controller\"\" target=\"\"_blank\"\" aria-label=\"\"actions-runner-controller. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">actions-runner-controller<span class=\"\"pop-out-icon\"\"></span></a></strong> is a Kubernetes <a href=\"\"https://kubernetes.io/docs/concepts/architecture/controller/\"\" target=\"\"_blank\"\" aria-label=\"\"controller. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">controller<span class=\"\"pop-out-icon\"\"></span></a> that operates <a href=\"\"https://docs.github.com/en/actions/hosting-your-own-runners\"\" target=\"\"_blank\"\" aria-label=\"\"self-hosted runners. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">self-hosted runners<span class=\"\"pop-out-icon\"\"></span></a> for <a href=\"\"https://www.thoughtworks.com/radar/platforms/github-actions\"\">GitHub Actions</a>. Self-hosted runners are helpful in scenarios where the job that GitHub Actions runs needs to access resources that are either not accessible to GitHub cloud runners or have specific operating system and environmental requirements that are different from what GitHub provides. In such scenarios where the team uses Kubernetes clusters, actions-runner-controller orchestrates and scales these runners. Our teams like its ability to scale runners based on the number of workflows running in a given repository, organization, enterprise or Kubernetes cluster, as well as its ability to handle both Linux and Windows runners.</p>\""
  },
  {
    "name": "Android Emulator Container",
    "ring": "trial",
    "quadrant": "tools",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://github.com/google/android-emulator-container-scripts\"\" target=\"\"_blank\"\" aria-label=\"\"Android Emulator Containers. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Android Emulator Containers<span class=\"\"pop-out-icon\"\"></span></a></strong> streamline Android app testing by eliminating the complexities arising from OS compatibility issues and system dependencies as well as from setting up emulators for multiple Android versions. Traditionally, this complexity led to extra effort or teams foregoing automated testing completely, which, in turn, resulted in slower development and testing cycles. Android Emulator Containers simplify this process, allowing seamless integration into CI pipelines for automated testing. Our teams utilize these containers primarily for <a href=\"\"https://developer.android.com/training/testing/instrumented-tests\"\" target=\"\"_blank\"\" aria-label=\"\"instrumented tests. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">instrumented tests<span class=\"\"pop-out-icon\"\"></span></a>, which are automatically executed with each commit to provide instantaneous feedback to developers. Additionally, we leverage Android Emulator Containers for running nightly end-to-end tests as well.</p>\""
  },
  {
    "name": "AWS CUDOS",
    "ring": "trial",
    "quadrant": "tools",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p>Our advice has always been to monitor costs as a <a href=\"\"https://www.thoughtworks.com/radar/techniques/run-cost-as-architecture-fitness-function\"\">fitness function</a>. Cloud providers offer a variety of services to monitor cloud spend such as <a href=\"\"https://aws.amazon.com/aws-cost-management/aws-cost-explorer/\"\" target=\"\"_blank\"\" aria-label=\"\"AWS Cost Explorer. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">AWS Cost Explorer<span class=\"\"pop-out-icon\"\"></span></a> or <a href=\"\"https://cloud.google.com/billing/docs/how-to/finops-hub\"\" target=\"\"_blank\"\" aria-label=\"\"Google Cloud FinOps Hub. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Google Cloud FinOps Hub<span class=\"\"pop-out-icon\"\"></span></a>. In the AWS ecosystem, our teams use the <a href=\"\"https://aws.amazon.com/blogs/awsmarketplace/using-cudos-dashboard-visualizations-aws-marketplace-spend-visibility-optimization/\"\" target=\"\"_blank\"\" aria-label=\"\"CUDOS (Cost and Usage Dashboards Operations Solution). This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">CUDOS (Cost and Usage Dashboards Operations Solution)<span class=\"\"pop-out-icon\"\"></span></a> dashboards to monitor <a href=\"\"https://aws.amazon.com/marketplace/\"\" target=\"\"_blank\"\" aria-label=\"\"AWS Marketplace. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">AWS Marketplace<span class=\"\"pop-out-icon\"\"></span></a> spend segregated by the business departments or legal entities in a large parent organization. This dashboard provides comprehensive cost and usage details, with resource-level granularity that helps optimize costs, track usage goals and achieve operational excellence.</p>\""
  },
  {
    "name": "aws-nuke",
    "ring": "trial",
    "quadrant": "tools",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://github.com/rebuy-de/aws-nuke\"\" target=\"\"_blank\"\" aria-label=\"\"aws-nuke. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">aws-nuke<span class=\"\"pop-out-icon\"\"></span></a></strong> is an open-source tool that tackles the common challenge of unused resources accumulating in development and sandbox AWS accounts that can lead to cost inefficiencies. The tool identifies and removes all deletable resources within an AWS account or region with the exception of default or AWS-managed resources, essentially resetting the environment to a Day One state. It also offers customizable exclusion policies to ensure critical resources remain protected. We’ve used this tool for the default use case of cost optimization as well as for disaster recovery (DR) contexts with good results. By automating cleanup in development and sandbox environments, aws-nuke helps minimize unnecessary resource expenditure. It also facilitates efficient teardown of temporary DR infrastructure after drills or exercises. Although stable, aws-nuke is a very destructive tool and is not intended to be used in production environments. Always perform a dry run to confirm that essential resources won’t be deleted.</p>\""
  },
  {
    "name": "Bruno",
    "ring": "trial",
    "quadrant": "tools",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://github.com/usebruno/bruno\"\" target=\"\"_blank\"\" aria-label=\"\"Bruno. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Bruno<span class=\"\"pop-out-icon\"\"></span></a></strong> is an open-source desktop alternative to <a href=\"\"https://www.thoughtworks.com/radar/tools/postman\"\">Postman</a> and <a href=\"\"https://www.thoughtworks.com/radar/tools/insomnia\"\">Insomnia</a> for API testing, development and debugging. It stores your collections locally on the filesystem so you can use Git or a version control of your choice to collaborate. Several Thoughtworks teams are using Bruno and like its simple offline-only design.</p>\""
  },
  {
    "name": "Develocity",
    "ring": "trial",
    "quadrant": "tools",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://gradle.com/develocity/\"\" target=\"\"_blank\"\" aria-label=\"\"Develocity. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Develocity<span class=\"\"pop-out-icon\"\"></span></a></strong> (previously Gradle Enterprise) addresses the pain point of lengthy build and test cycles in large-scale software projects. It employs performance improvements such as build caching and predictive test selection to shorten developer feedback loops in both local and CI/CD environments. Our platform teams have found it useful for speeding up builds and tests, analyzing commands to determine what part of the workflow still needs to be optimized, identifying and troubleshooting flaky tests and performing analysis on the hardware used to run them.</p>\""
  },
  {
    "name": "GitHub Copilot",
    "ring": "trial",
    "quadrant": "tools",
    "isNew": "FALSE",
    "status": "no change",
    "description": "\"<p>While the AI coding assistance market is getting busier and busier, <strong><a href=\"\"https://github.com/features/copilot\"\" target=\"\"_blank\"\" aria-label=\"\"GitHub Copilot. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">GitHub Copilot<span class=\"\"pop-out-icon\"\"></span></a></strong> remains our default choice and is used by many of our teams. Since we last wrote about GitHub Copilot, the most interesting improvements came in the chat feature. For instance, it's no longer necessary to clutter the code with comments as prompts; instead, an inline chat helps you prompt without writing a comment. The inline chat can also change code, not just write new lines. You can now also significantly expand the context of the chat when asking questions about your code by using the <code>@workspace</code> tag. This allows you to ask questions about the entire codebase, not just the open files. You can expand this context even further with the <a href=\"\"https://github.blog/2024-02-27-github-copilot-enterprise-is-now-generally-available/\"\" target=\"\"_blank\"\" aria-label=\"\"Copilot Enterprise. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Copilot Enterprise<span class=\"\"pop-out-icon\"\"></span></a> version, which pulls in context from all repositories you host on GitHub. Finally, GitHub has started routing some chat requests to a more powerful GPT-4–based model, and availability of the chat in the popular Jetbrains IDEs is imminent (although still in private beta at the time of writing). These releases show that the pace of improvements in the space has not slowed down. If you tried a coding assistant last year and dismissed it, we recommend that you keep monitoring the features being released and give it another try.</p>\""
  },
  {
    "name": "Gradio",
    "ring": "trial",
    "quadrant": "tools",
    "isNew": "FALSE",
    "status": "moved in",
    "description": "\"<p><strong><a href=\"\"https://github.com/gradio-app/gradio\"\" target=\"\"_blank\"\" aria-label=\"\"Gradio. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Gradio<span class=\"\"pop-out-icon\"\"></span></a></strong> is an open-source Python library that facilitates the creation of interactive web-based interfaces for machine learning (ML) models. A graphical user interface on top of ML models provides a better understanding of the inputs, constraints and outputs by nontechnical audiences. Gradio has gained a lot of traction in the generative AI space, as it is one of the tools that makes generative models so accessible to experiment with. Usually, we put technologies into the Trial ring when we’ve seen them used in production at least once. Gradio's purpose and strength are experimentation and prototyping, and we’ve used it for that purpose many times. Recently, one of our teams even used it to help a client with live demonstrations at big events. We’re very happy with Gradio's capabilities for those use cases, and therefore move it into the Trial ring.</p>\""
  },
  {
    "name": "Gradle Version Catalog",
    "ring": "trial",
    "quadrant": "tools",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://docs.gradle.org/current/userguide/platforms.html\"\" target=\"\"_blank\"\" aria-label=\"\"Gradle version catalog. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Gradle version catalog<span class=\"\"pop-out-icon\"\"></span></a></strong> is a useful feature of the Gradle build tool that allows you to manage dependencies centrally in the build file. Our teams have found it especially useful with <a href=\"\"https://developer.android.com/build/migrate-to-catalogs\"\" target=\"\"_blank\"\" aria-label=\"\"Android multi-module. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Android multi-module<span class=\"\"pop-out-icon\"\"></span></a> projects. Instead of hardcoding dependency names and versions in individual build files and managing upgrades, you can create a central version catalog of these dependencies and then reference it in a type-safe way with Android Studio assistance.</p>\""
  },
  {
    "name": "Maestro",
    "ring": "trial",
    "quadrant": "tools",
    "isNew": "FALSE",
    "status": "moved in",
    "description": "\"<p><strong><a href=\"\"https://maestro.mobile.dev/\"\" target=\"\"_blank\"\" aria-label=\"\"Maestro. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Maestro<span class=\"\"pop-out-icon\"\"></span></a></strong> is extremely useful when testing complex flows in mobile applications. Our teams have found it easy to learn, easy to understand and easy to integrate into our development workflow. Maestro supports a range of mobile platforms including iOS, Android, <a href=\"\"https://www.thoughtworks.com/radar/languages-and-frameworks/react-native\"\">React Native</a> and <a href=\"\"https://www.thoughtworks.com/radar/languages-and-frameworks/flutter\"\">Flutter</a> apps. Its declarative YAML syntax simplifies the automation of complex mobile UI interactions. Based on the tool's evolution, marked by enhanced features like comprehensive iOS support and the introduction of tools like <a href=\"\"https://maestro.mobile.dev/getting-started/maestro-studio\"\" target=\"\"_blank\"\" aria-label=\"\"Maestro Studio. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Maestro Studio<span class=\"\"pop-out-icon\"\"></span></a> and <a href=\"\"https://cloud.mobile.dev/\"\" target=\"\"_blank\"\" aria-label=\"\"Maestro Cloud. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Maestro Cloud<span class=\"\"pop-out-icon\"\"></span></a>, we encourage anyone seeking to optimize their mobile application testing processes to give it a try.</p>\""
  },
  {
    "name": "Microsoft SBOM tool",
    "ring": "trial",
    "quadrant": "tools",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://github.com/microsoft/sbom-tool\"\" target=\"\"_blank\"\" aria-label=\"\"Microsoft SBOM tool. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Microsoft SBOM tool<span class=\"\"pop-out-icon\"\"></span></a></strong> is an open-source tool to generate <a href=\"\"https://spdx.dev/\"\" target=\"\"_blank\"\" aria-label=\"\"SPDX. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">SPDX<span class=\"\"pop-out-icon\"\"></span></a>-compatible Software Bill of Materials (SBOM). We have blipped about <a href=\"\"https://www.thoughtworks.com/radar/techniques/software-bill-of-materials\"\">the need for SBOM</a> previously, and this tool makes it easier to get started. SBOM tool supports a variety of popular package managers (including npm, pip and Gradle), making it compatible with a wide range of projects. It’s very easy to use and can be integrated into existing development workflows, including integration with CI/CD pipelines. By leveraging SBOM generated with this tool, developers gain multiple advantages. Improved software security is a key benefit, as a clear view of components allows for easier vulnerability identification and risk management. License compliance is also enhanced, as developers can ensure adherence to all relevant agreements. Furthermore, SBOM promotes transparency within the software supply chain, aiding dependency tracking and mitigating potential risks. If you're looking to streamline SBOM generation, improve software security and gain control over your software supply chain, you should give Microsoft SBOM tool a try.</p>\""
  },
  {
    "name": "Open Policy Agent (OPA)",
    "ring": "trial",
    "quadrant": "tools",
    "isNew": "FALSE",
    "status": "no change",
    "description": "\"<p><strong><a href=\"\"https://www.openpolicyagent.org/\"\" target=\"\"_blank\"\" aria-label=\"\"Open Policy Agent (OPA). This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Open Policy Agent (OPA)<span class=\"\"pop-out-icon\"\"></span></a></strong> is a uniform framework and <a href=\"\"https://www.openpolicyagent.org/docs/latest/#rego\"\" target=\"\"_blank\"\" aria-label=\"\"language. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">language<span class=\"\"pop-out-icon\"\"></span></a> for declaring, enforcing and controlling policies. For our teams, it has become a favored way of defining policies for distributed systems, particularly where we need to implement <a href=\"\"https://www.youtube.com/watch?v=zyw2GJlL4Lg&amp;list=PLj6h78yzYM2M3-reG8FBlsE5s7P_UOvl4&amp;index=34\"\" target=\"\"_blank\"\" aria-label=\"\"compliance at the point of change. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">compliance at the point of change<span class=\"\"pop-out-icon\"\"></span></a>. OPA allows teams to implement various platform engineering patterns, such as controlling what is deployed to Kubernetes clusters, enforcing access control across services in a <a href=\"\"https://www.thoughtworks.com/radar/techniques/service-mesh\"\">service mesh</a> and implementing fine-grained <a href=\"\"https://www.thoughtworks.com/radar/techniques/security-policy-as-code\"\">security policy as code</a> for accessing application resources. While there is some complexity associated with OPA implementations, it has proven to be a highly valuable tool for ensuring <a href=\"\"https://martinfowler.com/articles/devops-compliance.html#change-compliance\"\" target=\"\"_blank\"\" aria-label=\"\"compliance in a DevOps culture. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">compliance in a DevOps culture<span class=\"\"pop-out-icon\"\"></span></a>. We’re also continuing to keep an eye on the extension and maturity of OPA beyond operational systems to (big) data-centric solutions.</p>\""
  },
  {
    "name": "Philips's self-hosted GitHub runner",
    "ring": "trial",
    "quadrant": "tools",
    "isNew": "FALSE",
    "status": "moved in",
    "description": "\"<p>While <a href=\"\"https://www.thoughtworks.com/radar/platforms/github-actions\"\">GitHub Actions</a> runners cover a wide range of the most common run times and are quickest to start with, teams sometimes need to manage self-hosted runners, such as when organizational policy only allows deployments to a privately hosted infrastructure from within the organization's own security perimeter. In such cases, teams can use <strong>Philips's <a href=\"\"https://github.com/philips-labs/terraform-aws-github-runner\"\" target=\"\"_blank\"\" aria-label=\"\"self-hosted GitHub runner. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">self-hosted GitHub runner<span class=\"\"pop-out-icon\"\"></span></a></strong>, a Terraform module that spins up custom runners on AWS EC2 spot instances. The module also creates a set of Lambdas that handles lifecycle management (scaling up and down) for these runners. In our experience, this tool greatly simplifies the provisioning and management of self-hosted GitHub Actions runners. An alternative for teams that use Kubernetes is <a href=\"\"https://www.thoughtworks.com/radar/tools/actions-runner-controller\"\">actions-runner-controller</a>.</p>\""
  },
  {
    "name": "Pop",
    "ring": "trial",
    "quadrant": "tools",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p>Pair programming continues to be an essential technique for us, because it helps improve code quality and spread knowledge within a team. Although it’s best done in person, our distributed teams have explored tools to make remote pairing as pleasant and effective as possible, such as <a href=\"\"https://www.thoughtworks.com/radar/tools/tuple\"\">Tuple</a>, <a href=\"\"https://www.thoughtworks.com/radar/tools/visual-studio-live-share\"\">Visual Studio Live Share</a>, <a href=\"\"https://www.thoughtworks.com/radar/tools/code-with-me\"\">Code With Me</a> and general-purpose chat and conferencing tools. The latest tool in the space that's caught our attention is <strong><a href=\"\"https://pop.com/\"\" target=\"\"_blank\"\" aria-label=\"\"Pop. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Pop<span class=\"\"pop-out-icon\"\"></span></a></strong> (formerly Screen). Coming from the founders of Screenhero, it supports multi-person screen sharing, annotations and high-quality audio/video. Some of our teams have used it extensively for pair programming and remote working sessions and report positively on their experience.</p>\""
  },
  {
    "name": "Renovate",
    "ring": "trial",
    "quadrant": "tools",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p>Automatically monitoring and updating dependencies as part of the software build process has become standard practice across the industry. It takes the guesswork out of staying current with security updates to open-source packages as they're released. For many years, <a href=\"\"https://www.thoughtworks.com/radar/tools/dependabot\"\">Dependabot</a> has been the standard tool for this practice, but <strong><a href=\"\"https://www.mend.io/renovate/\"\" target=\"\"_blank\"\" aria-label=\"\"Renovate. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Renovate<span class=\"\"pop-out-icon\"\"></span></a></strong> has become the preferred tool for many of our teams. They find that Renovate is more suitable to the modern software development environment where a deployable system relies not just on code and libraries but encompasses run-time tools, infrastructure and third-party services. Renovate covers dependencies on these ancillary artifacts in addition to code. Our teams also found that Renovate offers more flexibility through configuration and customization options. Although Dependabot remains a safe default choice and is conveniently integrated with GitHub, we'd recommend evaluating Renovate to see if it can further reduce the manual burden on developers to keep their application ecosystems safe and secure.</p>\""
  },
  {
    "name": "Terrascan",
    "ring": "trial",
    "quadrant": "tools",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://github.com/tenable/terrascan\"\" target=\"\"_blank\"\" aria-label=\"\"Terrascan. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Terrascan<span class=\"\"pop-out-icon\"\"></span></a></strong> is a static code analyzer for <a href=\"\"https://www.thoughtworks.com/radar/techniques/infrastructure-as-code\"\">infrastructure as code</a> (IaC) designed to detect security vulnerabilities and compliance issues before provisioning cloud-native infrastructure. It supports scanning for <a href=\"\"https://www.thoughtworks.com/radar/tools/terraform\"\">Terraform</a>, <a href=\"\"https://www.thoughtworks.com/radar/platforms/kubernetes\"\">Kubernetes</a> (JSON/YAML), <a href=\"\"https://www.thoughtworks.com/radar/tools/helm\"\">Helm</a>, AWS CloudFormation, Azure Resource Manager, Dockerfiles and GitHub. The <a href=\"\"https://runterrascan.io/docs/policies/\"\" target=\"\"_blank\"\" aria-label=\"\"default policy pack. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">default policy pack<span class=\"\"pop-out-icon\"\"></span></a> covers all the major cloud providers, GitHub, Docker and Kubernetes. Our teams use Terrascan locally as a pre-commit hook and integrate it into CI pipelines to detect IaC vulnerabilities and violations.</p>\""
  },
  {
    "name": "Velero",
    "ring": "trial",
    "quadrant": "tools",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://velero.io/\"\" target=\"\"_blank\"\" aria-label=\"\"Velero. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Velero<span class=\"\"pop-out-icon\"\"></span></a></strong> is an open-source tool for backing up and restoring Kubernetes resources and persistent volumes. It simplifies disaster recovery and cluster migrations by enabling on-demand and scheduled backups. Velero also allows finer-grained controls over which resources get backed up as well as over the backup/restore workflow (with hooks). Our teams appreciate its ease of use and its reliance on Kubernetes APIs instead of lower-level layers like <a href=\"\"https://etcd.io/\"\" target=\"\"_blank\"\" aria-label=\"\"etcd. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">etcd<span class=\"\"pop-out-icon\"\"></span></a>.</p>\""
  },
  {
    "name": "aider",
    "ring": "assess",
    "quadrant": "tools",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://github.com/paul-gauthier/aider\"\" target=\"\"_blank\"\" aria-label=\"\"aider. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">aider<span class=\"\"pop-out-icon\"\"></span></a></strong> is an open-source AI coding assistant. Like many open-source tools in this space, aider doesn’t have direct IDE integration but is started as a CLI in the terminal. aider is interesting because it provides a chat interface with write access to the codebase across multiple files, whereas many of the coding assistant products today either only read the code or can change only one file at a time. This allows aider to help you implement concepts that stretch over multiple files (e.g., \"\"add locators to my HTML and also use those in my functional test\"\") and to create new files and folder structures in the codebase (e.g., \"\"create a new component similar to the one in folder X\"\"). As aider is open source and not a hosted product, you have to bring your own OpenAI or Azure OpenAI API key to use it. On the one hand, this can be great for occasional use because you only have to pay per use. On the other hand, aider does seem to be quite chatty in its interactions with the AI API, so keep an eye on request costs and rate limits when using it.</p>\""
  },
  {
    "name": "Akvorado",
    "ring": "assess",
    "quadrant": "tools",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://github.com/akvorado/akvorado\"\" target=\"\"_blank\"\" aria-label=\"\"Akvorado. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Akvorado<span class=\"\"pop-out-icon\"\"></span></a></strong> is an open-source network monitoring and analysis tool. It captures the network flows, Netflow/IPFIX and sFlow, enriches them with interface names and geo information and then saves the updated flows in <a href=\"\"https://www.thoughtworks.com/radar/platforms/clickhouse\"\">ClickHouse</a> for future analysis. Although <a href=\"\"https://www.thoughtworks.com/radar/languages-and-frameworks/opentelemetry\"\">OpenTelemetry</a> is gaining adoption for observing application-level traffic, we often come across challenges in the network layer that can be difficult to spot and troubleshoot. Tools like Akvorado are quite handy in such situations as they help you analyze the network flows across various devices in the network topology.</p>\""
  },
  {
    "name": "Baichuan 2",
    "ring": "assess",
    "quadrant": "tools",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://github.com/baichuan-inc/Baichuan2/blob/main/README_EN.md\"\" target=\"\"_blank\"\" aria-label=\"\"Baichuan 2. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Baichuan 2<span class=\"\"pop-out-icon\"\"></span></a></strong> is part of a new generation of open-source large language models. It was trained on a high-quality corpus with 2.6 trillion tokens, achieving quite good performance for its size on Chinese, English and multi-language benchmarks. Baichuan has been trained on several domain-specific corpora, including healthcare and law data sets, which is why we prefer using it in these and related fields.</p>\""
  },
  {
    "name": "Cargo Lambda",
    "ring": "assess",
    "quadrant": "tools",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p>The efficiency and performance of <a href=\"\"https://www.thoughtworks.com/radar/languages-and-frameworks/rust\"\">Rust</a> make it a good fit for serverless computing. Another advantage is that Rust functions don’t require a run time, which results in fast startup times. However, the developer experience for writing the functions in Rust wasn’t great. That changed with <strong><a href=\"\"https://www.cargo-lambda.info/\"\" target=\"\"_blank\"\" aria-label=\"\"Cargo Lambda. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Cargo Lambda<span class=\"\"pop-out-icon\"\"></span></a></strong>. As a cargo subcommand, it integrates with the typical Rust workflow and allows you to run and test your <a href=\"\"https://www.thoughtworks.com/radar/platforms/aws-lambda\"\">AWS Lambda</a> functions on the developer machine without needing <a href=\"\"https://www.thoughtworks.com/radar/platforms/docker\"\">Docker</a>, VMs or other tools. Using a <a href=\"\"https://www.thoughtworks.com/radar/languages-and-frameworks/zig\"\">Zig</a> toolchain, Cargo Lambda can cross-compile the functions on several operating systems for the Linux sandboxes used by AWS Lambda, and both Arm and Intel are supported as target architectures.</p>\""
  },
  {
    "name": "Codium AI",
    "ring": "assess",
    "quadrant": "tools",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p>In the busy emerging space of AI coding assistants, some products, instead of competing with the strong incumbents, take a more focused approach. <strong><a href=\"\"https://www.codium.ai/\"\" target=\"\"_blank\"\" aria-label=\"\"Codium AI. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Codium AI<span class=\"\"pop-out-icon\"\"></span></a></strong> is focused on test generation with AI. It works for all languages but has advanced support for common stacks, such as JavaScript and Python. We particularly like that the tool, rather than taking developers straight to the test code, offers a list of scenario descriptions in natural language for review. This makes it easier for developers to reason about the scenarios and decide which ones to turn into test code. To further improve the test generation for a particular codebase and use case, users can provide example tests and general instructions to enhance the information used by the AI to generate the tests.</p>\""
  },
  {
    "name": "Continue",
    "ring": "assess",
    "quadrant": "tools",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://github.com/continuedev/continue\"\" target=\"\"_blank\"\" aria-label=\"\"Continue. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Continue<span class=\"\"pop-out-icon\"\"></span></a></strong> is an open-source autopilot for VS Code and JetBrains IDEs. We quite like it because it eliminates the pain of copying/pasting from a chat-based interface to large language models (LLMs) with a direct integration in the IDE. It supports several commercial and open-source <a href=\"\"https://continue.dev/docs/model-setup/select-model\"\" target=\"\"_blank\"\" aria-label=\"\"models. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">models<span class=\"\"pop-out-icon\"\"></span></a> and makes it easy to try different LLM <a href=\"\"https://continue.dev/docs/model-setup/select-provider\"\" target=\"\"_blank\"\" aria-label=\"\"providers. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">providers<span class=\"\"pop-out-icon\"\"></span></a>, including <a href=\"\"https://www.thoughtworks.com/radar/techniques/self-hosted-llms\"\">self-hosted LLMs</a>. You can even run Continue <a href=\"\"https://continue.dev/docs/walkthroughs/running-continue-without-internet\"\" target=\"\"_blank\"\" aria-label=\"\"without an internet connection. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">without an internet connection<span class=\"\"pop-out-icon\"\"></span></a>.</p>\""
  },
  {
    "name": "Fern Docs",
    "ring": "assess",
    "quadrant": "tools",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p>One hallmark of widely used REST APIs is that their contracts are thoroughly documented. Developers are more likely to adopt and use APIs whose behavior and syntax are described accurately in a structured, organized way. Keeping this documentation up to date as the contract evolves can be time-consuming and is a task that is easily overlooked. <strong><a href=\"\"https://buildwithfern.com/#docs\"\" target=\"\"_blank\"\" aria-label=\"\"Fern Docs. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Fern Docs<span class=\"\"pop-out-icon\"\"></span></a></strong> helps with this by reducing the toil involved in writing and maintaining API documentation. Fern automatically generates a website with attractive, usable documentation from a specification file that can be versioned alongside the API code. While our initial impressions of this product are positive, Fern does require you to maintain API information in a proprietary configuration file. While it provides a way to convert OpenAPI specs into its own configuration format, we'd prefer a tool that generates docs directly from annotated source code.</p>\""
  },
  {
    "name": "Granted",
    "ring": "assess",
    "quadrant": "tools",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p>Given how common multi-account strategy is in organizations' AWS environments, engineers frequently have to switch between multiple accounts within a short period of time. <strong><a href=\"\"https://github.com/common-fate/granted\"\" target=\"\"_blank\"\" aria-label=\"\"Granted. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Granted<span class=\"\"pop-out-icon\"\"></span></a></strong>, a command-line tool that simplifies the opening of multiple accounts in the browser simultaneously, streamlines account switching. It leverages each browser's native features to isolate multiple identities, Firefox's <a href=\"\"https://addons.mozilla.org/en-US/firefox/addon/multi-account-containers/\"\" target=\"\"_blank\"\" aria-label=\"\"Multi-Account Containers. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Multi-Account Containers<span class=\"\"pop-out-icon\"\"></span></a> and <a href=\"\"https://support.google.com/chrome/answer/2364824\"\" target=\"\"_blank\"\" aria-label=\"\"Chromium's Profiles. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Chromium's Profiles<span class=\"\"pop-out-icon\"\"></span></a>. If a specific service (such as S3) is specified as an argument, Granted will open the service's landing page. Granted currently only supports AWS. Notably, it stores AWS SSO's temporary credentials safely in the keychain rather than as plain text on the disk.</p>\""
  },
  {
    "name": "LinearB",
    "ring": "assess",
    "quadrant": "tools",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://linearb.io/\"\" target=\"\"_blank\"\" aria-label=\"\"LinearB. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">LinearB<span class=\"\"pop-out-icon\"\"></span></a></strong> is a platform designed to empower engineering leaders with data-driven insights for continuous improvement. It tackles three key areas: benchmarking, workflow automation and investment. Our experience with LinearB's metrics tooling highlights its potential to support a culture of continuous improvement. One of our teams leveraged the platform to track engineering metrics, identify and discuss improvement opportunities and define actionable steps based on data, leading to measurable progress. We're happy to see that this aligns with LinearB's core value proposition: benchmark, automate and improve. LinearB integrates with GitHub, GitLab, Bitbucket and Jira. It offers a comprehensive suite of preconfigured engineering metrics, with a strong focus on <a href=\"\"https://www.thoughtworks.com/radar/techniques/four-key-metrics\"\">DORA metrics</a> (deployment frequency, lead time, change failure rate and time to restore). As strong advocates of the four key metrics as defined by the DORA research, we appreciate LinearB's emphasis on measuring what truly matters for software delivery performance. Historically, gathering DORA-specific metrics has been a challenge. Teams have resorted to complex CD pipeline instrumentation, custom-built dashboards or rely on manual processes. Although our experience is limited to one team, LinearB seems to be a compelling alternative for gathering and tracking engineering metrics as well as fostering a data-driven approach to continuous improvement.</p>\""
  },
  {
    "name": "LLaVA",
    "ring": "assess",
    "quadrant": "tools",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://github.com/haotian-liu/LLaVA\"\" target=\"\"_blank\"\" aria-label=\"\"LLaVA (Large Language and Vision Assistant). This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">LLaVA (Large Language and Vision Assistant)<span class=\"\"pop-out-icon\"\"></span></a></strong> is an open-source, large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding. LLaVA's strong proficiency in instruction-following positions it as a highly competitive contender among multimodal AI models. The latest version, <a href=\"\"https://llava-vl.github.io/blog/2024-01-30-llava-next/\"\" target=\"\"_blank\"\" aria-label=\"\"LLaVA-NeXT. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">LLaVA-NeXT<span class=\"\"pop-out-icon\"\"></span></a>, allows for improved question answering. Among the open-source models for language and vision assistance, LLaVA is a promising option when <a href=\"\"https://encord.com/blog/gpt-vision-vs-llava/\"\" target=\"\"_blank\"\" aria-label=\"\"compared to GPT-4 Vision. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">compared to GPT-4 Vision<span class=\"\"pop-out-icon\"\"></span></a>. Our teams have been experimenting with it for visual question answering.</p>\""
  },
  {
    "name": "Marimo",
    "ring": "assess",
    "quadrant": "tools",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://marimo.io/\"\" target=\"\"_blank\"\" aria-label=\"\"Marimo. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Marimo<span class=\"\"pop-out-icon\"\"></span></a></strong> offers a fresh take on Python notebooks by prioritizing reproducibility and interactivity. It addresses challenges with hidden state in traditional notebooks (like <a href=\"\"https://www.thoughtworks.com/radar/tools/jupyter\"\">Jupyter</a>) which can lead to unexpected behavior and hinder reproducibility. It does that by storing notebooks as plain Python files with no hidden state and using a deterministic execution order based on dependencies (when a variable changes, all affected cells are automatically run). Marimo also comes with interactive UI elements that similarly propagate value changes to cells that depend on them. As it can be deployed as a web app, it’s also a useful tool for demos and prototyping purposes. Although we’re excited for the potential of Marimo, in particular in terms of reproducibility for data exploration and analysis purposes, we continue to caution against <a href=\"\"https://www.thoughtworks.com/radar/techniques/productionizing-notebooks\"\">productionizing notebooks</a>.</p>\""
  },
  {
    "name": "Mixtral",
    "ring": "assess",
    "quadrant": "tools",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://docs.mistral.ai/models/#mixtral-8x7b\"\" target=\"\"_blank\"\" aria-label=\"\"Mixtral. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Mixtral<span class=\"\"pop-out-icon\"\"></span></a></strong> is part of <a href=\"\"https://docs.mistral.ai/models/\"\" target=\"\"_blank\"\" aria-label=\"\"the family of open-weight large language models. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">the family of open-weight large language models<span class=\"\"pop-out-icon\"\"></span></a> <a href=\"\"https://mistral.ai/\"\" target=\"\"_blank\"\" aria-label=\"\"Mistral. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Mistral<span class=\"\"pop-out-icon\"\"></span></a> released, that utilizes the <a href=\"\"https://mistral.ai/news/mixtral-of-experts/\"\" target=\"\"_blank\"\" aria-label=\"\"sparse Mixture of Experts architecture. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">sparse Mixture of Experts architecture<span class=\"\"pop-out-icon\"\"></span></a>. The family of models are available both in raw pretrained and fine-tuned forms in 7B and 8x7B parameter sizes. Its sizes, open-weight nature, performance in benchmarks and context length of 32,000 tokens make it a compelling option for <a href=\"\"https://www.thoughtworks.com/radar/techniques/self-hosted-llms\"\">self-hosted LLMs</a>. Note that these open-weight models are not tuned for safety out of the box, and users need to refine moderation based on their own use cases. We have experience with this family of models in developing <a href=\"\"https://huggingface.co/opennyaiorg/Aalap-Mistral-7B-v0.1-bf16\"\" target=\"\"_blank\"\" aria-label=\"\"Aalap. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Aalap<span class=\"\"pop-out-icon\"\"></span></a>, a fine-tuned Mistral 7B model trained on data related to specific Indian legal tasks, which has performed <a href=\"\"https://huggingface.co/opennyaiorg/Aalap-Mistral-7B-v0.1-bf16#what-is-aalaps-intended-uses\"\" target=\"\"_blank\"\" aria-label=\"\"reasonably well. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">reasonably well<span class=\"\"pop-out-icon\"\"></span></a> on an affordable cost basis.</p>\""
  },
  {
    "name": "NeMo Guardrails",
    "ring": "assess",
    "quadrant": "tools",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://github.com/NVIDIA/NeMo-Guardrails\"\" target=\"\"_blank\"\" aria-label=\"\"NeMo Guardrails. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">NeMo Guardrails<span class=\"\"pop-out-icon\"\"></span></a></strong> is an easy-to-use open-source toolkit from NVIDIA that empowers developers to implement guardrails for large language models (LLMs) used in conversational applications. Although LLMs hold immense potential in building interactive experiences, their inherent limitations around factual accuracy, bias and potential misuse necessitate safeguards. Guardrails offer a promising approach to ensure responsible and trustworthy LLMs. Although you have a choice when it comes to LLM guardrails, our teams have found NeMo Guardrails particularly useful because it supports programmable rules and run-time integration and can be applied to existing LLM applications without extensive code modifications.</p>\""
  },
  {
    "name": "Ollama",
    "ring": "assess",
    "quadrant": "tools",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://github.com/ollama/ollama\"\" target=\"\"_blank\"\" aria-label=\"\"Ollama. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Ollama<span class=\"\"pop-out-icon\"\"></span></a></strong> is an open-source tool for running and managing large language models (LLMs) on your local machine. Previously, we talked about the benefits of <a href=\"\"https://www.thoughtworks.com/radar/techniques/self-hosted-llms\"\">self-hosted LLMs</a>, and we’re pleased to see the ecosystem mature with tools like Ollama. It supports several <a href=\"\"https://ollama.com/library\"\" target=\"\"_blank\"\" aria-label=\"\"popular models. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">popular models<span class=\"\"pop-out-icon\"\"></span></a> — including <a href=\"\"https://www.thoughtworks.com/radar/tools/llama-2\"\">LLaMA-2</a>, CodeLLaMA, Falcon and Mistral — that you can download and run locally. Once downloaded, you can use the CLI, API or SDK to interact with the model and execute your tasks. We're evaluating Ollama and are seeing early success as it improves the developer experience in working with LLMs locally.</p>\""
  },
  {
    "name": "OpenTofu",
    "ring": "assess",
    "quadrant": "tools",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://opentofu.org/\"\" target=\"\"_blank\"\" aria-label=\"\"OpenTofu. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">OpenTofu<span class=\"\"pop-out-icon\"\"></span></a></strong> is a fork of <a href=\"\"https://www.thoughtworks.com/radar/tools/terraform\"\">Terraform</a> made in response to a recent ambiguous license change by HashiCorp. It's open source and has been accepted by the Linux Foundation. It's backed by several organizations, including third-party vendors. The current version is compatible with the last open-source version of Terraform. Version 1.7 adds client-side encryption. The future of the OpenTofu project is unclear in terms of how closely it will support compatibility with future versions of Terraform. There are also questions around the long-term support by its current backers. We recommend keeping an eye on the project but remain cautious around usage, except for teams that have the capability to manage risks that may include being able to contribute to the codebase.</p>\""
  },
  {
    "name": "QAnything",
    "ring": "assess",
    "quadrant": "tools",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p>Large language models (LLMs) and <a href=\"\"https://www.thoughtworks.com/radar/techniques/retrieval-augmented-generation-rag\"\">retrieval-augmented generation (RAG)</a> techniques have greatly improved our ability to synthesize and extract information. We’re seeing emerging tools taking advantage of this, and <strong><a href=\"\"https://github.com/netease-youdao/QAnything/tree/master\"\" target=\"\"_blank\"\" aria-label=\"\"QAnything. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">QAnything<span class=\"\"pop-out-icon\"\"></span></a></strong> is one of them. QAnything is a knowledge management engine with a question-and-answer interface that can summarize and extract information from a wide range of file formats, including PDF, DOCX, PPTX, XLSX and MD files, among others. For data security concerns, QAnything also supports offline installation. Some of our teams use QAnything to build their team knowledge base. In GenAI scenarios with more industry depth (such as generating abstracts for investment reports), we also try to use this tool for proofs of concept before building real products and showing the potential of LLMs and RAG.</p>\""
  },
  {
    "name": "System Initiative",
    "ring": "assess",
    "quadrant": "tools",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p>Little has emerged in recent years to challenge the dominance of <a href=\"\"https://www.thoughtworks.com/radar/tools/terraform\"\">Terraform</a> as an <a href=\"\"https://www.thoughtworks.com/radar/techniques/infrastructure-as-code\"\">infrastructure coding</a> tool. Although alternatives such as <a href=\"\"https://www.thoughtworks.com/radar/platforms/pulumi\"\">Pulumi</a>, <a href=\"\"https://www.thoughtworks.com/radar/platforms/aws-cloud-development-kit\"\">CDK</a> and, more recently, <a href=\"\"https://www.thoughtworks.com/radar/platforms/winglang\"\">Wing</a> have emerged, Terraform's modular, declarative paradigm has proven to be the most enduring. Indeed, all of these approaches share the common goal of modular code creating monolithic infrastructure. <strong><a href=\"\"https://www.systeminit.com/\"\" target=\"\"_blank\"\" aria-label=\"\"System Initiative. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">System Initiative<span class=\"\"pop-out-icon\"\"></span></a></strong> is a new, experimental tool that represents a radical new direction for DevOps work. One way to view System Initiative is as a digital twin for your infrastructure. Interactive changes to the System Initiative state result in corresponding change sets that can be applied to the infrastructure itself. Likewise, changes to the infrastructure are reflected in the System Initiative state. One of the great advantages of this approach is the collaborative environment it creates for things like application deployment and observability. Engineers interact with System Initiative through a user interface that has a graphical representation of the entire environment. In addition to managing the cloud infrastructure, you can also use the tool to manage containers, scripts, tools and more. Although we're generally skeptical of these kinds of GUI tools, System Initiative can be extended to handle new assets or enforce policy via TypeScript code. We really like the creative thinking that has gone into this tool and hope it will encourage others to break with the status quo of infrastructure-as-code approaches. System Initiative is free and open source under an Apache 2.0 license and is currently in open beta. The maintainers themselves do not recommend the tool for production use yet, but we think it’s worth checking out in its current state to experience a completely different approach to DevOps tooling.</p>\""
  },
  {
    "name": "Tetragon",
    "ring": "assess",
    "quadrant": "tools",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://github.com/cilium/tetragon/\"\" target=\"\"_blank\"\" aria-label=\"\"Tetragon. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Tetragon<span class=\"\"pop-out-icon\"\"></span></a></strong> is an open-source eBPF-based security observability and run-time enforcement tool. We mentioned <a href=\"\"https://www.thoughtworks.com/radar/tools/falco\"\">Falco</a> for detecting security threats a while back in the Radar. Tetragon goes beyond threat detection by leveraging eBPF to <a href=\"\"https://tetragon.io/docs/concepts/enforcement/\"\" target=\"\"_blank\"\" aria-label=\"\"enforce security policies. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">enforce security policies<span class=\"\"pop-out-icon\"\"></span></a> at run time in the Linux kernel. You can use Tetragon either as a standalone tool on bare metal or inside the <a href=\"\"https://www.thoughtworks.com/radar/platforms/kubernetes\"\">Kubernetes</a> environment.</p>\""
  },
  {
    "name": "Winglang",
    "ring": "assess",
    "quadrant": "tools",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p>We’re seeing a lot of movement in the <a href=\"\"https://www.thoughtworks.com/radar/techniques/infrastructure-as-code\"\">infrastructure-as-code (IaC)</a> space with tools like <strong><a href=\"\"https://www.winglang.io/\"\" target=\"\"_blank\"\" aria-label=\"\"Winglang. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Winglang<span class=\"\"pop-out-icon\"\"></span></a></strong> emerging. Winglang takes a different approach to defining infrastructure and run-time behavior. It provides high-level abstractions over platform specifics provided by tools such as <a href=\"\"https://aws.amazon.com/cloudformation/\"\" target=\"\"_blank\"\" aria-label=\"\"CloudFormation. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">CloudFormation<span class=\"\"pop-out-icon\"\"></span></a>, <a href=\"\"https://www.thoughtworks.com/radar/tools/terraform\"\">Terraform</a>, <a href=\"\"https://www.thoughtworks.com/radar/platforms/pulumi\"\">Pulumi</a> and <a href=\"\"https://www.thoughtworks.com/radar/platforms/kubernetes\"\">Kubernetes</a>. With Winglang, you write code that runs at compile time to generate infrastructure configuration and then code that executes at run time for application behavior. It provides a simulation mode to run locally and has an integrated test framework. We’re keeping an eye on this interesting tool; it’s a potential preview of the future direction of IaC.</p>\""
  },
  {
    "name": "Astro",
    "ring": "trial",
    "quadrant": "languages-and-frameworks",
    "isNew": "FALSE",
    "status": "moved in",
    "description": "\"<p>The <strong><a href=\"\"https://astro.build/\"\" target=\"\"_blank\"\" aria-label=\"\"Astro. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Astro<span class=\"\"pop-out-icon\"\"></span></a></strong> framework is gaining more popularity in the community. One of our teams has used Astro to build content-driven websites like blogs and marketing websites. Astro is a multi-page application framework that renders HTML on the server and minimizes the amount of JavaScript sent over the wire. We like that Astro supports — when appropriate — select active components written in the front-end JavaScript framework of your choice even though it encourages sending only HTML. It does this through its <a href=\"\"https://docs.astro.build/en/concepts/islands/\"\" target=\"\"_blank\"\" aria-label=\"\"island architecture. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">island architecture<span class=\"\"pop-out-icon\"\"></span></a>. Islands are regions of interactivity within a single page where the necessary JavaScript is downloaded only when needed. In this way, most areas of the site are converted to fast, static HTML, and the JavaScript parts are optimized for parallel loading. Our team likes both its page rendering performance as well as its build speed. The <a href=\"\"https://docs.astro.build/en/basics/astro-syntax/\"\" target=\"\"_blank\"\" aria-label=\"\"Astro component syntax. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Astro component syntax<span class=\"\"pop-out-icon\"\"></span></a> is a simple extension of HTML, and the learning curve is quite gentle.</p>\""
  },
  {
    "name": "DataComPy",
    "ring": "trial",
    "quadrant": "languages-and-frameworks",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p>Comparing DataFrames is a common task in data engineering, often done to compare the output of two data transformation approaches to make sure no meaningful deviations or inconsistencies have occurred. <strong><a href=\"\"https://github.com/capitalone/datacompy\"\" target=\"\"_blank\"\" aria-label=\"\"DataComPy. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">DataComPy<span class=\"\"pop-out-icon\"\"></span></a></strong> is a Python library that facilitates the comparison of two DataFrames in pandas, Spark and more. The library goes beyond basic equality checks by providing detailed insights into discrepancies at both row and column levels. DataComPy also has the ability to specify absolute or relative tolerance for comparison of numeric columns as well as known differences it need not highlight in its report. Some of our teams use it as part of their smoke testing suite; they find it efficient when comparing large and wide DataFrames and consider its reports easy to understand and act upon.</p>\""
  },
  {
    "name": "Pinia",
    "ring": "trial",
    "quadrant": "languages-and-frameworks",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://pinia.vuejs.org/\"\" target=\"\"_blank\"\" aria-label=\"\"Pinia. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Pinia<span class=\"\"pop-out-icon\"\"></span></a></strong> is a store library and state management framework for <a href=\"\"https://www.thoughtworks.com/radar/languages-and-frameworks/vue-js\"\">Vue.js</a>. It uses declarative syntax and offers its own state management API. Compared to <a href=\"\"https://vuex.vuejs.org/\"\" target=\"\"_blank\"\" aria-label=\"\"Vuex. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Vuex<span class=\"\"pop-out-icon\"\"></span></a>, Pinia provides a simpler API with less ceremony, offers Composition-style APIs and, most importantly, has solid type inference support when used with <a href=\"\"https://www.thoughtworks.com/radar/languages-and-frameworks/typescript\"\">TypeScript</a>. Pinia is endorsed by the Vue.js team as a credible alternative to Vuex and is currently the official state management library for Vue.js. Our teams are leveraging Pinia for its simplicity and ease of implementation.</p>\""
  },
  {
    "name": "Ray",
    "ring": "trial",
    "quadrant": "languages-and-frameworks",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p>Today's machine learning (ML) workloads are increasingly compute-intensive. As convenient as they are, single-node development environments such as your laptop cannot scale to meet these demands. <strong><a href=\"\"https://github.com/ray-project/ray\"\" target=\"\"_blank\"\" aria-label=\"\"Ray. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Ray<span class=\"\"pop-out-icon\"\"></span></a></strong> is a unified framework for scaling AI and Python code from laptop to cluster. Ray is essentially a well-encapsulated distributed computing framework with a series of AI libraries to simplify ML work. By integrating with other frameworks (e.g., <a href=\"\"https://www.thoughtworks.com/radar/languages-and-frameworks/pytorch\"\">PyTorch</a> and <a href=\"\"https://www.thoughtworks.com/radar/languages-and-frameworks/tensorflow\"\">TensorFlow</a>), it can be used to build large-scale ML platforms. Companies like OpenAI and Bytedance use Ray heavily for model training and inference. We also use its AI libraries to help with <a href=\"\"https://docs.ray.io/en/latest/train/train.html\"\" target=\"\"_blank\"\" aria-label=\"\"distributed training. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">distributed training<span class=\"\"pop-out-icon\"\"></span></a> and <a href=\"\"https://docs.ray.io/en/latest/tune/index.html\"\" target=\"\"_blank\"\" aria-label=\"\"hyperparameter tuning. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">hyperparameter tuning<span class=\"\"pop-out-icon\"\"></span></a> on our projects. We recommend you try Ray when building scalable ML projects.</p>\""
  },
  {
    "name": "Android Adaptability",
    "ring": "assess",
    "quadrant": "languages-and-frameworks",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p>Some mobile applications and games can be so demanding they cause thermal throttling within a few minutes. In this state, CPU and GPU frequency are reduced to help cool the device, but it also results in reduced frame rates in games. When the thermal situation improves, the frame rates increase again and the cycle repeats, leading to the software feeling janky. <strong><a href=\"\"https://developer.android.com/codelabs/adaptability-codelab\"\" target=\"\"_blank\"\" aria-label=\"\"Android Adaptability. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Android Adaptability<span class=\"\"pop-out-icon\"\"></span></a></strong>, a new set of libraries, allows application developers to respond to changing performance and thermal situations. The <a href=\"\"https://developer.android.com/games/optimize/adpf\"\" target=\"\"_blank\"\" aria-label=\"\"Android Dynamic Performance Framework (ADPF). This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Android Dynamic Performance Framework (ADPF)<span class=\"\"pop-out-icon\"\"></span></a> includes the Thermal API to provide information about the thermal state and the Hint API to help Android choose the optimal CPU operating point and core placement. Teams using Unity will find the Unity Adaptive Performance package helpful, as it works with both APIs.</p>\""
  },
  {
    "name": "Concrete ML",
    "ring": "assess",
    "quadrant": "languages-and-frameworks",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p>Previously, we blipped the <a href=\"\"https://www.thoughtworks.com/radar/techniques/homomorphic-encryption\"\">Homomorphic Encryption</a> technique that allows computations to be performed directly on encrypted data. <strong><a href=\"\"https://github.com/zama-ai/concrete-ml\"\" target=\"\"_blank\"\" aria-label=\"\"Concrete ML. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Concrete ML<span class=\"\"pop-out-icon\"\"></span></a></strong> is one such open-source tool that allows for privacy-preserving machine learning. Built on top of <a href=\"\"https://github.com/zama-ai/concrete\"\" target=\"\"_blank\"\" aria-label=\"\"Concrete. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Concrete<span class=\"\"pop-out-icon\"\"></span></a>, it simplifies the use of fully homomorphic encryption (FHE) for data scientists to help them automatically turn machine learning models into their homomorphic equivalent. Concrete ML's built-in models have APIs that are almost identical to their scikit-learn counterparts. You can also convert <a href=\"\"https://www.thoughtworks.com/radar/languages-and-frameworks/pytorch\"\">PyTorch</a> networks to FHE with Concrete ML's conversion APIs. Note, however, that FHE with Concrete ML could be slow without <a href=\"\"https://www.intel.com/content/www/us/en/developer/articles/technical/homomorphic-encryption/accelerating-homomorphic-encryption-for-fpga.html\"\" target=\"\"_blank\"\" aria-label=\"\"tuned hardware. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">tuned hardware<span class=\"\"pop-out-icon\"\"></span></a>.</p>\""
  },
  {
    "name": "Crabviz",
    "ring": "assess",
    "quadrant": "languages-and-frameworks",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://github.com/chanhx/crabviz\"\" target=\"\"_blank\"\" aria-label=\"\"Crabviz. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Crabviz<span class=\"\"pop-out-icon\"\"></span></a></strong> is a <a href=\"\"https://www.thoughtworks.com/radar/tools/visual-studio-code\"\">Visual Studio Code</a> plug-in to create call graphs. The graphs are interactive, which is essential when working with even moderately large codebases such as a microservice. They show types, methods, functions and interfaces grouped by file and also display function calling relationships and interface implementation relationships. Because Crabviz is based on the <a href=\"\"https://www.thoughtworks.com/radar/platforms/language-server-protocol\"\">Language Server Protocol</a>, it supports any number of languages, as long as the corresponding language server is installed. This means, though, that Crabviz is limited to static code analysis, which might not be sufficient for some use cases. The plug-in is written in <a href=\"\"https://www.thoughtworks.com/radar/languages-and-frameworks/rust\"\">Rust</a> and is available on the Visual Studio Code Marketplace.</p>\""
  },
  {
    "name": "Crux",
    "ring": "assess",
    "quadrant": "languages-and-frameworks",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://github.com/redbadger/crux\"\" target=\"\"_blank\"\" aria-label=\"\"Crux. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Crux<span class=\"\"pop-out-icon\"\"></span></a></strong> is an open-source cross-platform app development framework written in Rust. Inspired by the Elm architecture, Crux organizes business logic code at the core and UI layer in native frameworks like <a href=\"\"https://www.thoughtworks.com/radar/languages-and-frameworks/swiftui\"\">SwiftUI</a>, <a href=\"\"https://www.thoughtworks.com/radar/languages-and-frameworks/jetpack-compose\"\">Jetpack Compose</a>, <a href=\"\"https://www.thoughtworks.com/radar/languages-and-frameworks/react-js\"\">React</a>/<a href=\"\"https://www.thoughtworks.com/radar/languages-and-frameworks/vue-js\"\">Vue</a> or <a href=\"\"https://www.thoughtworks.com/radar/languages-and-frameworks/webassembly\"\">WebAssembly</a>-based frameworks (like <a href=\"\"https://github.com/yewstack/yew\"\" target=\"\"_blank\"\" aria-label=\"\"Yew. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Yew<span class=\"\"pop-out-icon\"\"></span></a>). With Crux, you can write side effects–free behavior code in Rust and share it across iOS, Android and the web.</p>\""
  },
  {
    "name": "Databricks Asset Bundles",
    "ring": "assess",
    "quadrant": "languages-and-frameworks",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p>The recent <a href=\"\"https://www.databricks.com/blog/announcing-public-preview-databricks-asset-bundles-apply-software-development-best-practices\"\" target=\"\"_blank\"\" aria-label=\"\"public preview release. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">public preview release<span class=\"\"pop-out-icon\"\"></span></a> of <a href=\"\"https://docs.databricks.com/en/dev-tools/bundles/index.html\"\" target=\"\"_blank\"\" aria-label=\"\"Databricks Asset Bundles. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Databricks Asset Bundles<span class=\"\"pop-out-icon\"\"></span></a> (DABs), included with <a href=\"\"https://docs.databricks.com/en/dev-tools/cli/install.html\"\" target=\"\"_blank\"\" aria-label=\"\"Databricks CLI version 0.205 and above. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Databricks CLI version 0.205 and above<span class=\"\"pop-out-icon\"\"></span></a>, is becoming the officially recommended way to package Databricks assets for source control, testing and deployment. It has started to replace <a href=\"\"https://dbx.readthedocs.io/en/latest/intro/\"\" target=\"\"_blank\"\" aria-label=\"\"dbx. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">dbx<span class=\"\"pop-out-icon\"\"></span></a> among our teams. DABs supports packaging the configuration of workflows, jobs and tasks, as well as the code to be executed in those tasks, as a bundle that can be deployed to multiple environments. It comes with templates for common types of assets and supports custom templates. While DABs includes templates for notebooks and supports deploying them to production, we continue to recommend against <a href=\"\"https://www.thoughtworks.com/radar/techniques/productionizing-notebooks\"\">productionizing notebooks</a> and instead encourage intentionally writing production code with the engineering practices that support the maintainability, resiliency and scalability needs of such workloads.</p>\""
  },
  {
    "name": "Electric",
    "ring": "assess",
    "quadrant": "languages-and-frameworks",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://github.com/electric-sql/electric\"\" target=\"\"_blank\"\" aria-label=\"\"Electric. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Electric<span class=\"\"pop-out-icon\"\"></span></a></strong> is a <a href=\"\"https://www.thoughtworks.com/radar/techniques/local-first-application\"\">local-first</a> sync framework for mobile and web applications. Local-first is a development paradigm where your application code talks directly to an embedded local database and data syncs in the background via an active-active database replication to the central database. With Electric, you have SQLite as the local embedded option and PostgreSQL for the central store. Although local-first greatly improves user experience, it is not without challenges, and the inventors of <a href=\"\"https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type\"\" target=\"\"_blank\"\" aria-label=\"\"CRDT. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">CRDT<span class=\"\"pop-out-icon\"\"></span></a> have worked on the Electric framework to ease the pain.</p>\""
  },
  {
    "name": "LiteLLM",
    "ring": "assess",
    "quadrant": "languages-and-frameworks",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://github.com/BerriAI/litellm\"\" target=\"\"_blank\"\" aria-label=\"\"LiteLLM. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">LiteLLM<span class=\"\"pop-out-icon\"\"></span></a></strong> is a library for seamless integration with various large language model (LLM) providers' APIs that standardizes interactions through an <a href=\"\"https://platform.openai.com/docs/guides/text-generation/chat-completions-api\"\" target=\"\"_blank\"\" aria-label=\"\"OpenAI API format. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">OpenAI API format<span class=\"\"pop-out-icon\"\"></span></a>. It supports an extensive array of <a href=\"\"https://docs.litellm.ai/docs/providers\"\" target=\"\"_blank\"\" aria-label=\"\"providers and models. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">providers and models<span class=\"\"pop-out-icon\"\"></span></a> and offers a unified interface for completion, embedding and image generation functionalities. LiteLLM simplifies integration by translating inputs to match each provider's specific endpoint requirements. This is particularly valuable in the current landscape, where a lack of standardized API specifications for LLM providers complicates the inclusion of multiple LLMs in projects. Our teams have leveraged LiteLLM to swap underlying models in LLM applications, addressing a significant integration challenge. However, it's crucial to acknowledge that model responses to identical prompts vary, indicating that a consistent invocation method alone may not fully optimize completion performance. Note that LiteLLM has several other features, such as <a href=\"\"https://docs.litellm.ai/docs/proxy/quick_start\"\" target=\"\"_blank\"\" aria-label=\"\"proxy server. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">proxy server<span class=\"\"pop-out-icon\"\"></span></a>, that are not in the purview of this blip.</p>\""
  },
  {
    "name": "LLaMA-Factory",
    "ring": "assess",
    "quadrant": "languages-and-frameworks",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p>We continue to caution against <a href=\"\"https://www.thoughtworks.com/radar/techniques/rush-to-fine-tune-llms\"\">rushing to fine-tune large language models (LLMs)</a> unless it’s absolutely critical — it comes with a significant overhead in terms of costs and expertise. However, we think <strong><a href=\"\"https://github.com/hiyouga/LLaMA-Factory\"\" target=\"\"_blank\"\" aria-label=\"\"LLaMA-Factory. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">LLaMA-Factory<span class=\"\"pop-out-icon\"\"></span></a></strong> can be useful when fine-tuning is needed. It’s an open-source, easy-to-use fine-tuning and training framework for LLMs. With support for <a href=\"\"https://www.thoughtworks.com/radar/tools/llama-2\"\">LLaMA</a>, BLOOM, <a href=\"\"https://www.thoughtworks.com/radar/tools/mixtral\"\">Mistral</a>, <a href=\"\"https://www.thoughtworks.com/radar/tools/baichuan-2\"\">Baichuan</a>, Qwen and <a href=\"\"https://www.thoughtworks.com/radar/platforms/chatglm\"\">ChatGLM</a>, it makes a complex concept like fine-tuning relatively accessible. Our teams used <a href=\"\"https://github.com/hiyouga/LLaMA-Factory?tab=readme-ov-file#supported-training-approaches\"\" target=\"\"_blank\"\" aria-label=\"\"LLaMA-Factory's LoRA tuning. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">LLaMA-Factory's LoRA tuning<span class=\"\"pop-out-icon\"\"></span></a> for a LLaMA 7B model successfully, so, if you have a need for fine-tuning, this framework is worth assessing.</p>\""
  },
  {
    "name": "MLX",
    "ring": "assess",
    "quadrant": "languages-and-frameworks",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://github.com/ml-explore/mlx\"\" target=\"\"_blank\"\" aria-label=\"\"MLX. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">MLX<span class=\"\"pop-out-icon\"\"></span></a></strong> is an open-source array framework designed for efficient and flexible machine learning on Apple silicon. It lets data scientists and machine learning (ML) engineers access the integrated GPU, allowing them to choose the hardware best suited for their needs. The design of MLX is inspired by frameworks like NumPy, <a href=\"\"https://www.thoughtworks.com/radar/languages-and-frameworks/pytorch\"\">PyTorch</a> and Jax to name a few. One of the key differentiators is MLX's unified memory model, which eliminates the overhead of data transfers between the CPU and GPU, resulting in faster execution. This feature makes running the models on devices such as iPhones plausible, opening a huge opportunity for on-device AI applications. Although niche, this framework is worth pursuing for the ML developer community.</p>\""
  },
  {
    "name": "Mojo",
    "ring": "assess",
    "quadrant": "languages-and-frameworks",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://www.modular.com/max/mojo\"\" target=\"\"_blank\"\" aria-label=\"\"Mojo. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Mojo<span class=\"\"pop-out-icon\"\"></span></a></strong> is a new AI-first programming language. It aims to bridge the gap between research and production by combining the Python syntax and ecosystem with systems programming and metaprogramming features. It’s the first language to take advantage of the new <a href=\"\"https://mlir.llvm.org/\"\" target=\"\"_blank\"\" aria-label=\"\"MLIR. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">MLIR<span class=\"\"pop-out-icon\"\"></span></a> compiler backend and packs cool features like zero-cost abstraction, auto tuning, eager destruction, tail call optimization and better single instruction, multiple data (SIMD) ergonomics. We like Mojo a lot and encourage you to give it a try. The Mojo SDK is currently available for Ubuntu and macOS operating systems.</p>\""
  },
  {
    "name": "Otter",
    "ring": "assess",
    "quadrant": "languages-and-frameworks",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://github.com/maypok86/otter\"\" target=\"\"_blank\"\" aria-label=\"\"Otter. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Otter<span class=\"\"pop-out-icon\"\"></span></a></strong> is a contention-free cache library in Go. Although Go has several such libraries, we want to highlight Otter for two reasons: its excellent <a href=\"\"https://github.com/maypok86/otter#throughput\"\" target=\"\"_blank\"\" aria-label=\"\"throughput. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">throughput<span class=\"\"pop-out-icon\"\"></span></a> and its clever implementation of the S3-FIFO algorithm for good cache hit ratio. Otter also supports generics, so you can use any comparable types as keys and any types as values.</p>\""
  },
  {
    "name": "Pkl",
    "ring": "assess",
    "quadrant": "languages-and-frameworks",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://pkl-lang.org/\"\" target=\"\"_blank\"\" aria-label=\"\"Pkl. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Pkl<span class=\"\"pop-out-icon\"\"></span></a></strong> is a configuration language and tooling created for use internally by Apple and now open-sourced. The key feature of Pkl is its type and validation system, allowing configuration errors to be caught prior to deployment. It generates JSON, .plist, YAML and .properties files and has extensive IDE and language integration including code generation.</p>\""
  },
  {
    "name": "Rust for UI",
    "ring": "assess",
    "quadrant": "languages-and-frameworks",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p>The impact of <a href=\"\"https://www.thoughtworks.com/radar/languages-and-frameworks/rust\"\">Rust</a> continues to grow, and many of the build and command-line tools we’ve covered recently are written in Rust. Now, we’re seeing movement in using <strong>Rust for UI</strong> development as well. The majority of teams who prefer to use the same language for code running in the browser and on the server opt to use JavaScript or TypeScript. However, with <a href=\"\"https://www.thoughtworks.com/radar/languages-and-frameworks/webassembly\"\">WebAssembly</a> you can use Rust in the browser, and this is becoming a little more common now. Frameworks like <a href=\"\"https://leptos.dev/\"\" target=\"\"_blank\"\" aria-label=\"\"Leptos. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Leptos<span class=\"\"pop-out-icon\"\"></span></a> and <a href=\"\"https://github.com/ivanceras/sauron\"\" target=\"\"_blank\"\" aria-label=\"\"sauron. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">sauron<span class=\"\"pop-out-icon\"\"></span></a> focus on web development, while <a href=\"\"https://dioxuslabs.com/\"\" target=\"\"_blank\"\" aria-label=\"\"Dioxus. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Dioxus<span class=\"\"pop-out-icon\"\"></span></a> and several other frameworks support cross-platform desktop and mobile app development in addition to web development.</p>\""
  },
  {
    "name": "vLLM",
    "ring": "assess",
    "quadrant": "languages-and-frameworks",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://github.com/vllm-project/vllm\"\" target=\"\"_blank\"\" aria-label=\"\"vLLM. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">vLLM<span class=\"\"pop-out-icon\"\"></span></a></strong> is a high-throughput and memory-efficient inferencing and serving engine for large language models (LLMs) that’s particularly effective thanks to its implementation of <a href=\"\"https://www.anyscale.com/blog/continuous-batching-llm-inference\"\" target=\"\"_blank\"\" aria-label=\"\"continuous batching. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">continuous batching<span class=\"\"pop-out-icon\"\"></span></a> for incoming requests. It supports several <a href=\"\"https://docs.vllm.ai/en/latest/serving/distributed_serving.html#\"\" target=\"\"_blank\"\" aria-label=\"\"deployment options. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">deployment options<span class=\"\"pop-out-icon\"\"></span></a>, including deployment of distributed tensor-parallel inference and serving with <a href=\"\"https://www.thoughtworks.com/radar/languages-and-frameworks/ray\"\">Ray</a> run time, deployment in the cloud with <a href=\"\"https://github.com/skypilot-org/skypilot\"\" target=\"\"_blank\"\" aria-label=\"\"SkyPilot. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">SkyPilot<span class=\"\"pop-out-icon\"\"></span></a> and deployment with NVIDIA Triton, <a href=\"\"https://www.thoughtworks.com/radar/platforms/docker\"\">Docker</a> and <a href=\"\"https://www.thoughtworks.com/radar/languages-and-frameworks/langchain\"\">LangChain</a>. Our teams have had good experience running dockerized vLLM workers in an on-prem virtual machine, integrating with OpenAI compatible API server -— which, in turn, is leveraged by a range of applications, including IDE plugins for coding assistance and chatbots. Our teams leverage vLLM for running models such as <a href=\"\"https://huggingface.co/codellama/CodeLlama-70b-hf\"\" target=\"\"_blank\"\" aria-label=\"\"CodeLlama 70B. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">CodeLlama 70B<span class=\"\"pop-out-icon\"\"></span></a>, <a href=\"\"https://huggingface.co/codellama/CodeLlama-7b-hf\"\" target=\"\"_blank\"\" aria-label=\"\"CodeLlama 7B. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">CodeLlama 7B<span class=\"\"pop-out-icon\"\"></span></a> and <a href=\"\"https://www.thoughtworks.com/radar/tools/mixtral\"\">Mixtral</a>. Also notable is the engine’s scaling capability: it only takes a couple of config changes to go from running a 7B to a 70B model. If you’re looking to productionize LLMs, vLLM is worth exploring.</p>\""
  },
  {
    "name": "Voyager",
    "ring": "assess",
    "quadrant": "languages-and-frameworks",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://voyager.adriel.cafe/\"\" target=\"\"_blank\"\" aria-label=\"\"Voyager. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Voyager<span class=\"\"pop-out-icon\"\"></span></a></strong> is a navigation library built for Android's <a href=\"\"https://www.thoughtworks.com/radar/languages-and-frameworks/jetpack-compose\"\">Jetpack Compose</a>. It supports several navigation types, including Linear, BottomSheet, Tab and Nested, and its screen model integrates with popular frameworks like <a href=\"\"https://www.thoughtworks.com/radar/languages-and-frameworks/koin\"\">Koin</a> and <a href=\"\"https://www.thoughtworks.com/radar/languages-and-frameworks/jetpack-hilt\"\">Hilt</a>. When using Jetpack Compose in a multiplatform project, Voyager is a good choice to implement a common navigation pattern across all supported platforms. Development on Voyager has picked up again and the library reached version 1.0 in December 2023.</p>\""
  },
  {
    "name": "WGPU",
    "ring": "assess",
    "quadrant": "languages-and-frameworks",
    "isNew": "TRUE",
    "status": "new",
    "description": "\"<p><strong><a href=\"\"https://wgpu.rs/\"\" target=\"\"_blank\"\" aria-label=\"\"wgpu. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">wgpu<span class=\"\"pop-out-icon\"\"></span></a></strong> is a graphics library for <a href=\"\"https://www.thoughtworks.com/radar/languages-and-frameworks/rust\"\">Rust</a> based on the WebGPU API, notable for its capacity to handle general-purpose graphics and compute tasks on the GPU efficiently. wgpu aims to fill the gap left by the phasing out of older graphics standards such as OpenGL and WebGL. It introduces a modern approach to graphics development that spans both native applications and web-based projects. Its integration with <a href=\"\"https://www.thoughtworks.com/radar/languages-and-frameworks/webassembly\"\">WebAssembly</a> further enables graphics and compute applications to run in the browser. wgpu represents a step forward in making advanced graphics programming more accessible to web developers with a range of applications, from gaming to creating sophisticated web animations, positioning wgpu as an exciting technology to assess.</p>\""
  },
  {
    "name": "Zig",
    "ring": "assess",
    "quadrant": "languages-and-frameworks",
    "isNew": "FALSE",
    "status": "no change",
    "description": "\"<p><strong><a href=\"\"https://ziglang.org/\"\" target=\"\"_blank\"\" aria-label=\"\"Zig. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Zig<span class=\"\"pop-out-icon\"\"></span></a></strong> is a new language that shares many attributes with C but with stronger typing, easier memory allocation and support for namespacing, among a host of other features. Zig's aim is to provide a very simple language with straightforward compilation that minimizes side-effects and delivers predictable, easy-to-trace execution. Zig also provides simplified access to <a href=\"\"https://llvm.org/\"\" target=\"\"_blank\"\" aria-label=\"\"LLVM's cross-compilation capability. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">LLVM's cross-compilation capability<span class=\"\"pop-out-icon\"\"></span></a>. Some of our developers have found this feature so valuable they're using Zig as a cross-compiler, even though they’re not writing Zig code. We see teams in the industry using Zig to help build C/C++ toolchains. Zig is a novel language and worth looking into for applications where C is being considered or already in use.</p>\""
  },
  {
    "name": "LangChain",
    "ring": "hold",
    "quadrant": "languages-and-frameworks",
    "isNew": "FALSE",
    "status": "moved out",
    "description": "\"<p>We mentioned some of the emerging criticisms about <strong><a href=\"\"https://www.langchain.com/\"\" target=\"\"_blank\"\" aria-label=\"\"LangChain. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">LangChain<span class=\"\"pop-out-icon\"\"></span></a></strong> in the previous Radar. Since then, we’ve become even more wary of it. While the framework offers a powerful set of features for building applications with large language models (LLMs), we’ve found it to be hard to use and overcomplicated. LangChain gained early popularity and attention in the space, which turned it into a default for many developers. However, as LangChain is trying to evolve and keep up with the fast pace of change, it has become harder and harder to navigate those changes of concepts and patterns as a developer. We’ve also found the API design to be inconsistent and verbose. As such, it often obscures what is actually going on under the hood, making it hard for developers to understand and control how LLMs and the various patterns around them actually work. We’re moving LangChain to the Hold ring to reflect this. In many of our use cases, we’ve found that an implementation with minimum use of specialized frameworks is sufficient. Depending on your use case, you may also want to consider other frameworks such as <a href=\"\"https://www.thoughtworks.com/radar/languages-and-frameworks/semantic-kernel\"\">Semantic Kernel</a>, <a href=\"\"https://haystack.deepset.ai/\"\" target=\"\"_blank\"\" aria-label=\"\"Haystack. This is an external link. Opens in new tab\"\" class=\"\"pop-out\"\">Haystack<span class=\"\"pop-out-icon\"\"></span></a> or <a href=\"\"https://www.thoughtworks.com/radar/tools/litellm\"\">LiteLLM</a>.</p>\""
  }
]
