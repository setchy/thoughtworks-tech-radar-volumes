[
    {
        "name": "API expand-contract",
        "ring": "adopt",
        "quadrant": "techniques",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>The <strong>API expand-contract</strong> pattern, sometimes called <a href=\"\"https://www.martinfowler.com/bliki/ParallelChange.html\"\">parallel change</a>, will be familiar to many, especially when used with databases or code; however, we only see low levels of adoption with APIs. Specifically, we're seeing complex versioning schemes and breaking changes used in scenarios where a simple expand and then contract would suffice. For example, first adding to an API while deprecating an existing element, and then only later removing the deprecated elements once consumers are switched to the newer schema. This approach does require some coordination and visibility of the API consumers, perhaps through a technique such as <a href=\"\"https://martinfowler.com/articles/consumerDrivenContracts.html\"\">consumer-driven contract</a> testing.</p>\""
    },
    {
        "name": "Continuous delivery for machine learning (CD4ML)",
        "ring": "adopt",
        "quadrant": "techniques",
        "isNew": "FALSE",
        "status": "moved in",
        "description": "\"<p>We see <strong><a href=\"\"https://martinfowler.com/articles/cd4ml.html\"\">continuous delivery for machine learning (CD4ML)</a></strong> as a good default starting point for any ML solution that is being deployed into production. Many organizations are becoming more reliant on ML solutions for both customer offerings and internal operations so it makes sound business sense to apply the lessons and good practice captured by <a href=\"\"/radar/techniques/continuous-delivery-cd\"\">continuous delivery (CD)</a> to ML solutions.</p>\""
    },
    {
        "name": "Design systems",
        "ring": "adopt",
        "quadrant": "techniques",
        "isNew": "FALSE",
        "status": "moved in",
        "description": "\"<p>As application development becomes increasingly dynamic and complex, it's a challenge to deliver accessible and usable products with consistent style. This is particularly true in larger organizations with multiple teams working on different products. <strong>Design systems</strong> define a collection of design patterns, component libraries and good design and engineering practices that ensure consistent digital products. Built on the corporate style guides of the past, design systems offer shared libraries and documents that are easy to find and use. Generally, guidance is written down as code and kept under version control so that the guide is less ambiguous and easier to maintain than simple documents. Design systems have become a standard approach when working across teams and disciplines in product development because they allow teams to focus. They can address strategic challenges around the product itself without reinventing the wheel every time a new visual component is needed.</p>\""
    },
    {
        "name": "Platform engineering product teams",
        "ring": "adopt",
        "quadrant": "techniques",
        "isNew": "FALSE",
        "status": "moved in",
        "description": "\"<p>As noted in one of the themes for this edition, the industry is increasingly gaining experience with <strong>platform engineering product teams</strong> that create and support internal platforms. These platforms are used by teams across an organization and accelerate application development, reduce operational complexity and improve time to market. With increasing adoption we're also clearer on both good and bad patterns for this approach. When creating a platform, it’s critical to have clearly defined customers and products that will benefit from it rather than building in a vacuum. We caution against <a href=\"\"/radar/techniques/layered-platform-teams\"\">layered platform teams</a> that simply preserve existing technology silos but apply the \"\"platform team\"\" label as well as against ticket-driven platform operating models. We're still big fans of using concepts from <a href=\"\"https://teamtopologies.com/\"\">Team Topologies</a> as we think about how best to organize platform teams. We consider platform engineering product teams to be a standard approach and a significant enabler for high-performing IT.</p>\""
    },
    {
        "name": "Service account rotation approach",
        "ring": "adopt",
        "quadrant": "techniques",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>We strongly advise organizations to make sure, when they really need to use cloud service accounts, that they are rotating the credentials. Rotation is one of <a href=\"\"/radar/techniques/the-three-rs-of-security\"\">the three R's of security</a>. It is far too easy for organizations to forget about these accounts unless an incident occurs. This is leading to accounts with unnecessarily broad permissions remaining in use for long periods alongside a lack of planning for how to replace or rotate them. Regularly applying a cloud <strong>service account rotation approach</strong> also provides a chance to exercise the principle of least privilege.</p>\""
    },
    {
        "name": "Cloud sandboxes",
        "ring": "trial",
        "quadrant": "techniques",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>As the cloud is becoming more and more a commodity and being able to spin up <strong>cloud sandboxes</strong> is easier and available at scale, our teams prefer cloud-only (as opposed to local) development environments to reduce maintenance complexity. We're seeing that the tooling to do local simulation of cloud-native services limits the confidence in developer build and test cycles; therefore, we're looking to focus on standardizing cloud sandboxes over running cloud-native components on a developer machine. This will drive good <a href=\"\"/radar/techniques/infrastructure-as-code\"\">infrastructure-as-code</a> practices as a forcing function and good onboarding processes for provisioning sandbox environments for developers. There are risks associated with this transition, as it assumes that developers will have an absolute dependency on cloud environment availability, and it may slow down the developer feedback loop. We strongly recommend you adopt some lean governance practices regarding standardization of these sandbox environments, especially with regard to security, IAM and regional deployments.</p>\""
    },
    {
        "name": "Contextual bandits",
        "ring": "trial",
        "quadrant": "techniques",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p><a href=\"\"https://towardsdatascience.com/contextual-bandits-and-reinforcement-learning-6bdfeaece72a\"\"><strong>Contextual bandits</strong></a> is a type of reinforcement learning that is well suited for problems with exploration/exploitation trade-offs. Named after \"\"bandits,\"\" or slot machines, in casinos, the algorithm explores different options to learn more about expected outcomes and balances it by exploiting the options that perform well. We've successfully used this technique in scenarios where we've had little data to train and deploy other machine-learning models. The fact that we can add context to this explore/exploit trade-off makes it suitable for a wide variety of use cases including A/B testing, recommendations and layout optimizations.</p>\""
    },
    {
        "name": "Distroless Docker images",
        "ring": "trial",
        "quadrant": "techniques",
        "isNew": "FALSE",
        "status": "no change",
        "description": "\"<p>When building <a href=\"\"/radar/platforms/docker\"\">Docker</a> images for our applications, we're often concerned with two things: the security and the size of the image. Traditionally, we've used <a href=\"\"/radar/techniques/container-security-scanning\"\">container security scanning</a> tools to detect and patch <a href=\"\"https://cve.mitre.org/\"\">common vulnerabilities and exposures</a> and small distributions such as <a href=\"\"https://alpinelinux.org/\"\">Alpine Linux</a> to address the image size and distribution performance. But with rising security threats, eliminating all possible attack vectors is more important than ever. That's why <strong>distroless Docker images</strong> are becoming the default choice for deployment containers. Distroless Docker images reduce the footprint and dependencies by doing away with a full operating system distribution. This technique reduces security scan noise and the application attack surface. Moreover, fewer vulnerabilities need to be patched and as a bonus, these smaller images are more efficient. Google has published a set of <a href=\"\"https://github.com/GoogleContainerTools/distroless\"\">distroless container images</a> for different languages. You can create distroless application images using the Google build tool <a href=\"\"https://bazel.build/\"\">Bazel</a> or simply use multistage Dockerfiles. Note that distroless containers by default don't have a shell for debugging. However, you can easily find debug versions of distroless containers online, including a <a href=\"\"https://busybox.net/downloads/BusyBox.html\"\">BusyBox shell</a>. Distroless Docker images is a technique pioneered by Google and, in our experience, is still largely confined to Google-generated images. We would be more comfortable if there were more than one provider to choose from. Also, use caution when applying  <a href=\"\"/radar/tools/trivy\"\">Trivy</a> or similar vulnerability scanners since distroless containers are only supported in more recent versions.</p>\""
    },
    {
        "name": "Ethical Explorer",
        "ring": "trial",
        "quadrant": "techniques",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>The group behind <a href=\"\"/radar/techniques/ethical-os\"\">Ethical OS</a> — the Omidyar Network, a self-described social change venture created by eBay founder Pierre Omidyar — has released a new iteration called <strong><a href=\"\"https://ethicalexplorer.org/\"\">Ethical Explorer</a></strong>. The new Ethical Explorer pack draws on lessons learned from using Ethical OS and adds further questions for product teams to consider. The kit, which can be <a href=\"\"https://ethicalexplorer.org/download/\"\">downloaded for free</a> and folded into cards to trigger discussion, has open-ended question prompts for several technical \"\"risk zones,\"\" including surveillance (\"\"can someone use our product or service to track or identify other users?\"\"), disinformation, exclusion, algorithmic bias, addiction, data control, bad actors and outsized power. The included field guide has activities and workshops, ideas for starting conversations and tips for gaining organizational buy-in. While we've a long way to go as an industry to better represent the ethical externalities of our digital society, we've had some productive conversations using Ethical Explorer, and we're encouraged by the broadening awareness of the importance of product decisions in addressing societal issues.</p>\""
    },
    {
        "name": "Hypothesis-driven legacy renovation",
        "ring": "trial",
        "quadrant": "techniques",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>We're often asked to refresh, update or remediate legacy systems that we didn't originally build. Sometimes, technical issues need our attention such as improving performance or reliability. One common approach to address these issues is to create \"\"technical stories\"\" using the same format as a user story but with a technical outcome rather than a business one. But these technical tasks are often difficult to estimate, take longer than anticipated or don't end up having the desired outcome. An alternative, more successful method is to apply <strong>hypothesis-driven legacy renovation</strong>. Rather than working toward a standard backlog, the team takes ownership of a measurable technical outcome and collectively establishes a set of hypotheses about the problem. They then conduct iterative, time-boxed experiments to verify or disprove each hypothesis in order of priority. The resulting workflow is optimized for reducing uncertainty rather than following a plan toward a predictable outcome.</p>\""
    },
    {
        "name": "Lightweight approach to RFCs",
        "ring": "trial",
        "quadrant": "techniques",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>As organizations drive toward <a href=\"\"/radar/techniques/evolutionary-architecture\"\">evolutionary architecture</a>, it's important to capture decisions around design, architecture, techniques and teams' ways of workings. The process of collecting and aggregating feedback that will lead to these decisions begin with Request for Comments (RfCs). RfCs are a technique for collecting context, design and architectural ideas and collaborating with teams to ultimately come to decisions along with their context and consequences. We recommend that organizations take a <strong>lightweight approach to RFCs</strong> by using a simple standardized template across many teams as well as version control to capture RfCs.</p><br><br><p>It's important to capture these in an audit of these decisions to benefit future team members and to capture the technical and business evolution of an organization. Mature organizations have used RfCs in autonomous teams to drive better communication and collaboration especially in cross-team relevant decisions.</p>\""
    },
    {
        "name": "Simplest possible ML",
        "ring": "trial",
        "quadrant": "techniques",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>All major cloud providers offer a dazzling array of machine-learning (ML) solutions. These powerful tools can provide a lot of value, but come at a cost. There is the pure run cost for these services charged by the cloud provider. In addition, there is a kind of operations tax. These complex tools need to be understood and operated, and with each new tool added to the architecture this tax burden increases. In our experience, teams often choose complex tools because they underestimate the power of simpler tools such as linear regression. Many ML problems don't require a GPU or neural networks. For that reason we advocate for the <strong>simplest possible ML</strong>, using simple tools and models and a few hundred lines of Python on the compute platform you have at hand. Only reach for the complex tools when you can demonstrate the need for them.</p>\""
    },
    {
        "name": "SPA injection",
        "ring": "trial",
        "quadrant": "techniques",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>The <a href=\"\"https://martinfowler.com/bliki/StranglerFigApplication.html\"\">strangler fig pattern</a> is often the default strategy for legacy modernization, where the new code wraps around the old and slowly absorbs the ability to handle all the needed functionality. That sort of \"\"outside-in\"\" approach works well for a number of legacy systems, but now that we've had enough experience with single-page applications (SPA) for them to become legacy systems themselves, we're seeing the opposite \"\"inside-out\"\" approach used to replace them. Instead of wrapping the legacy system, we instead embed the beginning of the new SPA into the HTML document containing the old one and let it slowly expand in functionality. The SPA frameworks don't even need to be the same as long as users can tolerate the performance hit of the increased page size (e.g., embedding a new <a href=\"\"/radar/languages-and-frameworks/react-js\"\">React</a> app inside an old <a href=\"\"/radar/languages-and-frameworks/angularjs\"\">AngularJS</a> one). <strong>SPA injection</strong> allows you to iteratively remove the old SPA until the new one completely takes over. Whereas a strangler fig can be viewed as a type of parasite that uses the host tree's stable external surface to support itself until it takes root and the host itself dies, this approach is more like injecting an outside agent into the host, relying on functionality of the original SPA until it can completely take over.</p>\""
    },
    {
        "name": "Team cognitive load",
        "ring": "trial",
        "quadrant": "techniques",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>A system's architecture mimics organizational structure and its communication. It's not big news that we should be intentional about how teams interact — see, for instance, the <a href=\"\"/radar/techniques/inverse-conway-maneuver\"\">Inverse Conway Maneuver</a>. Team interaction is one of the variables for how fast and how easily teams can deliver value to their customers. We were happy to find a way to measure these interactions; we used the <a href=\"\"https://teamtopologies.com/book\"\">Team Topologies</a> author's <a href=\"\"https://github.com/TeamTopologies/Team-Cognitive-Load-Assessment\"\">assessment</a> which gives you an understanding of how easy or difficult the teams find it to build, test and maintain their services. By measuring <strong>team cognitive load</strong>, we could better advise our clients on how to change their teams' structure and evolve their interactions.</p>\""
    },
    {
        "name": "Tool-managed Xcodeproj",
        "ring": "trial",
        "quadrant": "techniques",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>Many of our developers coding iOS in Xcode often get headaches because the Xcodeproj file changes with every project change. The Xcodeproj file format is not human-readable, hence trying to handle merge conflicts is quite complicated and can lead to productivity loss and risk of messing up the entire project — if anything goes wrong with the file, Xcode won't work properly and developers will very likely be blocked. Instead of trying to merge and fix the file manually or version it, we recommend you use a <strong>tool-managed Xcodeproj</strong> approach: Define your Xcode project configuration in YAML (<a href=\"\"https://github.com/yonaskolb/XcodeGen\"\">XcodeGen</a>, <a href=\"\"https://github.com/lyptt/struct\"\">Struct</a>), Ruby (<a href=\"\"https://github.com/igor-makarov/xcake\"\">Xcake</a>) or Swift (<a href=\"\"https://github.com/tuist/tuist\"\">Tuist</a>). These tools generate the Xcodeproj file based on a configuration file and the project structure. As a result, merge conflicts in the Xcodeproj file will be a thing of the past, and when they do happen in the configuration file, they're much easier to handle.</p>\""
    },
    {
        "name": "UI/BFF shared types",
        "ring": "trial",
        "quadrant": "techniques",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>With <a href=\"\"/radar/languages-and-frameworks/typescript\"\">TypeScript</a> becoming a common language for front-end development and <a href=\"\"/radar/platforms/node-js\"\">Node.js</a> becoming the preferred <a href=\"\"/radar/techniques/bff-backend-for-frontends\"\">BFF</a> technology, we're seeing increasing use of <strong>UI/BFF shared types</strong>. In this technique, a single set of type definitions is used to define both the data objects returned by front-end queries and the data served to satisfy those queries by the back-end server. Ordinarily, we would be cautious about this practice because of the unnecessarily tight coupling it creates across process boundaries. However, many teams are finding that the benefits of this approach outweigh any risks of tight coupling. Since the BFF pattern works best when the same team owns both the UI code and the BFF, often storing both components in the same repository, the UI/BFF pair can be viewed as a single cohesive system. When the BFF offers strongly typed queries, the results can be tailored to the specific needs of the frontend rather than reusing a single, general-purpose entity that must serve the needs of many consumers and contain more fields than actually required. This reduces the risk of accidentally exposing data that the user shouldn't see, prevents incorrect interpretation of the returned data object and makes the query more expressive. This practice is particularly useful when implemented with <a href=\"\"/radar/languages-and-frameworks/io-ts\"\">io-ts</a> to enforce the run-time type safety.</p>\""
    },
    {
        "name": "Bounded low-code platforms",
        "ring": "assess",
        "quadrant": "techniques",
        "isNew": "FALSE",
        "status": "no change",
        "description": "\"<p>One of the most nuanced decisions facing companies at the moment is the adoption of low-code or no-code platforms, that is, platforms that solve very specific problems in very limited domains. Many vendors are pushing aggressively into this space. The problems we see with these platforms typically relate to an inability to apply good engineering practices such as versioning. Testing too is typically really hard. However, we noticed some interesting new entrants to the market — including <a href=\"\"https://www.honeycode.aws/\"\">Amazon Honeycode</a>, which makes it easy to create simple task or event management apps, and <a href=\"\"https://parabola.io/\"\">Parabola</a> for IFTTT-like cloud workflows — which is why we're once again including <strong>bounded low-code platforms</strong> in this volume. Nevertheless, we remain deeply skeptical about their wider applicability since these tools, like Japanese Knotweed, have a knack of escaping their bounds and tangling everything together. That's why we still strongly advise caution in their adoption.</p>\""
    },
    {
        "name": "Decentralized identity",
        "ring": "assess",
        "quadrant": "techniques",
        "isNew": "FALSE",
        "status": "no change",
        "description": "\"<p>In 2016, Christopher Allen, a key contributor to <a href=\"\"https://en.wikipedia.org/wiki/Transport_Layer_Security\"\">SSL/TLS</a>, inspired us with an introduction of 10 principles underpinning a new form of digital identity and a path to get there, <a href=\"\"http://www.lifewithalacrity.com/2016/04/the-path-to-self-soverereign-identity.html\"\">the path to self-sovereign identity</a>. Self-sovereign identity, also known as <strong>decentralized identity</strong>, is a “lifetime portable identity for any person, organization, or thing that does not depend on any centralized authority and can never be taken away,” according to the <a href=\"\"/radar/platforms/trust-over-ip-stack\"\">Trust over IP</a> standard. Adopting and implementing decentralized identity is gaining momentum and becoming attainable. We see its adoption in privacy-respecting <a href=\"\"https://www.civic.com/healthkey/\"\">customer health applications</a>, <a href=\"\"https://www.truu.id/\"\">government healthcare infrastructure</a> and <a href=\"\"https://id-bulletin.com/2020/06/04/news-gleif-and-evernym-demo-organization-wallets-to-deliver-trust-and-transparency-in-digital-business/\"\">corporate legal identity</a>. If you want to rapidly get started with decentralized identity, you can assess <a href=\"\"https://sovrin.org/\"\">Sovrin Network</a>, <a href=\"\"https://github.com/hyperledger/aries\"\">Hyperledger Aries</a> and <a href=\"\"https://github.com/hyperledger/indy-node\"\">Indy</a> OSS, as well as <a href=\"\"https://www.w3.org/TR/did-core/\"\">decentralized identifiers</a> and <a href=\"\"/radar/techniques/verifiable-credentials\"\">verifiable credentials</a> standards. We're watching this space closely as we help our clients with their strategic positioning in the new era of digital trust.</p>\""
    },
    {
        "name": "Deployment drift radiator",
        "ring": "assess",
        "quadrant": "techniques",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>A <strong>deployment drift radiator</strong> makes version drift visible for deployed software across multiple environments. Organizations using automated deployments may require manual approvals for environments that get closer to production, meaning the code in these environments might well be lagging several versions behind current development. This technique makes this lag visible via a simple dashboard showing how far behind each deployed component is for each environment. This helps to highlight the opportunity cost of completed software not yet in production while drawing attention to related risks such as security fixes not yet deployed.</p>\""
    },
    {
        "name": "Homomorphic encryption",
        "ring": "assess",
        "quadrant": "techniques",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>Fully <strong><a href=\"\"https://en.wikipedia.org/wiki/Homomorphic_encryption\"\">homomorphic encryption</a></strong> (HE) refers to a class of encryption methods that allow computations (such as search and arithmetic) to be performed directly on encrypted data. The result of such a computation remains in encrypted form, which at a later point can be decrypted and revealed. Although the HE problem was first proposed in 1978, a solution wasn't constructed until 2009. With advances in computing power and the availability of easy-to-use open-source libraries — including <a href=\"\"https://github.com/microsoft/SEAL#introduction\"\">SEAL</a>, <a href=\"\"https://github.com/ldsec/lattigo\"\">Lattigo</a>, <a href=\"\"https://github.com/homenc/HElib\"\">HElib</a> and <a href=\"\"https://github.com/data61/python-paillier\"\">partially homomorphic encryption in Python</a> — HE is becoming feasible in real-world applications. The motivating scenarios include privacy-preserving use cases, where computation can be outsourced to an untrusted party, for example, running computation on encrypted data in the cloud, or enabling a third party to aggregate homomorphically encrypted intermediate <a href=\"\"https://en.wikipedia.org/wiki/Federated_learning\"\">federated machine learning</a> results. Moreover, most HE schemes are considered to be <a href=\"\"https://csrc.nist.gov/Projects/Post-Quantum-Cryptography\"\">secure against quantum computers</a>, and efforts are underway to <a href=\"\"https://homomorphicencryption.org/standard/\"\">standardize</a> HE. Despite its current limitations, namely performance and feasibility of the types of computations, HE is worth your attention.</p>\""
    },
    {
        "name": "Hotwire",
        "ring": "assess",
        "quadrant": "techniques",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p><strong><a href=\"\"https://hotwire.dev/\"\">Hotwire</a></strong> (HTML over the wire) is a technique to build web applications. Pages are constructed out of components, but unlike modern SPAs the HTML for the components is generated on the server side and then sent \"\"over the wire\"\" to the browser. The application has only a small amount of JavaScript code in the browser to stitch the HTML fragments together. Our teams, and doubtlessly others too, experimented with this technique after asynchronous web requests gained cross-browser support around 2005, but for various reasons it never gained much traction.</p><br><br><p>Today, Hotwire uses modern web browser and HTTP capabilities to achieve the speed, responsiveness and dynamic nature of single-page apps (SPAs). It embraces simpler web application design by localizing the logic to the server and keeping the client-side code simple. The team at Basecamp has released a few Hotwire frameworks that power their own <a href=\"\"https://hey.com/\"\">application</a>, including <a href=\"\"https://turbo.hotwired.dev/\"\">Turbo</a> and <a href=\"\"https://stimulus.hotwire.dev/\"\">Stimulus</a>. Turbo includes a set of techniques and frameworks to speed up the application responsiveness by preventing whole page reloading, page preview from cache and decomposing the page into fragments with progressive enhancements on request. Stimulus is designed to enhance static HTML in the browser by connecting JavaScript objects to the page elements on the HTML.</p>\""
    },
    {
        "name": "Import maps for micro frontends",
        "ring": "assess",
        "quadrant": "techniques",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>When composing an application out of several <a href=\"\"/radar/techniques/micro-frontends\"\">micro frontends</a>, some part of the system needs to decide which micro frontends to load and where to load them from. So far, we've either built custom solutions or relied on a broader framework like <a href=\"\"/radar/languages-and-frameworks/single-spa\"\">single-spa</a>. Now there is a new standard, <a href=\"\"https://github.com/WICG/import-maps\"\">import maps</a>, that helps in both cases. Our first experiences show that using <strong>import maps for micro frontends</strong> allows for a neat separation of concerns. The JavaScript code states what to import and a small script tag in the initial HTML response specifies where to load the frontends from. That HTML is obviously generated on the server side, which makes it possible to use some dynamic configuration during its rendering. In many ways this technique reminds us of linker/loader paths for dynamic Unix libraries. At the moment import maps are only supported by Chrome, but with the <a href=\"\"https://github.com/systemjs/systemjs\"\">SystemJS</a> polyfill they're ready for wider use.</p>\""
    },
    {
        "name": "Open Application Model (OAM)",
        "ring": "assess",
        "quadrant": "techniques",
        "isNew": "FALSE",
        "status": "no change",
        "description": "\"<p>The <strong><a href=\"\"https://oam.dev/\"\">Open Application Model (OAM)</a></strong> is an attempt to bring some standardization to the space of shaping infrastructure platforms as products. Using the abstractions of components, application configurations, scopes and traits, developers can describe their applications in a platform-agnostic way, while platform implementers define their platform in terms of workload, trait and scope. Since we last talked about the OAM, we've followed one of its first implementations with interest, <a href=\"\"https://kubevela.io/\"\">KubeVela</a>. KubeVela is close to release 1.0, and we're curious to see if implementations like this can substantiate the promise of the OAM idea.</p>\""
    },
    {
        "name": "Privacy-focused web analytics",
        "ring": "assess",
        "quadrant": "techniques",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p><strong>Privacy-focused web analytics</strong> is a technique for gathering web analytics without compromising end user privacy by keeping the end users truly anonymous. One surprising consequence of General Data Protection Regulation (GDPR) compliance is the decision taken by many organizations to degrade the user experience with complex cookie consent processes, especially when the user doesn't immediately consent to the \"\"all the cookies\"\" default settings. Privacy-focused web analytics has the dual benefit of both observing the spirit and letter of GDPR while also avoiding the need to introduce intrusive cookie consent forms. One implementation of this approach is <a href=\"\"https://plausible.io/\"\">Plausible</a>.</p>\""
    },
    {
        "name": "Remote mob programming",
        "ring": "assess",
        "quadrant": "techniques",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>Mob programming is one of those techniques that our teams have found to be easier when done remotely. <strong>Remote mob programming</strong> is allowing teams to quickly \"\"mob\"\" around an issue or piece of code without the physical constraints of only being able to fit so many people around a pairing station. Teams can quickly collaborate on an issue or piece of code without having to connect to a big display, book a physical meeting room or find a whiteboard.</p>\""
    },
    {
        "name": "Secure multiparty computing",
        "ring": "assess",
        "quadrant": "techniques",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p><strong><a href=\"\"https://en.wikipedia.org/wiki/Secure_multi-party_computation\"\">Secure multiparty computing</a></strong> (MPC) solves the problem of collaborative computing that protects privacy between parties that do not trust each other. It's aim is to safely calculate an agreed-upon problem without a trusted third party, while each participant is required to partake in the calculation result and can't be obtained by other entities. A simple illustration for MPC is the <a href=\"\"https://en.wikipedia.org/wiki/Yao%27s_Millionaires%27_problem\"\">millionaires' problem</a>: two millionaires want to understand who is the richest, but neither want to share their actual net worth with each other nor trust a third party. The implementation approaches of MPC vary; scenarios may include secret sharing, oblivious transfer, garbled circuits or <a href=\"\"/radar/techniques/homomorphic-encryption\"\">homomorphic encryption</a>. Some commercial MPC solutions that have recently appeared (e.g., <a href=\"\"https://www.antchain.net/solutions/morse\"\">Antchain Morse</a>) claim to help solve the problems of secret sharing and secure machine learning in scenarios such as multiparty joint credit investigation and medical records data exchange. Although these platforms are attractive from a marketing perspective, we've yet to see whether they're really useful.</p>\""
    },
    {
        "name": "GitOps",
        "ring": "hold",
        "quadrant": "techniques",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>We suggest approaching <strong>GitOps</strong> with a degree of care, especially with regard to branching strategies. GitOps can be seen as a way of implementing <a href=\"\"/radar/techniques/infrastructure-as-code\"\">infrastructure as code</a> that involves continuously synchronizing and applying infrastructure code from <a href=\"\"/radar/tools/git\"\">Git</a> into various environments. When used with a \"\"branch per environment\"\" infrastructure, changes are promoted from one environment to the next by merging code. While treating code as the single source of truth is clearly a sound approach, we're seeing branch per environment lead to environmental drift and eventually environment-specific configs as code merges become problematic or even stop entirely. This is very similar to what we've seen in the past with <a href=\"\"/radar/techniques/long-lived-branches-with-gitflow\"\">long-lived branches with GitFlow</a>.</p>\""
    },
    {
        "name": "Layered platform teams",
        "ring": "hold",
        "quadrant": "techniques",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>The explosion of interest around software platforms has created a lot of value for organizations, but the path to building a platform-based delivery model is fraught with potential dead ends. It's common in the excitement of new paradigms to see a resurgence of older techniques rebranded with the new vernacular, making it easy to lose sight of the reasons we moved past those techniques in the first place. For an example of this rebranding, see our blip on traditional <a href=\"\"/radar/techniques/esbs-in-api-gateway-s-clothing\"\">ESBs make a comeback as API gateways</a> in the previous Radar. Another example we're seeing is rehashing the approach of dividing teams by technology layer but calling them platforms. In the context of building an application, it used to be common to have a front-end team separate from the business logic team separate from the data team, and we see analogs to that model when organizations segregate platform capabilities among teams dedicated to a business or data layer. Thanks to <a href=\"\"https://www.thoughtworks.com/insights/articles/demystifying-conways-law\"\">Conway's Law</a>, we know that organizing platform capability teams around <a href=\"\"https://martinfowler.com/articles/microservices.html#OrganizedAroundBusinessCapabilities\"\">business capabilities</a> is a more effective model, giving the team end-to-end ownership of the capability, including data ownership. This helps to avoid the dependency management headaches of <strong>layered platform teams</strong>, with the front-end team waiting on the business logic team waiting on the data team to get anything done.</p>\""
    },
    {
        "name": "Naive password complexity requirements",
        "ring": "hold",
        "quadrant": "techniques",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>Password policies are a standard default for many organizations today. However, we're still seeing organizations requiring passwords to include a variety of symbols, numbers, uppercase and lowercase letters as well as inclusion of special characters. These are <strong>naive password complexity requirements</strong> that lead to a false sense of security as users will opt for more insecure passwords because the alternative is difficult to remember and type. According to <a href=\"\"https://pages.nist.gov/800-63-3/sp800-63b.html\"\">NIST recommendations</a>, the primary factor in password strength is password length, and therefore users should choose long passphrases with a maximum requirement of 64 characters (including spaces). These passphrases are more secure and memorable.</p>\""
    },
    {
        "name": "Peer review equals pull request",
        "ring": "hold",
        "quadrant": "techniques",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>Some organizations seem to think <strong>peer review equals pull request</strong>; they've taken the view that the only way to achieve a peer review of code is via a pull request. We've seen this approach create significant team bottlenecks as well as significantly degrade the quality of feedback as overloaded reviewers begin to simply reject requests. Although the argument could be made that this is one way to demonstrate code review \"\"regulatory compliance\"\" one of our clients was told this was invalid since there was no evidence the code was actually read by anyone prior to acceptance. Pull requests are only one way to manage the code review workflow; we urge people to consider other approaches, especially where there is a need to coach and pass on feedback carefully.</p>\""
    },
    {
        "name": "SAFe™",
        "ring": "hold",
        "quadrant": "techniques",
        "isNew": "FALSE",
        "status": "no change",
        "description": "\"<p>Our positioning regarding \"\"being agile before doing agile\"\" and our opinions around this topic shouldn't come as a surprise; but since <strong><a href=\"\"http://www.scaledagileframework.com/\"\">SAFe™</a></strong> (Scaled Agile Framework®), per Gartner’s May 2019<a href=\"\"http://go.scaledagile.com/Gartner-a.html\"\"> report</a>, is the most considered and most used enterprise agile framework, and since we're seeing more and more enterprises going through organizational changes, we thought it was time to raise awareness on this topic again. We've come across organizations struggling with SAFe's over-standardized, phase-gated processes. Those processes create friction in the organizational structure and its operating model. It can also promote silos in the organization, preventing platforms from becoming real business capabilities enablers. The top-down control generates waste in the value stream and discourages engineering talent creativity, while limiting autonomy and experimentation in the teams. Rather than measuring effort and focusing on standardized ceremonies, we recommend a leaner, value-driven approach and governance to help eliminate organizational friction such as <a href=\"\"https://www.thoughtworks.com/books/edge\"\">EDGE</a>, as well as a <a href=\"\"/radar/techniques/team-cognitive-load\"\">team cognitive load</a> assessment to identify types of teams and determine how they should better interact with each other.</p><br><br><p>Scaled Agile Framework® and SAFe™ are trademarks of Scaled Agile, Inc.</p>\""
    },
    {
        "name": "Separate code and pipeline ownership",
        "ring": "hold",
        "quadrant": "techniques",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>Ideally, but especially when teams are practicing DevOps, the deployment pipeline and the code being deployed should be owned by the same team. Unfortunately, we still see organizations where there is <strong>separate code and pipeline ownership</strong>, with the deployment pipeline configuration owned by the infrastructure team; this results in delays to changes, barriers to improvements and a lack of development team ownership and involvement in deployments. One cause of this can clearly be the separate team, another can be the desire to retain “gatekeeper” processes and roles. Although there can be legitimate reasons for using this approach (e.g., regulatory control), in general we find it painful and unhelpful.</p>\""
    },
    {
        "name": "Ticket-driven platform operating models",
        "ring": "hold",
        "quadrant": "techniques",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>One of the ultimate goals of a platform should be to reduce ticket-based processes to an absolute minimum, as they create queues in the value stream. Sadly, we still see organizations not pushing forcefully enough toward this important goal, resulting in a <strong>ticket-driven platform operating model</strong>. This is particularly frustrating when ticket-based processes are put in front of platforms that are built on top of the self-service and API-driven features of public cloud vendors. It's hard and not necessary to achieve self-service with very few tickets right from the start, but it needs to be the destination.</p><br><br><p>Over-reliance on bureaucracy and lack of trust are among the causes of this reluctance to move away from ticket-based processes. Baking more automated checks and alerts into your platform is one way to help cut the cord from approval processes with tickets. For example, <a href=\"\"/radar/techniques/run-cost-as-architecture-fitness-function\"\">provide teams with visibility into their run costs</a> and put in automated guardrails to avoid accidental explosion of costs. Implement <a href=\"\"/radar/techniques/security-policy-as-code\"\">security policy as code</a> and use <a href=\"\"/radar/techniques/infrastructure-configuration-scanner\"\">configuration scanners</a> or analyzers like <a href=\"\"/radar/tools/recommender\"\">Recommender</a> to help teams do the right thing.</p>\""
    },
    {
        "name": "AWS Cloud Development Kit",
        "ring": "trial",
        "quadrant": "platforms",
        "isNew": "FALSE",
        "status": "moved in",
        "description": "\"<p>Many of our teams who are already on AWS have found <strong><a href=\"\"https://docs.aws.amazon.com/cdk/latest/guide/home.html\"\">AWS Cloud Development Kit</a></strong> (AWS CDK) to be a sensible AWS default for enabling infrastructure provisioning. In particular, they like the use of first-class programming languages instead of configuration files which allows them to use existing tools, test approaches and skills. Like similar tools, care is still needed to ensure deployments remain easy to understand and maintain. The development kit currently supports <a href=\"\"/radar/languages-and-frameworks/typescript\"\">TypeScript</a>, JavaScript, Python, Java, C# and .NET. New providers are being added to the CDK core. We've also used both AWS Cloud Development Kit and HashiCorp's <a href=\"\"https://learn.hashicorp.com/tutorials/terraform/cdktf\"\">Cloud Development Kit for Terraform</a> to generate Terraform configurations and enable provisioning with the Terraform platform with success.</p>\""
    },
    {
        "name": "Backstage",
        "ring": "trial",
        "quadrant": "platforms",
        "isNew": "FALSE",
        "status": "moved in",
        "description": "\"<p>We continue to see interest in and use of <strong><a href=\"\"https://backstage.io/\"\">Backstage</a></strong> grow, alongside the adoption of developer portals, as organizations look to support and streamline their development environments. As the number of tools and technologies increases, some form of standardization is becoming increasingly important for consistency so that developers are able to focus on innovation and product development instead of getting bogged down with reinventing the wheel. Backstage is an open-source developer portal platform created by Spotify, it's based upon software templates, unifying infrastructure tooling and consistent and centralized technical documentation. The plugin architecture allows for extensibility and adaptability into an organization’s infrastructure ecosystem.</p>\""
    },
    {
        "name": "Delta Lake",
        "ring": "trial",
        "quadrant": "platforms",
        "isNew": "FALSE",
        "status": "moved in",
        "description": "\"<p><strong><a href=\"\"https://delta.io/\"\">Delta Lake</a></strong> is an <a href=\"\"https://github.com/delta-io/delta\"\">open-source storage layer</a>, implemented by Databricks, that attempts to bring ACID transactions to big data processing. In our Databricks-enabled <a href=\"\"/radar/techniques/data-lake\"\">data lake</a> or <a href=\"\"/radar/techniques/data-mesh\"\">data mesh</a> projects, our teams continue to prefer using Delta Lake storage over the direct use of file storage types such <a href=\"\"https://aws.amazon.com/s3/\"\">S3</a> or <a href=\"\"https://azure.microsoft.com/en-au/services/storage/data-lake-storage/\"\">ADLS</a>. Of course this is limited to projects that use storage platforms that support <a href=\"\"https://docs.delta.io/latest/delta-storage.html\"\">Delta Lake</a> when using <a href=\"\"https://parquet.apache.org/\"\">Parquet</a> file formats. Delta Lake facilitates concurrent data read/write use cases where file-level transactionality is required. We find Delta Lake's seamless integration with Apache Spark <a href=\"\"https://docs.databricks.com/delta/delta-batch.html\"\">batch</a> and <a href=\"\"https://docs.databricks.com/delta/delta-streaming.html\"\">micro-batch</a> APIs greatly helpful, particularly features such as <a href=\"\"https://databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html\"\">time travel</a> — accessing data at a particular point in time or commit reversion — as well as <a href=\"\"https://databricks.com/blog/2019/09/24/diving-into-delta-lake-schema-enforcement-evolution.html\"\">schema evolution</a> support on write; though there are some limitations on these features.</p>\""
    },
    {
        "name": "Materialize",
        "ring": "trial",
        "quadrant": "platforms",
        "isNew": "FALSE",
        "status": "moved in",
        "description": "\"<p><strong><a href=\"\"https://materialize.io/\"\">Materialize</a></strong> is a streaming database that enables you to do incremental computation without complicated data pipelines. Just describe your computations via standard SQL views and connect Materialize to the data stream. The underlying differential data flow engine performs incremental computation to provide consistent and correct output with minimal latency. Unlike traditional databases, there are no restrictions in defining these materialized views, and the computations are executed in real time. We've used Materialize, together with Spring Cloud Stream and Kafka, to query over streams of events for insights in a distributed event-driven system, and we quite like the setup.</p>\""
    },
    {
        "name": "Snowflake",
        "ring": "trial",
        "quadrant": "platforms",
        "isNew": "FALSE",
        "status": "no change",
        "description": "\"<p>Since we last mentioned <strong><a href=\"\"https://www.snowflake.com/\"\">Snowflake</a></strong> in the Radar, we've gained more experience with it as well as with <a href=\"\"/radar/techniques/data-mesh\"\">data mesh</a> as an alternative to data warehouses and lakes. Snowflake continues to impress with features like time travel, zero-copy cloning, data sharing and its marketplace. We also haven't found anything we don't like about it, all of which has led to our consultants generally preferring it over the alternatives. Redshift is moving toward storage and compute separation, which has been a strong point of Snowflake, but even with Redshift Spectrum it isn't as convenient and flexible to use, partly because it is bound by its Postgres heritage (we do still like <a href=\"\"/radar/platforms/postgresql-for-nosql\"\">Postgres</a>, by the way). Federated queries can be a reason to go with Redshift. When it comes to operations, Snowflake is much simpler to run. <a href=\"\"/radar/platforms/bigquery\"\">BigQuery</a>, which is another alternative, is very easy to operate, but in a multicloud setup Snowflake is a better choice. We can also report that we've used Snowflake successfully with <a href=\"\"/radar/platforms/google-cloud-platform\"\">GCP</a>, <a href=\"\"/radar/platforms/aws\"\">AWS</a>, and <a href=\"\"/radar/platforms/azure\"\">Azure</a>.</p>\""
    },
    {
        "name": "Variable fonts",
        "ring": "trial",
        "quadrant": "platforms",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p><strong>Variable fonts</strong> are a way of avoiding the need to find and include separate font files for different weights and styles. Everything is in one font file, and you can use properties to select which style and weight you need. While not new, we still see sites and projects that could benefit from this simple approach. If you have pages that are including many variations of the same font, we suggest trying out variable fonts.</p>\""
    },
    {
        "name": "Apache Pinot",
        "ring": "assess",
        "quadrant": "platforms",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p><strong><a href=\"\"https://pinot.apache.org/\"\">Apache Pinot</a></strong> is a distributed OLAP data store, built to deliver real-time analytics with low latency. It can ingest from batch data sources (such as Hadoop HDFS, Amazon S3, Azure ADLS or Google Cloud Storage) as well as stream data sources (such as Apache Kafka). If the need is user-facing, low-latency analytics, SQL-on-Hadoop solutions don't offer the low latency that is needed. Modern OLAP engines like Apache Pinot (or <a href=\"\"https://druid.apache.org/\"\">Apache Druid</a> and <a href=\"\"https://clickhouse.tech/\"\">Clickhouse</a> among others) can achieve much lower latency and are particularly suited in contexts where fast analytics, such as aggregations, are needed on immutable data, possibly, with real-time data ingestion. Originally built by LinkedIn, Apache Pinot entered Apache incubation in late 2018 and has since added a plugin architecture and SQL support among other key capabilities. Apache Pinot can be fairly complex to operate and has many moving parts, but if your data volumes are large enough and you need low-latency query capability, we recommend you assess Apache Pinot.</p>\""
    },
    {
        "name": "Bit.dev",
        "ring": "assess",
        "quadrant": "platforms",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p><strong><a href=\"\"https://bit.dev/\"\">Bit.dev</a></strong> is a cloud-hosted collaborative platform for UI components extracted, modularized and reused with <a href=\"\"https://github.com/teambit/bit\"\">Bit</a>. <a href=\"\"/radar/platforms/web-components-standard\"\">Web components</a> have been around for a while, but building a modern front-end application by assembling small, independent components extracted from other projects has never been easy. Bit was designed to let you do exactly that: extract a component from an existing library or project. You can either build your own service on top of Bit for component collaboration or use Bit.dev.</p>\""
    },
    {
        "name": "DataHub",
        "ring": "assess",
        "quadrant": "platforms",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>Since we first mentioned <a href=\"\"/radar/techniques/data-discoverability\"\">data discoverability</a> in the Radar, LinkedIn has evolved <a href=\"\"https://engineering.linkedin.com/blog/2016/03/open-sourcing-wherehows--a-data-discovery-and-lineage-portal\"\">WhereHows</a> to <strong><a href=\"\"https://github.com/linkedin/datahub\"\">DataHub</a></strong>, the next generation platform that addresses data discoverability via an extensible metadata system. Instead of crawling and pulling metadata, DataHub adopts a push-based model where individual components of the data ecosystem publish metadata via an API or a stream to the central platform. This push-based integration shifts the ownership from the central entity to individual teams making them accountable for their metadata. As more and more companies are trying to become data driven, having a system that helps with data discovery and understanding data quality and lineage is critical, and we recommend you assess DataHub in that capacity.</p>\""
    },
    {
        "name": "Feature Store",
        "ring": "assess",
        "quadrant": "platforms",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p><strong><a href=\"\"https://www.featurestore.org/\"\">Feature Store</a></strong> is an ML-specific data platform that addresses some of the key challenges we face today in feature engineering with three fundamental capabilities: (1) it uses managed data pipelines to remove struggles with pipelines as new data arrives; (2) catalogs and stores feature data to promote discoverability and collaboration of features across models; and (3) consistently serves feature data during training and interference. </p><br><br><p>Since Uber revealed their <a href=\"\"https://eng.uber.com/michelangelo-machine-learning-platform/\"\">Michelangelo platform</a>, many organizations and startups have built their own versions of a feature store; examples include <a href=\"\"https://github.com/logicalclocks/hopsworks\"\">Hopsworks</a>, <a href=\"\"https://github.com/feast-dev/feast\"\">Feast</a> and <a href=\"\"https://www.tecton.ai/\"\">Tecton</a>. We see potential in Feature Store and recommend you carefully assess it.</p>\""
    },
    {
        "name": "JuiceFS",
        "ring": "assess",
        "quadrant": "platforms",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p><strong><a href=\"\"https://github.com/juicedata/juicefs\"\">JuiceFS</a></strong> is an open-source, distributed POSIX file system built on top of <a href=\"\"https://www.thoughtworks.com/radar/platforms/redis\"\">Redis</a> and an object store service (for example, Amazon S3). If you're building new applications, then our recommendation has always been to interact directly with the object store without going through another abstraction layer. However, JuiceFS can be an option if you're migrating legacy applications that depend on traditional POSIX file systems to the cloud.</p>\""
    },
    {
        "name": "Kafka API without Kafka",
        "ring": "assess",
        "quadrant": "platforms",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>As more businesses turn to events as a way to share data among microservices, collect analytics or feed data lakes, <a href=\"\"/radar/tools/apache-kafka\"\">Apache Kafka</a> has become a favorite platform to support an event-driven architectural style. Although Kafka was a revolutionary concept in scalable persistent messaging, a lot of moving parts are required to make it work, including ZooKeeper, brokers, partitions, and mirrors. While these can be particularly tricky to implement and operate, they do offer great flexibility and power when needed, especially at an industrial enterprise scale. Because of the high barrier to entry presented by the full Kafka ecosystem, we welcome the recent explosion of platforms offering the <strong>Kafka API without Kafka</strong>. Recent entries such as <a href=\"\"https://github.com/streamnative/kop\"\">Kafka on Pulsar</a> and <a href=\"\"/radar/platforms/redpanda\"\">Redpanda</a> offer alternative architectures, and <a href=\"\"https://github.com/Azure/azure-event-hubs-for-kafka\"\">Azure Event Hubs for Kafka</a> provides some compatibility with Kafka producer and consumer APIs. Some features of Kafka, like the streams client library, are not compatible with these alternative brokers, so there are still reasons to choose Kafka over alternative brokers. It remains to be seen, however, if developers actually adopt this strategy or if it is merely an attempt by competitors to lure users away from the Kafka platform. Ultimately, perhaps Kafka's most enduring impact could be the convenient protocol and API provided to clients.</p>\""
    },
    {
        "name": "NATS",
        "ring": "assess",
        "quadrant": "platforms",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p><a href=\"\"https://nats.io/about/\"\"><strong>NATS</strong></a> is a fast, secure message queueing system with an unusually wide range of features and potential deployment targets. At first glance, you would be forgiven for asking why the world needs another message queueing system. Message queues have been around in various forms for nearly as long as businesses have been using computers and have undergone years of refinement and optimization for various tasks. But NATS has several interesting characteristics and is unique in its ability to scale from embedded controllers to global, cloud-hosted superclusters. We're particularly intrigued by NATS's intent to support a continuous streaming flow of data from mobile devices and IoT and through a network of interconnected systems. However, some tricky issues need to be addressed, not the least of which is ensuring consumers see only the messages and topics to which they're allowed access, especially when the network spans organizational boundaries. NATS 2.0 introduced a security and access control framework that supports multitenant clusters where accounts restrict a user's access to queues and topics. Written in Go, NATS has primarily been embraced by the Go language community. Although clients exist for pretty much all widely used programming languages, the Go client is by far the most popular. However, some of our developers have found that all the language client libraries tend to reflect the Go origins of codebase. Increasing bandwidth and processing power on small, wireless devices means that the volume of data businesses must consume in real time will only increase. Assess NATS as a possible platform for streaming that data within and among businesses.</p>\""
    },
    {
        "name": "Opstrace",
        "ring": "assess",
        "quadrant": "platforms",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p><strong><a href=\"\"https://opstrace.com/\"\">Opstrace</a></strong> is an open-source observability platform intended to be deployed in the user's own network. If we don't use commercial solutions like Datadog (for example, because of cost or data residency concerns), the only solution is to build your own platform composed of open-source tools. This can take a lot of effort — Opstrace is intended to fill this gap. It uses open-source APIs and interfaces such as <a href=\"\"/radar/tools/prometheus\"\">Prometheus</a> and <a href=\"\"/radar/tools/grafana\"\">Grafana</a> and adds additional features on top like TLS and authentication. At the heart of Opstrace runs a <a href=\"\"https://github.com/cortexproject/cortex\"\">Cortex</a> cluster to provide the scalable Prometheus API as well as a <a href=\"\"https://github.com/grafana/loki\"\">Loki</a> cluster for the logs. It's fairly new and <a href=\"\"https://opstrace.com/docs/references/roadmap#opstrace-roadmap\"\">still misses features</a> when compared to solutions like Datadog or SignalFX. Still, it's a promising addition to this space and worth keeping an eye on.</p>\""
    },
    {
        "name": "Pulumi",
        "ring": "assess",
        "quadrant": "platforms",
        "isNew": "FALSE",
        "status": "no change",
        "description": "\"<p>We've seen interest in <strong><a href=\"\"https://pulumi.io/\"\">Pulumi</a></strong> slowly but steadily rising. Pulumi fills a gaping hole in the infrastructure coding world where <a href=\"\"/radar/tools/terraform\"\">Terraform</a> maintains a firm hold. While Terraform is a tried-and-true standby, its declarative nature suffers from inadequate abstraction facilities and limited testability. Terraform is adequate when the infrastructure is entirely static, but dynamic infrastructure definitions call for a real programming language. Pulumi distinguishes itself by allowing configurations to be written in <a href=\"\"/radar/languages-and-frameworks/typescript\"\">TypeScript</a>/JavaScript, <a href=\"\"/radar/languages-and-frameworks/python-3\"\">Python</a> and <a href=\"\"/radar/languages-and-frameworks/go-language\"\">Go</a> — no markup language or templating required. Pulumi is tightly focused on cloud-native architectures — including containers, serverless functions and data services — and provides good support for <a href=\"\"/radar/platforms/kubernetes\"\">Kubernetes</a>. Recently, <a href=\"\"/radar/platforms/aws-cloud-development-kit\"\">AWS CDK</a> has mounted a challenge, but Pulumi remains the only cloud-neutral tool in this area. We're anticipating wider Pulumi adoption in the future and looking forward to a viable tool and knowledge ecosystem emerging to support it.</p>\""
    },
    {
        "name": "Redpanda",
        "ring": "assess",
        "quadrant": "platforms",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p><strong><a href=\"\"https://github.com/vectorizedio/redpanda\"\">Redpanda</a></strong> is a streaming data platform for developers. It is API-compatible with <a href=\"\"/radar/tools/apache-kafka\"\">Apache Kafka</a>, allowing architectures already running in the Kafka ecosystem to benefit from Redpanda’s improved simplicity, performance and hardware efficiency <a href=\"\"https://redpanda.com/blog/redpanda-vs-kafka-faster-safer\"\">over Kafka</a>. Redpanda is free from external dependencies such as ZooKeeper, and written in C++, eliminating the need for JVM management. It leverages the Raft protocol for data replication and is <a href=\"\"https://redpanda.com/blog/redpanda-official-jepsen-report-and-analysis\"\">Jepsen-tested</a> to validate for correctness. Redpanda also offers much-reduced tail latencies and increased throughput due to a <a href=\"\"https://redpanda.com/blog/tpc-buffers\"\">series of optimizations</a>. Some advanced capabilities available for enterprise customers include <a href=\"\"https://docs.redpanda.com/docs/data-management/tiered-storage/\"\">tiered storage</a> and inline <a href=\"\"/radar/languages-and-frameworks/webassembly\"\">WebAssembly (WASM)</a> transformations.</p>\""
    },
    {
        "name": "Azure Machine Learning",
        "ring": "hold",
        "quadrant": "platforms",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>We've observed before that the cloud providers push more and more services onto the market. We've also documented our concerns that sometimes the services are made available when they're not quite ready for prime time. Unfortunately, in our experience, <strong><a href=\"\"https://azure.microsoft.com/en-us/services/machine-learning/\"\">Azure Machine Learning</a></strong> falls into the latter category. One of several <a href=\"\"https://towardsdatascience.com/top-8-no-code-machine-learning-platforms-you-should-use-in-2020-1d1801300dd0\"\">recent entrants</a> in the field of <a href=\"\"/radar/techniques/bounded-low-code-platforms\"\">bounded low-code platforms</a>, Azure ML promises more convenience for data scientists. Ultimately, however, it doesn't live up to its promise; in fact, it still feels easier for our data scientists to work in Python. Despite significant efforts, we struggled to make it scale and lack of adequate documentation proved to be another issue which is why we moved it to the Hold ring.</p>\""
    },
    {
        "name": "Homemade infrastructure-as-code (IaC) products",
        "ring": "hold",
        "quadrant": "platforms",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>Products supported by companies or communities are in constant evolution, at least the ones that get traction in the industry. Sometimes organizations tend to build frameworks or abstractions on top of the existing external products to cover very specific needs, thinking that the adaptation will provide more benefits than the existing ones. We're seeing organizations trying to create <strong>homemade infrastructure-as-code (IaC) products</strong> on top of the existing ones; they underestimate the required effort to keep those solutions evolving according to their needs, and after a short period of time, they realize that the original version is in much better shape than their own; there are even cases where the abstraction on top of the external product reduces the original capabilities. Although we've seen success stories of organizations building homemade solutions, we want to caution about this approach as the effort required to do so isn't negligible, and a long-term product vision is required to have the expected outcomes.</p>\""
    },
    {
        "name": "Sentry",
        "ring": "adopt",
        "quadrant": "tools",
        "isNew": "FALSE",
        "status": "moved in",
        "description": "\"<p><strong><a href=\"\"https://sentry.io/\"\">Sentry</a></strong> has become the default choice for many of our teams when it comes to front-end error reporting. The convenience of features like the grouping of errors or defining patterns for discarding errors with certain parameters helps deal with the flood of errors coming from many end user devices. Integrating Sentry in your CD pipeline allows you to upload source maps for more efficient error debugging, and it helps easily trace back which errors occurred in which version of the software. We also appreciate that while Sentry is primarily a SaaS offering, its source code is publicly available and it can be used for free for smaller use cases and <a href=\"\"https://develop.sentry.dev/self-hosted/\"\">self-hosting</a>.</p>\""
    },
    {
        "name": "axe-core",
        "ring": "trial",
        "quadrant": "tools",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>Making the web inclusive requires serious attention to ensure accessibility is considered <em>and</em> validated at all stages of software delivery. Many of the popular accessibility testing tools are designed for testing after a web application is complete; as a result, issues are detected late and often are harder to fix, accumulating as debt. In our recent internal work on ThoughtWorks websites, we included the open-source accessibility (a11y) testing engine <strong><a href=\"\"https://github.com/dequelabs/axe-core\"\">axe-core</a></strong> as part of our build processes. It provided team members with early feedback on adherence to accessibility rules, even during early increments. Not every issue can be found through automated inspection, though. Extending the functionality of axe-core is the commercially available <a href=\"\"https://www.deque.com/axe/devtools/\"\">axe DevTools</a>, including functionality that guides team members through exploratory testing for a majority of accessibility issues.</p>\""
    },
    {
        "name": "dbt",
        "ring": "trial",
        "quadrant": "tools",
        "isNew": "FALSE",
        "status": "moved in",
        "description": "\"<p>Since we last wrote about <strong><a href=\"\"https://www.getdbt.com/\"\">dbt</a></strong>, we've used it in a few projects and like what we've seen. For example, we like that dbt makes the transformation part of ELT pipelines more accessible to consumers of the data as opposed to just the data engineers building the pipelines. It does this while encouraging good engineering practices such as versioning, automated testing and deployment. SQL continues to be the lingua franca of the data world (including databases, warehouses, query engines, data lakes and analytical platforms) and most of these systems support it to some extent. This allows dbt to be used against these systems for transformations by just building adaptors. The number of native connectors has grown to include <a href=\"\"/radar/platforms/snowflake\"\">Snowflake</a>, <a href=\"\"/radar/platforms/bigquery\"\">BigQuery</a>, Redshift and Postgres, as has the range of <a href=\"\"https://docs.getdbt.com/docs/available-adapters\"\">community plugins</a>. We see tools like dbt helping data platforms become more \"\"self service\"\" capable.</p>\""
    },
    {
        "name": "esbuild",
        "ring": "trial",
        "quadrant": "tools",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>We've always been keen to find tools that can shorten the software development feedback cycle; <strong><a href=\"\"https://github.com/evanw/esbuild\"\">esbuild</a></strong> is such an example. As the front-end codebase grows larger, we usually face a packaging time of minutes. As a JavaScript bundler optimized for speed, esbuild can reduce this time by a factor of 10 to 100. It is written in Golang and uses a more efficient approach in the process of parsing, printing and source map generation which significantly surpasses build tools such as <a href=\"\"/radar/tools/webpack\"\">Webpack</a> and <a href=\"\"/radar/tools/parcel\"\">Parcel</a> in building time. esbuild may not be as comprehensive as those tools in JavaScript syntax transformation; however, this doesn't stop many of our teams from switching to esbuild as their default.</p>\""
    },
    {
        "name": "Flipper",
        "ring": "trial",
        "quadrant": "tools",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p><strong><a href=\"\"https://github.com/facebook/flipper\"\">Flipper</a></strong> is an extensible mobile application debugger. Out of the box it supports profiling, interactive layout inspection, log viewer and a network inspector for iOS, Android and <a href=\"\"/radar/languages-and-frameworks/react-native\"\">React Native</a> applications. Compared to other debugging tools for mobile apps, we find Flipper to be lightweight, feature rich and easy to set up.</p>\""
    },
    {
        "name": "Great Expectations",
        "ring": "trial",
        "quadrant": "tools",
        "isNew": "FALSE",
        "status": "moved in",
        "description": "\"<p>We wrote about <strong><a href=\"\"https://docs.greatexpectations.io/en/latest/\"\">Great Expectations</a></strong> in the previous edition of the Radar. We continue to like it and have moved it to Trial in this edition. Great Expectations is a framework that enables you to craft built-in controls that flag anomalies or quality issues in data pipelines. Just as unit tests run in a build pipeline, Great Expectations makes assertions during execution of a data pipeline. We like its simplicity and ease of use — the rules stored in JSON can be modified by our data domain experts without necessarily needing data engineering skills.</p>\""
    },
    {
        "name": "k6",
        "ring": "trial",
        "quadrant": "tools",
        "isNew": "FALSE",
        "status": "moved in",
        "description": "\"<p>We've had a bit more experience performance testing with <a href=\"\"https://k6.io/\"\"><strong>k6</strong></a> since we first covered it in the Radar, and with good results. Our teams have enjoyed the focus on the developer experience and flexibility of the tool. Although it's easy to get started with k6 all on its own, it really shines with its ease of integration into a developer ecosystem. For example, using the <a href=\"\"https://k6.io/docs/results-visualization/datadog\"\">Datadog adapter</a>, one team was quickly able to visualize performance in a distributed system and identify significant concerns before releasing the system to production. Another team, with the commercial version of k6, was able to use the <a href=\"\"https://marketplace.visualstudio.com/items?itemName=k6.k6-load-test&ssr=false\"\">Azure pipelines marketplace extension</a> to wire performance tests into their CD pipeline and get Azure DevOps reporting with little effort. Since k6 supports thresholds that allow for automated testing assertions out of the box, it's relatively easy to add a stage to your pipeline that detects performance degradation of new changes, adding a powerful feedback mechanism for developers.</p>\""
    },
    {
        "name": "MLflow",
        "ring": "trial",
        "quadrant": "tools",
        "isNew": "FALSE",
        "status": "no change",
        "description": "\"<p><strong><a href=\"\"https://mlflow.org/\"\">MLflow</a></strong> is an open-source tool for <a href=\"\"/radar/tools/experiment-tracking-tools-for-machine-learning\"\">machine-learning experiment tracking</a> and lifecycle management. The workflow to develop and continuously evolve a machine-learning model includes a series of experiments (a collection of runs), tracking the performance of these experiments (a collection of metrics) and tracking and tweaking models (projects). MLflow facilitates this workflow nicely by supporting existing open standards and integrates well with many other tools in the ecosystem. <a href=\"\"https://databricks.com/product/managed-mlflow\"\">MLflow as a managed service by Databricks</a> on the cloud, available in <a href=\"\"/radar/platforms/aws\"\">AWS</a> and <a href=\"\"/radar/platforms/azure\"\">Azure</a>, is rapidly maturing, and we've used it successfully in our projects. We find MLflow a great tool for model management and tracking, supporting both UI-based and API-based interaction models. Our only growing concern is that MLflow is attempting to deliver too many conflating concerns as a single platform, such as model serving and scoring.</p>\""
    },
    {
        "name": "OR-Tools",
        "ring": "trial",
        "quadrant": "tools",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p><strong><a href=\"\"https://developers.google.com/optimization\"\">OR-Tools</a></strong> is an open-source software suite for solving combinatorial optimization problems. These optimization problems have a very large set of possible solutions, and tools like OR-Tools are quite helpful in seeking the best solution. You can model the problem in any one of the supported languages — Python, Java, C# or C++ — and choose the solvers from several supported open-source or commercial solvers. We've successfully used OR-Tools in multiple optimization projects with integer and mixed-integer programming.</p>\""
    },
    {
        "name": "Playwright",
        "ring": "trial",
        "quadrant": "tools",
        "isNew": "FALSE",
        "status": "moved in",
        "description": "\"<p><strong><a href=\"\"https://playwright.dev/\"\">Playwright</a></strong> allows you to write Web UI tests for Chromium and Firefox as well as WebKit, all through the same API. The tool has gained some attention for its support of all the major browser engines which it achieves by including patched versions of Firefox and Webkit. We continue to hear positive experience reports with Playwright, in particular its stability. Teams have also found it easy to migrate from <a href=\"\"/radar/languages-and-frameworks/puppeteer\"\">Puppeteer</a>, which has a very similar API.</p>\""
    },
    {
        "name": "Prowler",
        "ring": "trial",
        "quadrant": "tools",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>We welcome the increased availability and maturity of <a href=\"\"/radar/techniques/infrastructure-configuration-scanner\"\">infrastructure configuration scanning</a> tools: <strong><a href=\"\"https://github.com/toniblyx/prowler\"\">Prowler</a></strong> helps teams scan their AWS infrastructure setups and improve security based on the results. Although Prowler has been around for a while, it has evolved a lot over the past few years, and we've found it very valuable to enable teams to take responsibility for proper security with a short feedback loop. Prowler categorizes <a href=\"\"https://d0.awsstatic.com/whitepapers/compliance/AWS_CIS_Foundations_Benchmark.pdf\"\">AWS CIS benchmarking</a> checks into different groups (Identity and Access Management, Logging, Monitoring, Networking, CIS Level 1, CIS Level 2, EKS-CIS), and it includes many checks that help you gain insights into your PCI DSS and GDPR compliance.</p>\""
    },
    {
        "name": "Pyright",
        "ring": "trial",
        "quadrant": "tools",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>While <a href=\"\"https://en.wikipedia.org/wiki/Duck_typing\"\">duck typing</a> is certainly seen as a feature by many Python programmers, sometimes — especially for larger codebases — type checking can be useful, too. For that reason a number of type annotations are proposed as Python Enhancement Proposals (PEPs), and <strong><a href=\"\"https://github.com/Microsoft/pyright\"\">Pyright</a></strong> is a type checker that works with these annotations. In addition, it provides some type inference and guards that understand conditional code flow constructs. Designed with large codebases in mind, Pyright is fast, and its watch mode checks happen incrementally as files are changed to further shorten the feedback cycle. Pyright can be used directly on the command line, but integrations for VS Code, Emacs, vim, Sublime, and possibly other editors are available, too. In our experience, Pyright is preferable to alternatives like mypy.</p>\""
    },
    {
        "name": "Redash",
        "ring": "trial",
        "quadrant": "tools",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>Adopting a \"\"you build it, you run it\"\" DevOps philosophy means teams have increased attention on both technical and business metrics that can be extracted from the systems they deploy. Often we find that analytics tooling is difficult to access for most developers, so the work to capture and present metrics is left to other teams — long after features are shipped to end users. Our teams have found <strong><a href=\"\"https://redash.io/\"\">Redash</a></strong> to be very useful for querying product metrics and creating dashboards in a way that can be self-served by general developers, shortening feedback cycles and focusing the whole team on the business outcomes.</p>\""
    },
    {
        "name": "Terratest",
        "ring": "trial",
        "quadrant": "tools",
        "isNew": "FALSE",
        "status": "moved in",
        "description": "\"<p><strong><a href=\"\"https://github.com/gruntwork-io/terratest\"\">Terratest</a></strong> caught our attention in the past as an interesting option for infrastructure testing. Since then, our teams have been using it, and they're very excited about it because of its stability and the experience it provides. Terratest is a Golang library that makes it easier to write automated tests for infrastructure code. Using infrastructure-as-code tools such as <a href=\"\"/radar/tools/terraform\"\">Terraform</a>, you can create real infrastructure components (such as servers, firewalls, or load balancers) to deploy applications on them and then validate the expected behavior using Terratest. At the end of the test, Terratest can undeploy the apps and clean up resources. This makes it largely useful for end-to-end tests of your infrastructure in a real environment.</p>\""
    },
    {
        "name": "Tuple",
        "ring": "trial",
        "quadrant": "tools",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p><strong><a href=\"\"https://tuple.app/\"\">Tuple</a></strong> is a relatively new tool optimized for remote paired programming, designed to fill the gap Slack left in the marketplace after abandoning Screenhero. Although it still exhibits some growing pains — platform availability is limited to Mac OS for now (with Linux support coming soon), and it has some UI quirks to work through — we've had good experience using it within those constraints. Unlike general-purpose video- and screen-sharing tools like Zoom, Tuple supports dual control with two mouse cursors, and unlike options such as <a href=\"\"/radar/tools/visual-studio-live-share\"\">Visual Studio Live Share</a>, it isn't tied to an IDE. Tuple supports voice and video calls, clipboard sharing, and lower latency than general-purpose tools; and its ability to let you draw and erase in your pair's screen with ease makes Tuple a very intuitive and developer-friendly tool.</p>\""
    },
    {
        "name": "Why Did You Render",
        "ring": "trial",
        "quadrant": "tools",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>When working with <a href=\"\"/radar/languages-and-frameworks/react-js\"\">React</a>, we often encounter situations where our page is very slow because some components are re-rendering when they shouldn't be. <strong><a href=\"\"https://github.com/welldone-software/why-did-you-render\"\">Why Did You Render</a></strong> is a library that helps detect why a component is re-rendering. It does this by monkey patching React. We've used it in a few of our projects to debug performance issues with great effect.</p>\""
    },
    {
        "name": "Buildah and Podman",
        "ring": "assess",
        "quadrant": "tools",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>Even though <a href=\"\"/radar/platforms/docker\"\">Docker</a> has become the sensible default for containerization, we're seeing new players in this space that are catching our attention. That is the case for <strong><a href=\"\"https://github.com/containers/buildah\"\">Buildah</a> and <a href=\"\"https://github.com/containers/podman\"\">Podman</a></strong>, which are complementary projects to build images (Buildah) and run containers (Podman) using a <a href=\"\"/radar/platforms/rootless-containers\"\">rootless</a> approach in multiple Linux distributions. Podman introduces a daemonless engine for managing and running containers which is an interesting approach in comparison to what Docker does. The fact that Podman can use either <a href=\"\"https://opencontainers.org/\"\">Open Container Initiative (OCI)</a> images built by Buildah or Docker images makes this tool even more attractive and easy to use.</p>\""
    },
    {
        "name": "GitHub Actions",
        "ring": "assess",
        "quadrant": "tools",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>CI servers and build tools are some of the oldest and most widely used in our kit. They run the gamut from simple cloud-hosted services to complex, code-defined pipeline servers that support fleets of build machines. Given our experience and the wide range of options already available, we were initially skeptical when <strong><a href=\"\"https://docs.github.com/en/actions\"\">GitHub Actions</a></strong> were introduced as another mechanism to manage the build and integration workflow. But the opportunity for developers to start small and easily customize behavior means that GitHub Actions are moving toward the default category for smaller projects. It's hard to argue with the convenience of having the build tool integrated directly into the source code repository. An enthusiastic community has emerged around this feature and that means a wide range of user-contributed tools and workflows are available to get started. Tools vendors are also getting on board via the <a href=\"\"https://github.com/marketplace?type=actions\"\">GitHub Marketplace</a>. However, we still recommend you proceed with caution. Although code and <a href=\"\"/radar/tools/git\"\">Git</a> history can be exported into alternative hosts, a development workflow based on GitHub Actions can't. Also, use your best judgment to determine when a project is large or complex enough to warrant an independently supported pipeline tool. But for getting up and running quickly on smaller projects, it's worth considering GitHub Actions and the ecosystem that is growing around them.</p>\""
    },
    {
        "name": "Graal Native Image",
        "ring": "assess",
        "quadrant": "tools",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p><strong><a href=\"\"https://www.graalvm.org/reference-manual/native-image/\"\">Graal Native Image</a></strong> is a technology that compiles Java code into an operating system's native binary — in the form of a statically linked executable or a shared library. A native image is optimized to reduce the memory footprint and startup time of an application. Our teams have successfully used Graal native images, executed as small Docker containers, in the <a href=\"\"/radar/techniques/serverless-architecture\"\">serverless architecture</a> where reducing start time matters. Although designed for use with programming languages such as <a href=\"\"/radar/languages-and-frameworks/go-language\"\">Go</a> or <a href=\"\"/radar/languages-and-frameworks/rust\"\">Rust</a> that natively compile and require smaller binary sizes and shorter start times, Graal Native Image can be equally useful to teams that have other requirements and want to use JVM-based languages.</p><br><br><p>Graal Native Image Builder, <em>native-image</em>, supports JVM-based languages — such as Java, Scala, Clojure and Kotlin — and builds executables on multiple operating systems including Mac OS, Windows and multiple distributions of Linux. Since it requires a closed-world assumption, where all code is known at compile time, additional configuration is needed for features such as <em>reflection</em> or <em>dynamic class loading</em> where types can't be deduced at build time from the code alone.</p>\""
    },
    {
        "name": "HashiCorp Boundary",
        "ring": "assess",
        "quadrant": "tools",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p><strong><a href=\"\"https://www.boundaryproject.io/\"\">HashiCorp Boundary</a></strong> combines the secure networking and identity management capabilities needed for brokering access to your hosts and services in one place and across a mix of cloud and on-premise resources if needed. Key management can be done by integrating the key management service of your choice, be it from a cloud vendor or something like <a href=\"\"/radar/tools/hashicorp-vault\"\">HashiCorp Vault</a>. HashiCorp Boundary supports a growing number of identity providers and can be integrated with parts of your service landscape to help define permissions, not just on host but also on a service level. For example, it enables you to control fine-grained access to a Kubernetes cluster, and dynamically pulling in service catalogs from various sources is on the roadmap. All of this stays out of the way of the engineering end users who get the shell experience they're used to, securely connected through Boundary's network management layer.</p>\""
    },
    {
        "name": "imgcook",
        "ring": "assess",
        "quadrant": "tools",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>Remember the research project <a href=\"\"https://github.com/tonybeltramelli/pix2code\"\">pix2code</a> that showed how to automatically generate code from GUI screenshots? Now there is a productized version of this technique — <strong><a href=\"\"https://www.imgcook.com/\"\">imgcook</a></strong> is a SaaS product from Alibaba that can intelligently transform various design files (Sketch/PSD/static images) into front-end code. Alibaba needs to customize a large number of campaign pages during the Double Eleven shopping festival. These are usually one-time pages that need to be developed quickly. Through the deep-learning method, the UX's design is initially processed into front-end code and then adjusted by the developer. Our team is evaluating this tech: although the image processing takes place on the server side while the main interface is on the web, imgcook provides <a href=\"\"https://github.com/imgcook\"\">tools</a> that could integrate with the software design and development lifecycle. imgcook can generate static code as well as some data-binding component code if you define a DSL. The technology is not perfect yet; designers need to refer to certain specifications to improve the accuracy of code generation (which still needs to be adjusted by developers afterward). We've always been cautious about magic code generation, because the generated code is usually difficult to maintain in the long run, and imgcook is no exception. But if you limit the usage to a specific context, such as one-time campaign pages, it's worth a try.</p>\""
    },
    {
        "name": "Longhorn",
        "ring": "assess",
        "quadrant": "tools",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p><strong><a href=\"\"https://longhorn.io/\"\">Longhorn</a></strong> is a distributed block storage system for <a href=\"\"/radar/platforms/kubernetes\"\">Kubernetes</a>. There are many <a href=\"\"https://kubernetes-csi.github.io/docs/drivers.html\"\">persistent storage options</a> for Kubernetes; unlike most, however, Longhorn is built from the ground up to provide incremental snapshots and backups, thereby easing the pain of running a replicated storage for non–cloud-hosted Kubernetes. With the recent <a href=\"\"https://longhorn.io/blog/longhorn-v1.1.0/\"\">experimental support for ReadWriteMany (RWX)</a> you can even mount the same volume for read and write access across many nodes. Choosing the right storage system for Kubernetes is a nontrivial task, and we recommend you assess Longhorn based on your needs.</p>\""
    },
    {
        "name": "Operator Framework",
        "ring": "assess",
        "quadrant": "tools",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p><strong><a href=\"\"https://operatorframework.io/\"\">Operator Framework</a></strong> is a set of open-source tools that simplifies building and managing the lifecycle of <a href=\"\"https://kubernetes.io/docs/concepts/extend-kubernetes/operator/\"\">Kubernetes operators</a>. The Kubernetes operator pattern, originally <a href=\"\"https://web.archive.org/web/20170129131616/https://coreos.com/blog/introducing-operators.html\"\">introduced by CoreOS</a>, is an approach to encapsulate the knowledge of operating an application using Kubernetes native capabilities; it includes <em>resources</em> to be managed and <em>controller code</em> that ensures the resources are matching their target state. This approach has been used to extend Kubernetes to manage <a href=\"\"https://operatorhub.io/\"\">many applications</a>, particularly the stateful ones, natively. Operator Framework has three components: <a href=\"\"https://sdk.operatorframework.io/\"\">Operator SDK</a>, which simplifies building, testing and packaging Kubernetes operators; <a href=\"\"https://github.com/operator-framework/operator-lifecycle-manager/\"\">Operator lifecycle manager</a> to install, manage and upgrade the operators; and a <a href=\"\"https://operatorhub.io/\"\">catalog</a> to publish and share third-party operators. Our teams have found Operator SDK particularly powerful in rapidly developing Kubernetes-native applications.</p>\""
    },
    {
        "name": "Recommender",
        "ring": "assess",
        "quadrant": "tools",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>The number of services offered by the big cloud providers keeps growing, but so does the convenience and maturity of tools that help you use them securely and efficiently. <a href=\"\"https://cloud.google.com/recommender\"\"><strong>Recommender</strong></a> is a service on Google Cloud that analyzes your resources and gives you recommendations on how to optimize them based on your actual usage. The service consists of a range of \"\"recommenders\"\" in areas such as security, compute usage or cost savings. For example, the <a href=\"\"https://cloud.google.com/iam/docs/role-recommendations\"\">IAM Recommender</a> helps you better implement the principle of least privilege by pointing out permissions that are never actually used and therefore are potentially too broad.</p>\""
    },
    {
        "name": "Remote - WSL",
        "ring": "assess",
        "quadrant": "tools",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>Over the past few years <a href=\"\"https://docs.microsoft.com/en-us/windows/wsl/\"\">Windows Subsystem for Linux (WSL)</a> has come up a few times in our discussions. Although we liked what we saw, including the improvements in WSL 2, it never made it into the Radar. In this edition we want to highlight an extension for Visual Studio Code that greatly improves the experience working with WSL. Although Windows-based editors could always access files on a WSL file system, they were unaware of the isolated Linux environment. With the <strong><a href=\"\"https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-wsl\"\">Remote - WSL</a></strong> extension, Visual Studio Code becomes aware of WSL, allowing developers to launch a Linux shell. This also enables debugging of binaries running inside WSL from Windows. Jetbrains' IntelliJ too has seen steady improvement in its <a href=\"\"https://youtrack.jetbrains.com/issue/IDEA-171510#focus=Comments-27-4155034.0-0\"\">support for WSL</a>.</p>\""
    },
    {
        "name": "Spectral",
        "ring": "assess",
        "quadrant": "tools",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>One of the patterns we've seen repeat itself in this publication is that static error- and style-checking tools emerge quickly after a new language gains popularity. These tools are generically known as linters — after the classic and beloved Unix utility <em>lint</em>, which statically analyzes C code. We like these tools because they catch errors early, before code even gets compiled. The latest instance of this pattern is <strong><a href=\"\"https://stoplight.io/open-source/spectral/\"\">Spectral</a></strong>, a linter for YAML and JSON. Although Spectral is a generic tool for these formats, its main target is OpenAPI (the evolution of <a href=\"\"/radar/tools/swagger\"\">Swagger</a>) and <a href=\"\"/radar/tools/asyncapi\"\">AsyncAPI</a>. Spectral ships with a comprehensive set of out-of-the-box rules for these specs that can save developers headaches when designing and implementing APIs or event-driven collaboration. These rules check for proper API parameter specifications or the existence of a license statement in the spec, among other things. While this tool is a welcome addition to the API development workflow, it does raise the question of whether a non-executable specification should be so complex as to require an error-checking technique designed for programming languages. Perhaps developers should be writing code instead of specs?</p>\""
    },
    {
        "name": "Yelp detect-secrets",
        "ring": "assess",
        "quadrant": "tools",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p><strong><a href=\"\"https://github.com/Yelp/detect-secrets\"\">Yelp detect-secrets</a></strong> is a Python module for detecting secrets within a codebase; it scans files within a directory looking for secrets. It can be used as a <a href=\"\"/radar/tools/git\"\">Git</a> pre-commit hook or to perform a scan in multiple places within the CI/CD pipeline. It comes with a default configuration that makes it very easy to use but can be modified to suit your needs. You can also install custom plugins to add to its default heuristic searches. Compared to similar offerings, we found that this tool detects more types of secrets with its out-of-the-box configuration.</p>\""
    },
    {
        "name": "Zally",
        "ring": "assess",
        "quadrant": "tools",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>As the API specification ecosystem matures, we're seeing more tools built to automate style checks. <strong><a href=\"\"https://github.com/zalando/zally\"\">Zally</a></strong> is a minimalist OpenAPI linter that helps to ensure an API conforms to the team's API style guide. Out of the box, it will validate against a rule set developed for <a href=\"\"https://opensource.zalando.com/restful-api-guidelines/\"\">Zalando's API style guide</a>, but it also supports a Kotlin extension mechanism to develop custom rules. Zally includes a web UI that provides an intuitive interface for understanding style violations and includes a CLI that makes it easy to plug into your CD pipeline.</p>\""
    },
    {
        "name": "AWS CodePipeline",
        "ring": "hold",
        "quadrant": "tools",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>Based on the experiences of multiple ThoughtWorks teams we suggest approaching <strong><a href=\"\"https://aws.amazon.com/codepipeline/\"\">AWS CodePipeline</a></strong> with caution. Specifically, we've found that once teams move beyond simple pipelines, this tool can become hard to work with. While it may seem like a \"\"quick win\"\" when first starting out with <a href=\"\"/radar/platforms/aws\"\">AWS</a>, we suggest taking a step back and checking whether AWS CodePipeline will meet your longer-term needs, for example, pipeline fan-out and fan-in or more complex deployment and testing scenarios featuring nontrivial dependencies and triggers.</p>\""
    },
    {
        "name": "Combine",
        "ring": "adopt",
        "quadrant": "languages-and-frameworks",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>A long time ago we placed <a href=\"\"/radar/languages-and-frameworks/reactivex\"\">ReactiveX</a> — a family of open-source frameworks for reactive programming — into the Adopt ring of the Radar. In 2017, we mentioned the addition of <a href=\"\"https://github.com/ReactiveX/RxSwift\"\">RxSwift</a>, which brought reactive programming to iOS development using Swift. Since then, Apple has introduced its own take on reactive programming in the form of the <a href=\"\"https://developer.apple.com/documentation/combine\"\"><strong>Combine</strong></a> framework. Combine has become our default choice for apps that support iOS 13 as an acceptable deployment target. It's easier to learn than RxSwift and integrates really well with <a href=\"\"/radar/languages-and-frameworks/swiftui\"\">SwiftUI</a>. If you're planning to convert an existing application from RxSwift to Combine or work with both in the same project, you might want to look at <a href=\"\"https://github.com/CombineCommunity/RxCombine\"\">RxCombine</a>.</p>\""
    },
    {
        "name": "LeakCanary",
        "ring": "adopt",
        "quadrant": "languages-and-frameworks",
        "isNew": "FALSE",
        "status": "moved in",
        "description": "\"<p>Our mobile teams now view <strong><a href=\"\"http://github.com/square/leakcanary\"\">LeakCanary</a></strong> as a good default choice for Android development. It detects annoying memory leaks in Android apps, is extremely simple to hook up and provides notifications with a clear trace-back to the cause of the leak. LeakCanary can save you tedious hours troubleshooting out-of-memory errors on multiple devices, and we recommend you add it to your toolkit.</p>\""
    },
    {
        "name": "Angular Testing Library",
        "ring": "trial",
        "quadrant": "languages-and-frameworks",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>As we continue developing web applications in JavaScript, we continue enjoying the <a href=\"\"/radar/languages-and-frameworks/testing-library\"\">Testing Library</a> approach of testing applications; and carry on exploring and gaining experience with its packages — beyond that of <a href=\"\"/radar/languages-and-frameworks/react-testing-library\"\">React Testing Library</a>. <strong><a href=\"\"https://testing-library.com/docs/angular-testing-library/intro/\"\">Angular Testing Library</a></strong> brings all the benefits of its family when testing UI components in a user-centric way, pushing for more maintainable tests focused primarily on behavior rather than testing UI implementation details. Although it falls short in documentation, Angular Testing Library does provide <a href=\"\"https://github.com/testing-library/angular-testing-library/tree/main/apps/example-app/src/app/examples\"\">good sample tests</a> that helped us in getting started faster for various cases. We've had great success with this testing library in our <a href=\"\"/radar/languages-and-frameworks/angular\"\">Angular</a> projects and advise you to trial this solid testing approach.</p>\""
    },
    {
        "name": "AWS Data Wrangler",
        "ring": "trial",
        "quadrant": "languages-and-frameworks",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p><strong><a href=\"\"https://github.com/awslabs/aws-data-wrangler\"\">AWS Data Wrangler</a></strong> is an open-source library that extends the capabilities of <a href=\"\"https://github.com/pandas-dev/pandas\"\">Pandas</a> to AWS by connecting data frames to AWS data-related services. In addition to Pandas, this library leverages <a href=\"\"https://github.com/apache/arrow\"\">Apache Arrow</a> and <a href=\"\"https://github.com/boto/boto3\"\">Boto3</a> to expose <a href=\"\"https://aws-data-wrangler.readthedocs.io/en/2.5.0/api.html\"\">several APIs</a> to load, transform and save data from data lakes and data warehouses. An important limitation is that you can't do large distributed data pipelines with this library. However, you can leverage the native data services — like Athena, Redshift and Timestream — to do the heavy lifting and pull data in order to express complex transformations that are well suited for data frames. We've used AWS Data Wrangler in production and like that it lets you focus on writing transformations without spending too much time on the connectivity to AWS data services.</p>\""
    },
    {
        "name": "Blazor",
        "ring": "trial",
        "quadrant": "languages-and-frameworks",
        "isNew": "FALSE",
        "status": "moved in",
        "description": "\"<p>Although JavaScript and its ecosystem is dominant in the web UI development space, new opportunities are opening up with the emergence of <a href=\"\"/radar/languages-and-frameworks/webassembly\"\">WebAssembly</a>. <strong><a href=\"\"https://dotnet.microsoft.com/apps/aspnet/web-apps/blazor\"\">Blazor</a></strong> continues to demand our attention; it's producing good results with our teams building interactive rich user interfaces using C# on top of WebAssembly. The fact that our teams can use C# on the frontend too allows them to share code and reuse existing libraries. That, along with the existing tooling for debugging and testing, such as <a href=\"\"/radar/languages-and-frameworks/bunit\"\">bUnit</a>, make this open-source framework worth trying.</p>\""
    },
    {
        "name": "FastAPI",
        "ring": "trial",
        "quadrant": "languages-and-frameworks",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>We're seeing more teams adopting Python as the preferred language to build solutions, not just for data science but for back-end services too. In these scenarios, we're having good experiences with <strong><a href=\"\"https://fastapi.tiangolo.com/\"\">FastAPI</a></strong> — a modern, fast (high-performance), web framework for building APIs with Python 3.6 or later. Additionally, this framework and its ecosystem include features such as API documentation using OpenAPI that allow our teams to focus on the business functionalities and quickly create REST APIs, which makes FastAPI a good alternative to existing solutions in this space.</p>\""
    },
    {
        "name": "io-ts",
        "ring": "trial",
        "quadrant": "languages-and-frameworks",
        "isNew": "FALSE",
        "status": "moved in",
        "description": "\"<p>We've really enjoyed using <a href=\"\"/radar/languages-and-frameworks/typescript\"\">TypeScript</a> for a while now and love the safety that the strong typing provides. However, getting data into the bounds of the type system — from, for example, a call to a back-end service — can lead to run-time errors. One library that helps solve this problem is <strong><a href=\"\"https://gcanti.github.io/io-ts/\"\">io-ts</a></strong>. It bridges the gap between compile-time type-checking and run-time consumption of external data by providing encode and decode functions. It can also be used as a custom type guard. As we gain more experience with io-ts in our work, our initially positive impressions are confirmed, and we still like the elegance of its approach.</p>\""
    },
    {
        "name": "Kotlin Flow",
        "ring": "trial",
        "quadrant": "languages-and-frameworks",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>The introduction of <a href=\"\"https://kotlinlang.org/docs/coroutines-overview.html\"\">coroutines to Kotlin</a> opened the door for several innovations — <strong><a href=\"\"https://kotlinlang.org/docs/flow.html\"\">Kotlin Flow</a></strong> is one of them, directly integrated into the coroutines library. It's an implementation of Reactive Streams on top of coroutines. Unlike <a href=\"\"https://github.com/ReactiveX/RxJava\"\">RxJava</a>, flows are a native Kotlin API similar to the familiar sequence API with methods that include <code>map</code> and <code>filter</code>. Like sequences, flows are <em>cold</em>, meaning that the values of the sequence are only constructed when needed. All of this makes writing multithreaded code much simpler and easier to understand than other approaches. The <code>toList</code> method, predictably, converts a flow into a list which is a common pattern in tests.</p>\""
    },
    {
        "name": "LitElement",
        "ring": "trial",
        "quadrant": "languages-and-frameworks",
        "isNew": "FALSE",
        "status": "moved in",
        "description": "\"<p>Steady progress has been made since we first wrote about <a href=\"\"/radar/platforms/web-components-standard\"\">Web Components</a> in 2014. <strong><a href=\"\"https://lit-element.polymer-project.org/\"\">LitElement</a></strong>, part of the <a href=\"\"https://www.polymer-project.org/\"\">Polymer Project</a>, is a simple library that you can use to create lightweight web components. It's really just a base class that removes the need for a lot of the common boilerplate, making writing web components a lot easier. We've had success using it on projects, and as we see the technology maturing and the library being well liked, LitElement is becoming more commonly used in our Web Components-based projects.</p>\""
    },
    {
        "name": "Next.js",
        "ring": "trial",
        "quadrant": "languages-and-frameworks",
        "isNew": "FALSE",
        "status": "moved in",
        "description": "\"<p>We've had a bit more experience using <strong><a href=\"\"https://nextjs.org/\"\">Next.js</a></strong> for <a href=\"\"/radar/languages-and-frameworks/react-js\"\">React</a> codebases since the last time we wrote about it. Next.js is an opinionated, zero-configuration framework that includes simplified routing, automatic compilation and bundling with <a href=\"\"/radar/tools/webpack\"\">Webpack</a> and <a href=\"\"/radar/tools/babel\"\">Babel</a>, fast hot reloading for a convenient developer workflow among other features. It provides server-side rendering by default, improves search engine optimization and the initial load time and supports incremental static generation. We've had positive experience reports from teams using Next.js and, given its large community, continue to be excited about the evolution of the framework.</p>\""
    },
    {
        "name": "On-demand modules",
        "ring": "trial",
        "quadrant": "languages-and-frameworks",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p><strong><a href=\"\"https://developer.android.com/codelabs/on-demand-dynamic-delivery#0\"\">On-demand modules</a></strong> for Android is a framework that allows tailored APKs containing only required functionality to be downloaded and installed for a suitably structured app. This could be worth trialing for larger apps where download speed might be an issue, or if a user is likely only to use some functionality on initial installation. It can also simplify the handling of multiple devices without requiring different APKs. A similar framework is available for <a href=\"\"https://developer.apple.com/app-clips/\"\">iOS</a>.</p>\""
    },
    {
        "name": "Streamlit",
        "ring": "trial",
        "quadrant": "languages-and-frameworks",
        "isNew": "FALSE",
        "status": "moved in",
        "description": "\"<p><strong><a href=\"\"https://www.streamlit.io/\"\">Streamlit</a></strong> is an open-source application framework in Python used by data scientists for building interactive data applications. Tuning machine-learning models takes time; instead of going back and forth on the main application (the one that uses these models), we've found value in quickly building standalone prototypes in Streamlit and gathering feedback during experimentation cycles. Streamlit stands out from competitors such as <a href=\"\"/radar/tools/dash\"\">Dash</a> because of its focus on rapid prototyping and support for a wide range of visualization libraries, including <a href=\"\"https://plotly.com/\"\">Plotly</a> and <a href=\"\"/radar/tools/bokeh\"\">Bokeh</a>. We're using it in a few projects and like how we can put together interactive visualizations with very little effort.</p>\""
    },
    {
        "name": "SWR",
        "ring": "trial",
        "quadrant": "languages-and-frameworks",
        "isNew": "FALSE",
        "status": "moved in",
        "description": "\"<p>When used in appropriate circumstances, our teams have found that the <a href=\"\"/radar/languages-and-frameworks/react-hooks\"\">React Hooks</a> library <strong><a href=\"\"https://github.com/vercel/swr\"\">SWR</a></strong> can result in cleaner code and much improved performance. SWR implements the <a href=\"\"https://tools.ietf.org/html/rfc5861\"\">stale-while-revalidate</a> HTTP caching strategy, first returning data from cache (stale), then sending the fetch request (revalidate) and finally refreshing the values with the up-to-date response. We caution teams to only use the SWR caching strategy when an application is supposed to return stale data. Note that <a href=\"\"https://tools.ietf.org/html/rfc2616\"\">HTTP</a> requires that caches respond to a request with the most up-to-date response; only in <em>carefully considered circumstances</em> is a stale response allowed to be returned.</p>\""
    },
    {
        "name": "TrustKit",
        "ring": "trial",
        "quadrant": "languages-and-frameworks",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p><a href=\"\"https://owasp.org/www-community/controls/Certificate_and_Public_Key_Pinning\"\">SSL public key pinning</a> is tricky. If you select the wrong policy or don't have a backup pin, your application will stop working unexpectedly. This is where <strong><a href=\"\"https://github.com/datatheorem/TrustKit\"\">TrustKit</a></strong> is useful — it's an open-source framework that makes SSL public key pinning easier for iOS applications. There is an equivalent <a href=\"\"https://github.com/datatheorem/TrustKit-Android\"\">framework for Android</a> as well. Picking the correct pinning strategy is a nuanced topic, and you can find more details about it in the TrustKit <a href=\"\"https://github.com/datatheorem/TrustKit/blob/master/docs/getting-started.md\"\">Getting Started guide</a>. We've used TrustKit in several projects in production, and it has worked out well.</p>\""
    },
    {
        "name": ".NET 5",
        "ring": "assess",
        "quadrant": "languages-and-frameworks",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>We don't call out every new .NET version in the Radar, but <strong>.NET 5</strong> represents a significant step forward in bringing .NET Core and .NET Framework into a single platform. Organizations should start to develop a strategy to migrate their development environments — a fragmented mix of frameworks depending on the deployment target — to a single version of .NET 5 or 6 when it becomes available. The advantage of this approach will be a common development platform regardless of the intended environment: Windows, Linux, cross-platform mobile devices (via <a href=\"\"/radar/tools/xamarin\"\">Xamarin</a>) or the browser (using <a href=\"\"/radar/languages-and-frameworks/blazor\"\">Blazor</a>). While polyglot development will remain the preferred approach for companies with the engineering culture to support it, others will find it more efficient to standardize on a single platform for .NET development. For now, we want to keep this in the Assess ring to see how well the final unified framework performs in .NET 6.</p>\""
    },
    {
        "name": "bUnit",
        "ring": "assess",
        "quadrant": "languages-and-frameworks",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p><strong><a href=\"\"https://bunit.egilhansen.com/\"\">bUnit</a></strong> is a testing library for <a href=\"\"/radar/languages-and-frameworks/blazor\"\">Blazor</a> that makes it easy to create tests for Blazor components in existing unit testing frameworks such as NUnit, xUnit or MSUnit. It provides a facade around the component allowing it to be run and tested within the familiar unit test paradigm, thus allowing very fast feedback and testing of the component in isolation. If you're developing for Blazor, we recommend that you add bUnit to your list of tools to try out.</p>\""
    },
    {
        "name": "Dagster",
        "ring": "assess",
        "quadrant": "languages-and-frameworks",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p><strong><a href=\"\"https://github.com/dagster-io/dagster\"\">Dagster</a></strong> is an open-source data orchestration framework for machine learning, analytics and plain ETL data pipelines. Unlike other task-driven frameworks, Dagster is aware of data flowing through the pipeline and can provide type-safety. With this unified view of pipelines and assets produced, Dagster can schedule and orchestrate Pandas, <a href=\"\"/radar/platforms/apache-spark\"\">Spark</a>, SQL or anything else that Python can invoke. The framework is relatively new, and we recommend that you assess its capabilities for your data pipelines.</p>\""
    },
    {
        "name": "Flutter for Web",
        "ring": "assess",
        "quadrant": "languages-and-frameworks",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>So far, <a href=\"\"/radar/languages-and-frameworks/flutter\"\">Flutter</a> has primarily supported native iOS and Android applications. However, the Flutter team's vision is to support building applications on every platform. <strong>Flutter for Web</strong> is one step in that direction — it allows us to build apps for iOS, Android and the browser from the same codebase. It has been available for over a year now on the \"\"Beta\"\" channel, but with the recent Flutter 2.0 release, Flutter for Web has hit the stable milestone. In the initial release of web support, the Flutter team is focusing on <a href=\"\"https://web.dev/what-are-pwas/\"\">progressive web apps</a>, <a href=\"\"https://en.wikipedia.org/wiki/Single-page_application\"\">single-page apps</a> and expanding existing mobile apps to the web. The application and framework code (all in <a href=\"\"/radar/languages-and-frameworks/google-dart\"\">Dart</a>) are compiled to JavaScript instead of ARM machine code, which is used for mobile applications. Flutter’s web engine offers a choice of two renderers: an HTML renderer, which uses HTML, CSS, Canvas and SVG, and a CanvasKit renderer that uses <a href=\"\"/radar/languages-and-frameworks/webassembly\"\">WebAssembly</a> and WebGL to render Skia paint commands to the browser canvas. A few of our teams have started using Flutter for Web and like the initial results.</p>\""
    },
    {
        "name": "Jotai and Zustand",
        "ring": "assess",
        "quadrant": "languages-and-frameworks",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>In the previous Radar, we commented on the beginning of a phase of experimentation with state management in <a href=\"\"/radar/languages-and-frameworks/react-js\"\">React</a> applications. We moved <a href=\"\"/radar/languages-and-frameworks/redux\"\">Redux</a> back into the Trial ring, documenting that it is no longer our default choice, and we mentioned Facebook's <a href=\"\"/radar/languages-and-frameworks/recoil\"\">Recoil</a>. In this volume we want to highlight <strong><a href=\"\"https://github.com/pmndrs/jotai\"\">Jotai</a> and <a href=\"\"https://github.com/pmndrs/zustand\"\">Zustand</a></strong>: Both are state management libraries for React; both aim to be small and simple to use; and, perhaps not by complete coincidence, both names are translations of the word <em>state</em> into Japanese and German, respectively. Beyond these similarities, however, they differ in their design. Jotai's design is closer to that of Recoil in that state consists of atoms stored within the React component tree, whereas Zustand stores the state outside of React in a single state object, much like the approach taken by Redux. The authors of Jotai provide a helpful <a href=\"\"https://github.com/pmndrs/jotai/blob/master/docs/introduction/comparison.md\"\">checklist</a> to decide when to use which.</p>\""
    },
    {
        "name": "Kotlin Multiplatform Mobile",
        "ring": "assess",
        "quadrant": "languages-and-frameworks",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>Following the trend of cross-platform mobile development, <strong><a href=\"\"https://kotlinlang.org/docs/mobile/home.html\"\">Kotlin Multiplatform Mobile</a></strong> (KMM) is a new entry in this space. KMM is an SDK provided by JetBrains that leverages the <a href=\"\"https://kotlinlang.org/docs/multiplatform.html\"\">multiplatform capabilities</a> in <a href=\"\"/radar/languages-and-frameworks/kotlin\"\">Kotlin</a> and includes tools and features designed to make the end-to-end experience of building mobile cross-platform applications more enjoyable and efficient. With KMM you write code once for business logic and the app core in Kotlin and then  share it with both Android and iOS applications. Write platform-specific code only when necessary, for example, to take advantage of native UI elements; and the specific code is kept in different views for each platform. Although still in Alpha, Kotlin Multiplatform Mobile is <a href=\"\"https://kotlinlang.org/docs/mobile/kmm-evolution.html\"\">evolving rapidly</a>. We'll certainly keep an eye on it, and you should too.</p>\""
    },
    {
        "name": "LVGL",
        "ring": "assess",
        "quadrant": "languages-and-frameworks",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>With the increasing popularity of smart home and wearable devices, demand for intuitive graphical user interfaces (GUIs)is increasing. However, if you're engaged in embedded device development, rather than Android/iOS, GUI development may take a lot of effort. As an open-source embedded graphics library, <strong><a href=\"\"https://github.com/lvgl/lvgl\"\">LVGL</a></strong> has become increasingly popular. LVGL has been adapted to mainstream embedded platforms such as NXP, STM32, PIC, Arduino, and ESP32. It has a very small memory footprint: 64 kB flash and 8 kB RAM is enough to make it work, and it can run smoothly on various Cortex-M0 low-power MCUs. LVGL supports input types such as touchscreen, mouse and buttons and contains more than 30 controls, including TileView suitable for smart watches. The MIT license it chose doesn’t restrict enterprise and commercial use. Our teams’ feedback on this tool has been positive and one of our projects using LVGL is already in production, more specifically in small batch manufacturing.</p>\""
    },
    {
        "name": "React Hook Form",
        "ring": "assess",
        "quadrant": "languages-and-frameworks",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>Building forms for the web remains one of the perennial challenges of front-end development, in particular with <a href=\"\"/radar/languages-and-frameworks/react-js\"\">React</a>. Many of our teams working with React have been using <a href=\"\"/radar/languages-and-frameworks/formik\"\">Formik</a> to make this easier, but some are now assessing <strong><a href=\"\"https://react-hook-form.com/\"\">React Hook Form</a></strong> as a potential alternative. <a href=\"\"/radar/languages-and-frameworks/react-hooks\"\">React Hooks</a> already existed when React Hook Form was created, so it could use them as a first-class concept: the framework is registering and tracking form elements as uncontrolled components via a hook, thereby significantly reducing the need for re-rendering. It's also quite lightweight in size and in the amount of boilerplate code needed.</p>\""
    },
    {
        "name": "River",
        "ring": "assess",
        "quadrant": "languages-and-frameworks",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>At the heart of many approaches to machine learning lies the creation of a model from a set of training data. Once a model is created, it can be used over and over again. However, the world isn't stationary, and often the model needs to change as new data becomes available. Simply re-running the model creation step can be slow and <a href=\"\"https://medium.com/syncedreview/the-staggering-cost-of-training-sota-ai-models-e329e80fa82\"\">costly</a>. Incremental learning addresses this issue, making it possible to learn from streams of data incrementally to react to change faster. As a bonus the compute and memory requirements are lower and predictable. In our implementations we've had good experience with the <strong><a href=\"\"https://riverml.xyz/dev/\"\">River</a></strong> framework, but so far we've added checks, sometimes manual, after updates to the model.</p>\""
    },
    {
        "name": "Webpack 5 Module Federation",
        "ring": "assess",
        "quadrant": "languages-and-frameworks",
        "isNew": "TRUE",
        "status": "new",
        "description": "\"<p>The release of the <strong><a href=\"\"https://webpack.js.org/concepts/module-federation/\"\">Webpack 5 Module Federation</a></strong> feature has been highly anticipated by developers of <a href=\"\"/radar/techniques/micro-frontends\"\">micro frontend</a> architectures. The feature introduces a more standardized way to optimize how module dependencies and shared code are managed and loaded. Module federation allows for the specification of shared modules, which helps with the deduplication of dependencies across micro frontends by loading code used by multiple modules only once. It also lets you distinguish between local and remote modules, where the remote modules are not actually part of the build itself but loaded asynchronously. Compared to build-time dependencies like npm packages, this can significantly simplify the deployment of a module update with many downstream dependencies. Be aware, though, that this requires you to bundle all of your micro frontends with Webpack, as opposed to approaches such as <a href=\"\"/radar/techniques/import-maps-for-micro-frontends\"\">import maps</a>, which might eventually become part of the W3C standard.</p>\""
    }
]